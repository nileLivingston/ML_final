The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied|0|arxiv|0|abstract|MISC
If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed|1|arxiv|0|abstract|MISC
For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures|2|arxiv|0|abstract|MISC
We show that this is even the case if the model class contains only Bernoulli distributions|3|arxiv|0|abstract|AIM
We derive a new upper bound on the prediction error for countable Bernoulli classes|4|arxiv|0|abstract|OWN
This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes|5|arxiv|0|abstract|OWN
We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models|6|arxiv|0|abstract|OWN
``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class|7|arxiv|0|introduction|MISC
In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive|8|arxiv|0|introduction|CONT
The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence|9|arxiv|0|introduction|MISC
In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined|10|arxiv|0|introduction|MISC
How good are the predictions by Bayes mixtures and MDL|11|arxiv|0|introduction|MISC
This question has attracted much attention|12|arxiv|0|introduction|MISC
In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor|13|arxiv|0|introduction|MISC
In particular the square loss is often considered|14|arxiv|0|introduction|MISC
Assume that the outcome space is finite, and the model class is continuously parameterized|15|arxiv|0|introduction|MISC
Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION|16|arxiv|0|introduction|MISC
This corresponds to an  instantaneous  loss bound of  SYMBOL|17|arxiv|0|introduction|MISC
For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior|18|arxiv|0|introduction|MISC
Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION|19|arxiv|0|introduction|MISC
On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL|20|arxiv|0|introduction|MISC
The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning|21|arxiv|0|introduction|MISC
It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly|22|arxiv|0|introduction|MISC
For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds|23|arxiv|0|introduction|MISC
This bound is exponentially larger than the Solomonoff bound, and it is sharp in general|24|arxiv|0|introduction|MISC
A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one|25|arxiv|0|introduction|MISC
In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability|26|arxiv|0|introduction|MISC
Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often|27|arxiv|0|introduction|MISC
So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture|28|arxiv|0|introduction|MISC
We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases|29|arxiv|0|introduction|MISC
It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general)|30|arxiv|0|introduction|MISC
It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions|31|arxiv|0|introduction|MISC
In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes|32|arxiv|0|introduction|AIM
Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction|33|arxiv|0|introduction|OWN
Nevertheless, for consistency of terminology, we keep the term predictor|34|arxiv|0|introduction|OWN
It might be surprising to discover that in general the cumulative loss is still exponential|35|arxiv|0|introduction|MISC
On the other hand, we will give mild conditions on the prior guaranteeing a small bound|36|arxiv|0|introduction|OWN
Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case|37|arxiv|0|introduction|MISC
The same holds for MDL, as we will see|38|arxiv|0|introduction|MISC
If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section|39|arxiv|0|introduction|MISC
A particular motivation to consider discrete model classes arises in Algorithmic Information Theory|40|arxiv|0|introduction|MISC
From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION|41|arxiv|0|introduction|MISC
Thus each model corresponds to a program, and there are countably many programs|42|arxiv|0|introduction|MISC
Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)|43|arxiv|0|introduction|MISC
Each model has a natural description length, namely the length of the corresponding program|44|arxiv|0|introduction|MISC
If we agree that programs are binary strings, then a prior is defined by two to the negative description length|45|arxiv|0|introduction|MISC
By the Kraft inequality, the priors sum up to at most one|46|arxiv|0|introduction|MISC
Also the Bernoulli case can be studied in the view of Algorithmic Information Theory|47|arxiv|0|introduction|MISC
We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL|48|arxiv|0|introduction|MISC
The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program|49|arxiv|0|introduction|MISC
A prior weight may then be defined by  SYMBOL|50|arxiv|0|introduction|MISC
If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION|51|arxiv|0|introduction|MISC
That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant|52|arxiv|0|introduction|MISC
Many Machine Learning tasks are or can be reduced to sequence prediction tasks|53|arxiv|0|introduction|MISC
An important example is classification|54|arxiv|0|introduction|MISC
The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL|55|arxiv|0|introduction|MISC
Typically the (instance,class) pairs are iid|56|arxiv|0|introduction|MISC
Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION|57|arxiv|0|introduction|MISC
Then we can solve classification problems in the standard form|58|arxiv|0|introduction|MISC
It is not obvious if and how the proofs in this paper can be conditionalized|59|arxiv|0|introduction|OWN
Our main tool for obtaining results is the Kullback-Leibler divergence|60|arxiv|0|introduction|OWN
Lemmata for this quantity are stated in Section|61|arxiv|0|introduction|OWN
Section  shows that the exponential error bound obtained in  CITATION  is sharp in general|62|arxiv|0|introduction|OWN
In Section , we give an upper bound on the instantaneous and the cumulative losses|63|arxiv|0|introduction|OWN
The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section|64|arxiv|0|introduction|OWN
Section  treats the universal setup|65|arxiv|0|introduction|OWN
Finally, in Section  we discuss the results and give conclusions|66|arxiv|0|introduction|OWN
The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied|0|arxiv|0|abstract|MISC
If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed|1|arxiv|0|abstract|MISC
For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures|2|arxiv|0|abstract|MISC
We show that this is even the case if the model class contains only Bernoulli distributions|3|arxiv|0|abstract|AIM
We derive a new upper bound on the prediction error for countable Bernoulli classes|4|arxiv|0|abstract|OWN
This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes|5|arxiv|0|abstract|OWN
We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models|6|arxiv|0|abstract|OWN
``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class|7|arxiv|0|introduction|MISC
In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive|8|arxiv|0|introduction|CONT
The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence|9|arxiv|0|introduction|MISC
In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined|10|arxiv|0|introduction|MISC
How good are the predictions by Bayes mixtures and MDL|11|arxiv|0|introduction|MISC
This question has attracted much attention|12|arxiv|0|introduction|MISC
In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor|13|arxiv|0|introduction|MISC
In particular the square loss is often considered|14|arxiv|0|introduction|MISC
Assume that the outcome space is finite, and the model class is continuously parameterized|15|arxiv|0|introduction|MISC
Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION|16|arxiv|0|introduction|MISC
This corresponds to an  instantaneous  loss bound of  SYMBOL|17|arxiv|0|introduction|MISC
For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior|18|arxiv|0|introduction|MISC
Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION|19|arxiv|0|introduction|MISC
On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL|20|arxiv|0|introduction|MISC
The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning|21|arxiv|0|introduction|MISC
It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly|22|arxiv|0|introduction|MISC
For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds|23|arxiv|0|introduction|MISC
This bound is exponentially larger than the Solomonoff bound, and it is sharp in general|24|arxiv|0|introduction|MISC
A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one|25|arxiv|0|introduction|MISC
In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability|26|arxiv|0|introduction|MISC
Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often|27|arxiv|0|introduction|MISC
So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture|28|arxiv|0|introduction|MISC
We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases|29|arxiv|0|introduction|MISC
It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general)|30|arxiv|0|introduction|MISC
It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions|31|arxiv|0|introduction|MISC
In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes|32|arxiv|0|introduction|AIM
Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction|33|arxiv|0|introduction|OWN
Nevertheless, for consistency of terminology, we keep the term predictor|34|arxiv|0|introduction|OWN
It might be surprising to discover that in general the cumulative loss is still exponential|35|arxiv|0|introduction|OWN
On the other hand, we will give mild conditions on the prior guaranteeing a small bound|36|arxiv|0|introduction|OWN
Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case|37|arxiv|0|introduction|MISC
The same holds for MDL, as we will see|38|arxiv|0|introduction|MISC
If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section  |39|arxiv|0|introduction|MISC
A particular motivation to consider discrete model classes arises in Algorithmic Information Theory|40|arxiv|0|introduction|MISC
From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION|41|arxiv|0|introduction|MISC
Thus each model corresponds to a program, and there are countably many programs|42|arxiv|0|introduction|MISC
Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)|43|arxiv|0|introduction|MISC
Each model has a natural description length, namely the length of the corresponding program|44|arxiv|0|introduction|MISC
If we agree that programs are binary strings, then a prior is defined by two to the negative description length|45|arxiv|0|introduction|MISC
By the Kraft inequality, the priors sum up to at most one|46|arxiv|0|introduction|MISC
Also the Bernoulli case can be studied in the view of Algorithmic Information Theory|47|arxiv|0|introduction|MISC
We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL|48|arxiv|0|introduction|MISC
The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program|49|arxiv|0|introduction|MISC
A prior weight may then be defined by  SYMBOL|50|arxiv|0|introduction|MISC
If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION|51|arxiv|0|introduction|MISC
That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant|52|arxiv|0|introduction|MISC
Many Machine Learning tasks are or can be reduced to sequence prediction tasks|53|arxiv|0|introduction|MISC
An important example is classification|54|arxiv|0|introduction|MISC
The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL|55|arxiv|0|introduction|MISC
Typically the (instance,class) pairs are iid|56|arxiv|0|introduction|MISC
Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION|57|arxiv|0|introduction|MISC
Then we can solve classification problems in the standard form|58|arxiv|0|introduction|MISC
It is not obvious if and how the proofs in this paper can be conditionalized|59|arxiv|0|introduction|OWN
Our main tool for obtaining results is the Kullback-Leibler divergence|60|arxiv|0|introduction|OWN
Lemmata for this quantity are stated in Section|61|arxiv|0|introduction|OWN
Section  shows that the exponential error bound obtained in  CITATION  is sharp in general|62|arxiv|0|introduction|OWN
In Section , we give an upper bound on the instantaneous and the cumulative losses|63|arxiv|0|introduction|OWN
The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section|64|arxiv|0|introduction|OWN
Section  treats the universal setup|65|arxiv|0|introduction|OWN
Finally, in Section  we discuss the results and give conclusions|66|arxiv|0|introduction|OWN
 The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied|0|arxiv|1|abstract|OWN
 If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed|1|arxiv|1|abstract|MISC
 For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures|2|arxiv|1|abstract|MISC
 We show that this is even the case if the model class contains only Bernoulli distributions|3|arxiv|1|abstract|AIM
 We derive a new upper bound on the prediction error for countable Bernoulli classes|4|arxiv|1|abstract|OWN
 This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes|5|arxiv|1|abstract|OWN
 We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of iid models|6|arxiv|1|abstract|OWN
 ``Bayes mixture", ``Solomonoff induction", ``marginalization", all these terms refer to a central induction principle: Obtain a predictive distribution by integrating the product of prior and evidence over the model class|7|arxiv|1|introduction|MISC
 In many cases however, the Bayes mixture is computationally infeasible, and even a sophisticated approximation is expensive|8|arxiv|1|introduction|MISC
 The MDL or MAP (maximum a posteriori) estimator is both a common approximation for the Bayes mixture and interesting for its own sake: Use the model with the largest product of prior and evidence|9|arxiv|1|introduction|MISC
 In practice, the MDL estimator is usually being approximated too, since only a local maximum is determined|10|arxiv|1|introduction|MISC
 How good are the predictions by Bayes mixtures and MDL|11|arxiv|1|introduction|MISC
 This question has attracted much attention|12|arxiv|1|introduction|MISC
 In many cases, an important quality measure is the  total  or cumulative  expected loss  of a predictor|13|arxiv|1|introduction|MISC
 In particular the square loss is often considered|14|arxiv|1|introduction|MISC
 Assume that the outcome space is finite, and the model class is continuously parameterized|15|arxiv|1|introduction|MISC
 Then for Bayes mixture prediction, the cumulative expected square loss is usually small but unbounded, growing with  SYMBOL , where  SYMBOL  is the sample size  CITATION|16|arxiv|1|introduction|MISC
 This corresponds to an  instantaneous  loss bound of  SYMBOL|17|arxiv|1|introduction|MISC
 For the MDL predictor, the losses behave similarly  CITATION  under appropriate conditions, in particular with a specific prior|18|arxiv|1|introduction|MISC
 Note that in order to do MDL for continuous model classes, one needs to  discretize  the parameter space, see also  CITATION|19|arxiv|1|introduction|MISC
 On the other hand, if the model class is discrete, then Solomonoff's theorem  CITATION  bounds the cumulative expected square loss for the Bayes mixture predictions finitely, namely by  SYMBOL , where  SYMBOL  is the prior weight of the ``true" model  SYMBOL|20|arxiv|1|introduction|MISC
 The only necessary assumption is that the true distribution  SYMBOL  is contained in the model class, ie that we are dealing with  proper learning|21|arxiv|1|introduction|MISC
 It has been demonstrated  CITATION , that for both Bayes mixture and MDL, the proper learning assumption can be essential: If it is violated, then learning may fail very badly|22|arxiv|1|introduction|MISC
 For MDL predictions in the proper learning case, it has been shown  CITATION  that a bound of  SYMBOL  holds|23|arxiv|1|introduction|MISC
 This bound is exponentially larger than the Solomonoff bound, and it is sharp in general|24|arxiv|1|introduction|MISC
 A finite bound on the total expected square loss is particularly interesting:   It implies convergence of the predictive to the true probabilities with probability one|25|arxiv|1|introduction|MISC
 In contrast, an instantaneous loss bound of  SYMBOL  implies only convergence in probability|26|arxiv|1|introduction|MISC
 Additionally, it gives a  convergence speed , in the sense that errors of a certain magnitude cannot occur too often|27|arxiv|1|introduction|MISC
 So for both, Bayes mixtures and MDL, convergence with probability one holds, while the convergence speed is exponentially worse for MDL compared to the Bayes mixture|28|arxiv|1|introduction|MISC
 We avoid the term ``convergence rate" here, since the order of convergence is identical in both cases|29|arxiv|1|introduction|MISC
 It is eg  SYMBOL  if we additionally assume that the error is monotonically decreasing, which is not necessarily true in general|30|arxiv|1|introduction|MISC
 It is therefore natural to ask if there are model classes where the cumulative loss of MDL is comparable to that of Bayes mixture predictions|31|arxiv|1|introduction|MISC
 In the present work, we concentrate on the simplest possible stochastic case, namely discrete Bernoulli classes|32|arxiv|1|introduction|AIM
 Note that then the MDL ``predictor" just becomes an estimator, in that it estimates the true parameter and directly uses that for prediction|33|arxiv|1|introduction|OWN
 Nevertheless, for consistency of terminology, we keep the term predictor|34|arxiv|1|introduction|OWN
 It might be surprising to discover that in general the cumulative loss is still exponential|35|arxiv|1|introduction|OWN
 On the other hand, we will give mild conditions on the prior guaranteeing a small bound|36|arxiv|1|introduction|OWN
 Moreover, it is well-known that the instantaneous square loss of the Maximum Likelihood estimator decays as  SYMBOL  in the Bernoulli case|37|arxiv|1|introduction|MISC
 The same holds for MDL, as we will see|38|arxiv|1|introduction|OWN
 If convergence speed is measured in terms of instantaneous losses, then much more general statements are possible  CITATION , this is briefly discussed in Section|39|arxiv|1|introduction|OWN
 A particular motivation to consider discrete model classes arises in Algorithmic Information Theory|40|arxiv|1|introduction|MISC
 From a computational point of view, the largest relevant model class is the class of all computable models on some fixed universal Turing machine, precisely prefix machine  CITATION|41|arxiv|1|introduction|MISC
 Thus each model corresponds to a program, and there are countably many programs|42|arxiv|1|introduction|MISC
 Moreover, the models are stochastic, precisely they are  semimeasures  on strings (programs need not halt, otherwise the models were even measures)|43|arxiv|1|introduction|MISC
 Each model has a natural description length, namely the length of the corresponding program|44|arxiv|1|introduction|MISC
 If we agree that programs are binary strings, then a prior is defined by two to the negative description length|45|arxiv|1|introduction|MISC
 By the Kraft inequality, the priors sum up to at most one|46|arxiv|1|introduction|MISC
 Also the Bernoulli case can be studied in the view of Algorithmic Information Theory|47|arxiv|1|introduction|MISC
 We call this the  universal setup : Given a universal Turing machine, the related class of Bernoulli distributions is isomorphic to the countable set of computable reals in  SYMBOL|48|arxiv|1|introduction|MISC
 The description length  SYMBOL  of a parameter  SYMBOL  is then given by the length of its shortest program|49|arxiv|1|introduction|MISC
 A prior weight may then be defined by  SYMBOL|50|arxiv|1|introduction|MISC
 If a string  SYMBOL  is generated by a Bernoulli distribution with computable parameter  SYMBOL , then with high probability the two-part complexity of  SYMBOL  with respect to the Bernoulli class does not exceed its algorithmic complexity by more than a constant, as shown by Vovk  CITATION|51|arxiv|1|introduction|MISC
 That is, the two-part complexity with respect to the Bernoulli class  is  the shortest description, save for an additive constant|52|arxiv|1|introduction|MISC
 Many Machine Learning tasks are or can be reduced to sequence prediction tasks|53|arxiv|1|introduction|MISC
 An important example is classification|54|arxiv|1|introduction|MISC
 The task of classifying a new instance  SYMBOL  after having seen (instance,class) pairs  SYMBOL  can be phrased as to predict the continuation of the sequence  SYMBOL|55|arxiv|1|introduction|MISC
 Typically the (instance,class) pairs are iid|56|arxiv|1|introduction|MISC
 Cumulative loss bounds for prediction usually generalize to prediction  conditionalized  to some inputs  CITATION|57|arxiv|1|introduction|MISC
 Then we can solve classification problems in the standard form|58|arxiv|1|introduction|OWN
 It is not obvious if and how the proofs in this paper can be conditionalized|59|arxiv|1|introduction|OWN
 Our main tool for obtaining results is the Kullback-Leibler divergence|60|arxiv|1|introduction|OWN
 Lemmata for this quantity are stated in Section|61|arxiv|1|introduction|OWN
 Section  shows that the exponential error bound obtained in  CITATION  is sharp in general|62|arxiv|1|introduction|OWN
 In Section , we give an upper bound on the instantaneous and the cumulative losses|63|arxiv|1|introduction|OWN
 The latter bound is small eg under certain conditions on the distribution of the weights, this is the subject of Section|64|arxiv|1|introduction|OWN
 Section  treats the universal setup|65|arxiv|1|introduction|OWN
 Finally, in Section  we discuss the results and give conclusions|66|arxiv|1|introduction|OWN
although the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy|0|arxiv|0|abstract|MISC
an as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments|1|arxiv|0|abstract|MISC
in this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy|2|arxiv|0|abstract|AIM
we successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent |3|arxiv|0|abstract|OWN
we release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases|4|arxiv|0|abstract|OWN
we believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet|5|arxiv|0|abstract|OWN
the rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments|6|arxiv|0|introduction|MISC
from  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet|7|arxiv|0|introduction|MISC
this impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure|8|arxiv|0|introduction|MISC
in particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies|9|arxiv|0|introduction|MISC
statistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth|10|arxiv|0|introduction|MISC
in topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies|11|arxiv|0|introduction|MISC
for example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company|12|arxiv|0|introduction|MISC
the university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls |13|arxiv|0|introduction|MISC
on the other hand  the small company will most probably have a single router and a simple network topology|14|arxiv|0|introduction|MISC
since there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot MISC	characterize the composing ases|15|arxiv|0|introduction|MISC
moreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns|16|arxiv|0|introduction|MISC
for example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps|17|arxiv|0|introduction|MISC
on the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time|18|arxiv|0|introduction|MISC
thus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models|19|arxiv|0|introduction|MISC
an as taxonomy is also necessary for mapping ip addresses to different types of users|20|arxiv|0|introduction|MISC
for example  in traffic analysis studies its often required to distinguish between packets that come from home and business users|21|arxiv|0|introduction|MISC
given an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies|22|arxiv|0|introduction|MISC
in this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy|23|arxiv|0|introduction|AIM
we develop an algorithm to classify ases based on empirically observed differences between as characteristics|24|arxiv|0|introduction|OWN
we use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types|25|arxiv|0|introduction|OWN
then  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures|26|arxiv|0|introduction|AIM
we derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types|27|arxiv|0|introduction|OWN
our validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct|28|arxiv|0|introduction|OWN
finally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution|29|arxiv|0|introduction|OWN
in section  we start with a brief discussion of related work|30|arxiv|0|introduction|OWN
section  describes the data we used  and in section  we specify the set of as classes we use in our experiments|31|arxiv|0|introduction|OWN
section  introduces our classification approach and results|32|arxiv|0|introduction|OWN
we validate them in section  and conclude in section |33|arxiv|0|introduction|OWN
although the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy|0|arxiv|0|abstract|MISC
an as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments|1|arxiv|0|abstract|MISC
in this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy|2|arxiv|0|abstract|AIM
we successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent |3|arxiv|0|abstract|OWN
we release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases|4|arxiv|0|abstract|OWN
we believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet|5|arxiv|0|abstract|OWN
the rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments|6|arxiv|0|introduction|MISC
from  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet|7|arxiv|0|introduction|MISC
this impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure|8|arxiv|0|introduction|MISC
in particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies|9|arxiv|0|introduction|MISC
statistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth|10|arxiv|0|introduction|MISC
in topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies|11|arxiv|0|introduction|MISC
for example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company|12|arxiv|0|introduction|MISC
the university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls |13|arxiv|0|introduction|MISC
on the other hand  the small company will most probably have a single router and a simple network topology|14|arxiv|0|introduction|MISC
since there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot characterize the composing ases|15|arxiv|0|introduction|MISC
moreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns|16|arxiv|0|introduction|MISC
for example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps|17|arxiv|0|introduction|MISC
on the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time|18|arxiv|0|introduction|MISC
thus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models|19|arxiv|0|introduction|MISC
an as taxonomy is also necessary for mapping ip addresses to different types of users|20|arxiv|0|introduction|MISC
for example  in traffic analysis studies its often required to distinguish between packets that come from home and business users|21|arxiv|0|introduction|MISC
given an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies|22|arxiv|0|introduction|MISC
in this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy|23|arxiv|0|introduction|AIM
we develop an algorithm to classify ases based on empirically observed differences between as characteristics|24|arxiv|0|introduction|AIM
we use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types|25|arxiv|0|introduction|OWN
then  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures|26|arxiv|0|introduction|OWN
we derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types|27|arxiv|0|introduction|OWN
our validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct|28|arxiv|0|introduction|OWN
finally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution|29|arxiv|0|introduction|OWN
in section  we start with a brief discussion of related work|30|arxiv|0|introduction|BASE
section  describes the data we used  and in section  we specify the set of as classes we use in our experiments|31|arxiv|0|introduction|OWN
section  introduces our classification approach and results|32|arxiv|0|introduction|OWN
we validate them in section  and conclude in section |33|arxiv|0|introduction|OWN
 Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy|0|arxiv|1|abstract|MISC
 An AS "node" can represent a wide variety of organizations, e g , large ISP, or small private business, university, with vastly different network characteristics, external connectivity patterns, network growth tendencies, and other properties that we can hardly neglect while working on veracious Internet representations in simulation environments|1|arxiv|1|abstract|MISC
 In this paper, we introduce a radically new approach based on machine learning techniques to map all the ASes in the Internet into a natural AS taxonomy|2|arxiv|1|abstract|AIM
 We successfully classify ~95.3\% of ASes with expected accuracy of ~78.1\%|3|arxiv|1|abstract|OWN
 We release to the community the AS-level topology dataset augmented with: 1) the AS taxonomy information and 2) the set of AS attributes we used to classify ASes|4|arxiv|1|abstract|OWN
 We believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the Internet|5|arxiv|1|abstract|OWN
 The rapid expansion of the Internet in the last two decades has produced a large-scale system of thousands of diverse, independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments|6|arxiv|1|introduction|MISC
 From 1997 to 2005 the number of globally routable AS identifiers has increased from less than 2,000 to more than 20,000, exerting significant pressure on interdomain routing as well as other functional and structural parts of the Internet|7|arxiv|1|introduction|MISC
 This impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the Internet infrastructure|8|arxiv|1|introduction|MISC
 In particular, the AS-level topology is an intermix of networks owned and operated by many different organizations, e g , backbone providers, regional providers, access providers, universities and private companies|9|arxiv|1|introduction|MISC
 Statistical information that faithfully characterizes different AS types is on the critical path toward understanding the structure of the Internet, as well as for modeling its topology and growth|10|arxiv|1|introduction|MISC
 In topology modeling, knowledge of AS types is mandatory for augmenting synthetically constructed or measured AS topologies with realistic intra-AS and inter-AS router-level topologies|11|arxiv|1|introduction|MISC
 For example, we expect the network of a dual-homed university to be drastically different from that of a dual-homed small company|12|arxiv|1|introduction|MISC
 The university will likely contain dozens of internal routers, thousands of hosts, and many other network elements (switches, servers, firewalls)|13|arxiv|1|introduction|MISC
 On the other hand, the small company will most probably have a single router and a simple network topology|14|arxiv|1|introduction|MISC
 Since there is such a diversity among different network types, we cannot accurately augment the AS-level topology with appropriate router-level topologies if we cannot characterize the composing ASes|15|arxiv|1|introduction|MISC
 Moreover, annotating the ASes in the AS topology with their types is a prerequisite for modeling the evolution of the Internet, since different types of ASes exhibit different growth patterns|16|arxiv|1|introduction|MISC
 For example, Internet Service Providers (ISP) grow by attracting new customers and by engaging in business agreements with other ISPs|17|arxiv|1|introduction|MISC
 On the other hand, small companies that connect to the Internet through one or few ISPs do not grow significantly over time|18|arxiv|1|introduction|MISC
 Thus, categorizing different types of ASes in the Internet is necessary to identify network evolution patterns and develop accurate evolution models|19|arxiv|1|introduction|MISC
 An AS taxonomy is also necessary for mapping IP addresses to different types of users|20|arxiv|1|introduction|MISC
 For example, in traffic analysis studies its often required to distinguish between packets that come from home and business users|21|arxiv|1|introduction|MISC
 Given an AS taxonomy, its possible to realize this goal by checking the type of AS that originates the prefix in which an IP address lies|22|arxiv|1|introduction|MISC
 In this work, we introduce a radically new approach based on machine learning to construct a representative AS taxonomy|23|arxiv|1|introduction|AIM
 We develop an algorithm to classify ASes based on empirically observed differences between AS characteristics|24|arxiv|1|introduction|OWN
 We use a large set of data from the Internet Routing Registries~(IRR)~ CITATION  and from RouteViews~ CITATION  to identify intrinsic differences between ASes of different types|25|arxiv|1|introduction|BASE
 Then, we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ASes into six representative classes that reflect ASes with different network properties and infrastructures|26|arxiv|1|introduction|OWN
 We derive macroscopic statistics on the different types of ASes in the Internet and validate our results using a sample of~1200 manually identified AS types|27|arxiv|1|introduction|OWN
 Our validation demonstrates that our classification algorithm achieves high accuracy:~78 1\% of the examined classifications were correct|28|arxiv|1|introduction|OWN
 Finally, we make our results and our classifier publicly available to promote further research and understanding of the Internet's structure and evolution|29|arxiv|1|introduction|OWN
 In Section~ we start with a brief discussion of related work|30|arxiv|1|introduction|OWN
 Section~ describes the data we used, and in Section~ we specify the set of AS classes we use in our experiments|31|arxiv|1|introduction|OWN
 Section~ introduces our classification approach and results|32|arxiv|1|introduction|OWN
 We validate them in Section~ and conclude in Section~|33|arxiv|1|introduction|OWN
in this paper we derive the equations for loop corrected belief propagation on a continuous variable gaussian model|0|arxiv|0|abstract|AIM
using the exactness of the averages for belief propagation for gaussian models  a  different way of obtaining the covariances is found   based on belief propagation on cavity graphs|1|arxiv|0|abstract|OWN
we discuss the relation of this  loop correction algorithm to expectation propagation  algorithms for the case in which the model is no longer  gaussian  but slightly perturbed by nonlinear terms|2|arxiv|0|abstract|OWN
message passing techniques in graphical models allow for the computation of  approximate   marginal probabilities in a time interval scaling polynomially in the  model size|3|arxiv|0|introduction|MISC
their discovery has consequently revolutionized several  fields of applications in the past years  of which error correcting codes and vision are probably the most prominent examples|4|arxiv|0|introduction|MISC
in many cases  the corresponding graphs are loopy  implying either that the error resulting from the application of loopy belief propagation  bp  is negligible for the particular model  or it  can be tolerated for the particular purpose bp serves|5|arxiv|0|introduction|MISC
in other cases  more sophisticated refinements of bp are necessary  taking into account  part of  the loop errors|6|arxiv|0|introduction|MISC
finding the optimal treatment of these   loop errors    motivates an active field of research  in which  different solutions applying to different model classes are developed|7|arxiv|0|introduction|MISC
for models involving many short loops   like on regular lattices  cvm type approaches work well  CITATION   or tree ep approaches  CITATION|8|arxiv|0|introduction|MISC
the latter may also be  applied to correct for an incidental large loop|9|arxiv|0|introduction|MISC
unifying frameworks like the region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes|10|arxiv|0|introduction|MISC
a recent analysis has shown that the local update equations of bp may be interpreted as the zero order term of an expansion in   cavity connected correlations  |11|arxiv|0|introduction|MISC
these quantities are parameterizations of the   cavity distributions     i e   the  distribution over neighbor variables of a central variable which has been removed from the graph|12|arxiv|0|introduction|MISC
the bethe approximation and bp are recovered when this  cavity distribution is assumed to factorize  whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION|13|arxiv|0|introduction|MISC
estimation of these pair cumulants is possible with extra runs of bp  allowing for new polynomial time algorithms  reducing errors to order    when applying algorithms of which running time scales with an extra factor  of    CITATION|14|arxiv|0|introduction|MISC
although this scaling seems heavy  the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree structures  since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once|15|arxiv|0|introduction|MISC
the above   loop correction    strategy is applicable in the class of models where a perturbative expansion around the bethe approximation makes sense  i e   in models with large loops and relatively weak interactions|16|arxiv|0|introduction|MISC
the principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants  and third order cumulants are even smaller  etc|17|arxiv|0|introduction|MISC
however  heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION|18|arxiv|0|introduction|MISC
so far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION|19|arxiv|0|introduction|CONT
in this paper we apply the idea to graphical models for continuous variables|20|arxiv|0|introduction|AIM
we derive the loop corrected belief propagation equations  for simple tractable gaussian models   yielding a message passing scheme that  besides the correct average marginals  also yields the correct variances|21|arxiv|0|introduction|OWN
besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary   and the relation with expectation propagation|22|arxiv|0|introduction|OWN
a by product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for gaussian models like the one discussed in  CITATION   but without explicitly using linear response|23|arxiv|0|introduction|CONT
 In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model|0|arxiv|0|abstract|AIM
 Using the exactness of the averages for belief propagation for Gaussian models, a  different way of obtaining the covariances is found,  based on Belief Propagation on cavity graphs|1|arxiv|0|abstract|OWN
 We discuss the relation of this  loop correction algorithm to Expectation Propagation  algorithms for the case in which the model is no longer  Gaussian, but slightly perturbed by nonlinear terms|2|arxiv|0|abstract|AIM
 Message passing techniques in graphical models allow for the computation of (approximate)  marginal probabilities in a time interval scaling polynomially in the  model size|3|arxiv|0|introduction|MISC
 Their discovery has consequently revolutionized several  fields of applications in the past years, of which error correcting codes and vision are probably the most prominent examples|4|arxiv|0|introduction|MISC
 In many cases, the corresponding graphs are loopy, implying either that the error resulting from the application of loopy belief propagation (BP) is negligible for the particular model, or it  can be tolerated for the particular purpose BP serves|5|arxiv|0|introduction|MISC
 In other cases  more sophisticated refinements of BP are necessary, taking into account (part of) the loop errors|6|arxiv|0|introduction|MISC
 Finding the optimal treatment of these ``loop errors''  motivates an active field of research, in which  different solutions applying to different model classes are developed|7|arxiv|0|introduction|MISC
 For models involving many short loops,  like on regular lattices, CVM type approaches work well  CITATION , or tree EP approaches  CITATION|8|arxiv|0|introduction|MISC
 The latter may also be  applied to correct for an incidental large loop|9|arxiv|0|introduction|MISC
 Unifying frameworks like the Region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes|10|arxiv|0|introduction|MISC
 A recent analysis has shown that the local update equations of BP may be interpreted as the zero order term of an expansion in ``cavity connected correlations''|11|arxiv|0|introduction|MISC
 These quantities are parameterizations of the ``cavity distributions'',  i e , the  distribution over neighbor variables of a central variable which has been removed from the graph|12|arxiv|0|introduction|MISC
 The Bethe approximation and BP are recovered when this  cavity distribution is assumed to factorize, whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION|13|arxiv|0|introduction|MISC
 Estimation of these pair cumulants is possible with extra runs of BP, allowing for new polynomial time algorithms, reducing errors to order  SYMBOL   when applying algorithms of which running time scales with an extra factor  of  SYMBOL   CITATION|14|arxiv|0|introduction|MISC
 Although this scaling seems heavy, the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree-structures, since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once|15|arxiv|0|introduction|MISC
 The above ``loop correction''  strategy is applicable in the class of models where a perturbative expansion around the Bethe approximation makes sense, i e , in models with large loops and relatively weak interactions|16|arxiv|0|introduction|MISC
 The principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants, and third order cumulants are even smaller, etc|17|arxiv|0|introduction|MISC
 However, heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION|18|arxiv|0|introduction|MISC
 So far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION|19|arxiv|0|introduction|MISC
 In this paper we apply the idea to graphical models for continuous variables|20|arxiv|0|introduction|AIM
 We derive the loop corrected belief propagation equations  for simple tractable Gaussian models,  yielding a message passing scheme that, besides the correct average marginals, also yields the correct variances|21|arxiv|0|introduction|OWN
 Besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary,  and the relation with expectation propagation|22|arxiv|0|introduction|AIM
 A by-product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for Gaussian models like the one discussed in  CITATION , but without explicitly using linear response|23|arxiv|0|introduction|OWN
 In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model|0|arxiv|1|abstract|AIM
 Using the exactness of the averages for belief propagation for Gaussian models, a  different way of obtaining the covariances is found,  based on Belief Propagation on cavity graphs|1|arxiv|1|abstract|BASE
 We discuss the relation of this  loop correction algorithm to Expectation Propagation  algorithms for the case in which the model is no longer  Gaussian, but slightly perturbed by nonlinear terms|2|arxiv|1|abstract|OWN
 Message passing techniques in graphical models allow for the computation of (approximate)  marginal probabilities in a time interval scaling polynomially in the  model size|3|arxiv|1|introduction|MISC
 Their discovery has consequently revolutionized several  fields of applications in the past years, of which error correcting codes and vision are probably the most prominent examples|4|arxiv|1|introduction|MISC
 In many cases, the corresponding graphs are loopy, implying either that the error resulting from the application of loopy belief propagation (BP) is negligible for the particular model, or it  can be tolerated for the particular purpose BP serves|5|arxiv|1|introduction|MISC
 In other cases  more sophisticated refinements of BP are necessary, taking into account (part of) the loop errors|6|arxiv|1|introduction|MISC
 Finding the optimal treatment of these ``loop errors''  motivates an active field of research, in which  different solutions applying to different model classes are developed|7|arxiv|1|introduction|MISC
 For models involving many short loops,  like on regular lattices, CVM type approaches work well  CITATION , or tree EP approaches  CITATION|8|arxiv|1|introduction|MISC
 The latter may also be  applied to correct for an incidental large loop|9|arxiv|1|introduction|MISC
 Unifying frameworks like the Region graphs of  CITATION   lead to general strategies for selecting the basic clusters underlying such approaches for general model classes|10|arxiv|1|introduction|MISC
 A recent analysis has shown that the local update equations of BP may be interpreted as the zero order term of an expansion in ``cavity connected correlations''|11|arxiv|1|introduction|MISC
 These quantities are parameterizations of the ``cavity distributions'',  i e , the  distribution over neighbor variables of a central variable which has been removed from the graph|12|arxiv|1|introduction|MISC
 The Bethe approximation and BP are recovered when this  cavity distribution is assumed to factorize, whereas the first order correction to the local update equations is obtained when one takes into account the pair cumulants  CITATION|13|arxiv|1|introduction|MISC
 Estimation of these pair cumulants is possible with extra runs of BP, allowing for new polynomial time algorithms, reducing errors to order  SYMBOL   when applying algorithms of which running time scales with an extra factor  of  SYMBOL   CITATION|14|arxiv|1|introduction|MISC
 Although this scaling seems heavy, the large benefit of the approach is that it does not require selection of basic clusters or underlying  tree-structures, since it takes into account the effect of all loops that contribute to nontrivial correlations in the cavity distribution at once|15|arxiv|1|introduction|MISC
 The above ``loop correction''  strategy is applicable in the class of models where a perturbative expansion around the Bethe approximation makes sense, i e , in models with large loops and relatively weak interactions|16|arxiv|1|introduction|MISC
 The principal requirement  is that the magnitude of pair variable cumulants of cavity distributions is an order smaller than the  single variable cumulants, and third order cumulants are even smaller, etc|17|arxiv|1|introduction|MISC
 However, heuristics based on the strategy allow for other good algorithms  performing well outside these parameter regimes  CITATION|18|arxiv|1|introduction|MISC
 So far the approach has been developed for discrete variable models on a more abstract  CITATION  versus practical level  CITATION|19|arxiv|1|introduction|MISC
 In this paper we apply the idea to graphical models for continuous variables|20|arxiv|1|introduction|AIM
 We derive the loop corrected belief propagation equations  for simple tractable Gaussian models,  yielding a message passing scheme that, besides the correct average marginals, also yields the correct variances|21|arxiv|1|introduction|OWN
 Besides that we discuss some approaches potentially  applicable to cases in which extra function approximations are necessary,  and the relation with expectation propagation|22|arxiv|1|introduction|OWN
 A by-product of our loop corrected belief propagation equations is an algorithm that calculates exact covariance matrices for Gaussian models like the one discussed in  CITATION , but without explicitly using linear response|23|arxiv|1|introduction|BASE
defensive forecasting is a method of transforming laws of probability  stated in game theoretic terms as strategies for sceptic  into forecasting algorithms|0|arxiv|0|abstract|MISC
there are two known varieties of defensive forecasting    continuous    in which sceptic s moves are assumed to depend on the forecasts in a  semi continuous manner and which produces deterministic forecasts  and   randomized    in which the dependence of sceptic s moves on the forecasts is arbitrary and forecaster s moves are allowed to be randomized|1|arxiv|0|abstract|MISC
this note shows that the randomized variety can be obtained from the continuous variety by smearing sceptic s moves to make them continuous  |2|arxiv|0|abstract|AIM
textbf new as compared to version  NUMBER    NUMBER  august  NUMBER   of this report   the assumption of version  NUMBER  that the outcome space   is finite is relaxed  and now it is only assumed to be compact|3|arxiv|0|abstract|CONT
in the case where   is finite  it is shown that forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most|4|arxiv|0|abstract|CONT
the continuous variety of defensive forecasting was essentially introduced by levin  CITATION   but was later rediscovered by kakade and foster  CITATION  and takemura  et al    CITATION|5|arxiv|0|introduction|MISC
the randomized variety was introduced  in the case of von mises s version of the game theoretic approach to probability  by foster and vohra  CITATION  and further developed by  among others  sandroni  et al    CITATION   these papers  however  were only concerned with asymptotic calibration|6|arxiv|0|introduction|CONT
non asymptotic versions of the randomized variety were proposed by sandroni  CITATION   based on standard measure theoretic probability  and vovk and shafer  CITATION   based on game theoretic probability |7|arxiv|0|introduction|CONT
kakade and foster  CITATION  noticed that some calibration results require very little randomization  this will be an important aspect of our theorem  |8|arxiv|0|introduction|BASE
this note states two simple results about defensive forecasting  theorem  about the continuous variety and theorem  about the randomized variety|9|arxiv|0|introduction|AIM
the proof of theorem  is obtained from the proof of theorem  by blurring sceptic s moves|10|arxiv|0|introduction|OWN
in our informal discussions we will be assuming that the set   of all possible outcomes is finite  although we will try to make mathematical statements as general as possible|11|arxiv|0|introduction|OWN
the reader who is only interested in the main ideas might choose to specialize theorems  and  and their proofs to the case of finite|12|arxiv|0|introduction|OWN
defensive forecasting is a method of transforming laws of probability  stated in game theoretic terms as strategies for sceptic  into forecasting algorithms|0|arxiv|0|abstract|MISC
there are two known varieties of defensive forecasting    continuous    in which sceptic s moves are assumed to depend on the forecasts in a  semi continuous manner and which produces deterministic forecasts  and   randomized    in which the dependence of sceptic s moves on the forecasts is arbitrary and forecaster s moves are allowed to be randomized|1|arxiv|0|abstract|MISC
this note shows that the randomized variety can be obtained from the continuous variety by smearing sceptic s moves to make them continuous|2|arxiv|0|abstract|MISC
new as compared to version  NUMBER    NUMBER  august  NUMBER   of this report   the assumption of version  NUMBER  that the outcome space   is finite is relaxed  and now it is only assumed to be compact|3|arxiv|0|abstract|OWN
in the case where   is finite  it is shown that forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most|4|arxiv|0|abstract|OWN
the continuous variety of defensive forecasting was essentially introduced by levin  CITATION   but was later rediscovered by kakade and foster  CITATION  and takemura  et al    CITATION|5|arxiv|0|introduction|MISC
the randomized variety was introduced  in the case of von mises s version of the game theoretic approach to probability  by foster and vohra  CITATION  and further developed by  among others  sandroni  et al    CITATION   these papers  however  were only concerned with asymptotic calibration|6|arxiv|0|introduction|MISC
non asymptotic versions of the randomized variety were proposed by sandroni  CITATION   based on standard measure theoretic probability  and vovk and shafer  CITATION   based on game theoretic probability |7|arxiv|0|introduction|MISC
kakade and foster  CITATION  noticed that some calibration results require very little randomization  this will be an important aspect of our theorem  |8|arxiv|0|introduction|BASE
this note states two simple results about defensive forecasting  theorem  about the continuous variety and theorem  about the randomized variety|9|arxiv|0|introduction|AIM
the proof of theorem  is obtained from the proof of theorem  by blurring sceptic s moves|10|arxiv|0|introduction|OWN
in our informal discussions we will be assuming that the set   of all possible outcomes is finite  although we will try to make mathematical statements as general as possible|11|arxiv|0|introduction|AIM
the reader who is only interested in the main ideas might choose to specialize theorems  and  and their proofs to the case of finite|12|arxiv|0|introduction|OWN
 Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms|0|arxiv|1|abstract|MISC
 There are two known varieties of defensive forecasting: ``continuous'', in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and ``randomized'', in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized|1|arxiv|1|abstract|MISC
 This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous|2|arxiv|1|abstract|AIM
 New as compared to version 1 (17 August 2007) of this report: The assumption of version 1 that the outcome space  SYMBOL  is finite is relaxed, and now it is only assumed to be compact|3|arxiv|1|abstract|OWN
 In the case where  SYMBOL  is finite, it is shown that Forecaster can choose his randomized forecasts concentrated on a finite set of cardinality at most  SYMBOL|4|arxiv|1|abstract|OWN
 The continuous variety of defensive forecasting was essentially introduced by Levin  CITATION , but was later rediscovered by Kakade and Foster  CITATION  and Takemura  et al CITATION|5|arxiv|1|introduction|MISC
 The randomized variety was introduced (in the case of von Mises's version of the game-theoretic approach to probability) by Foster and Vohra  CITATION  and further developed by, among others, Sandroni  et al CITATION ; these papers, however, were only concerned with asymptotic calibration|6|arxiv|1|introduction|MISC
 Non-asymptotic versions of the randomized variety were proposed by Sandroni  CITATION  (based on standard measure-theoretic probability) and Vovk and Shafer  CITATION  (based on game-theoretic probability)|7|arxiv|1|introduction|MISC
 Kakade and Foster  CITATION  noticed that some calibration results require very little randomization (this will be an important aspect of our Theorem )|8|arxiv|1|introduction|BASE
 This note states two simple results about defensive forecasting, Theorem  about the continuous variety and Theorem  about the randomized variety|9|arxiv|1|introduction|AIM
 The proof of Theorem  is obtained from the proof of Theorem  by blurring Sceptic's moves|10|arxiv|1|introduction|OWN
 In our informal discussions we will be assuming that the set  SYMBOL  of all possible outcomes is finite, although we will try to make mathematical statements as general as possible|11|arxiv|1|introduction|OWN
 The reader who is only interested in the main ideas might choose to specialize Theorems  and  and their proofs to the case of finite  SYMBOL|12|arxiv|1|introduction|MISC
most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used  independently of any algorithm|0|arxiv|0|abstract|CONT
in contrast  the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties|1|arxiv|0|abstract|CONT
however  as in much of learning theory  existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed|2|arxiv|0|abstract|CONT
in many machine learning applications  however  this assumption does not hold|3|arxiv|0|abstract|CONT
the observations received by the learning algorithm often have some inherent temporal dependence|4|arxiv|0|abstract|CONT
this paper studies the scenario where the observations are drawn from a stationary   mixing or   mixing sequence  a widely adopted assumption in the study of non independent and identically distributed   processes that implies a dependence between observations weakening over time|5|arxiv|0|abstract|AIM
we prove novel and distinct stability based generalization bounds for stationary   mixing and   mixing sequences|6|arxiv|0|abstract|OWN
these bounds strictly generalize the bounds given in the independent and identically distributed   case and apply to all stable learning algorithms  thereby extending the use of stability bounds to non independent and identically distributed   scenarios|7|arxiv|0|abstract|BASE
we also illustrate the application of our   mixing generalization bounds to general classes of learning algorithms  including support vector regression  kernel ridge regression  and support vector machines  and many other kernel regularization based and relative entropy based regularization algorithms|8|arxiv|0|abstract|OWN
these novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non independent and identically distributed   scenarios   |9|arxiv|0|abstract|OWN
most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used  such as the vc dimension  covering numbers  or rademacher complexity|10|arxiv|0|introduction|CONT
these measures characterize a class of hypotheses  independently of any algorithm|11|arxiv|0|introduction|CONT
in contrast  the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties|12|arxiv|0|introduction|CONT
a learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set|13|arxiv|0|introduction|CONT
algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION|14|arxiv|0|introduction|CONT
but  as in much of learning theory  existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed  independent and identically distributed  |15|arxiv|0|introduction|CONT
in many machine learning applications  this assumption  however  does not hold  in fact  the independent and identically distributed   assumption is not tested or derived from any data analysis|16|arxiv|0|introduction|CONT
the observations received by the learning algorithm often have some inherent temporal dependence|17|arxiv|0|introduction|CONT
this is clear in system diagnosis or time series prediction problems|18|arxiv|0|introduction|CONT
clearly  prices of different stocks on the same day  or of the same stock on different days  may be dependent|19|arxiv|0|introduction|CONT
but  a less apparent time dependency may affect data sampled in many other tasks as well|20|arxiv|0|introduction|CONT
this paper studies the scenario where the observations are drawn from a stationary   mixing or   mixing sequence  a widely adopted assumption in the study of non independent and identically distributed   processes that implies a dependence between observations weakening over time  CITATION|21|arxiv|0|introduction|AIM
we prove novel and distinct stability based generalization bounds for stationary   mixing and   mixing sequences|22|arxiv|0|introduction|OWN
these bounds strictly generalize the bounds given in the independent and identically distributed   case and apply to all stable learning algorithms  thereby extending the usefulness of stability bounds to non independent and identically distributed   scenarios|23|arxiv|0|introduction|BASE
our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION   which is commonly used in such contexts|24|arxiv|0|introduction|BASE
however  our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size|25|arxiv|0|introduction|CONT
for our analysis of stationary   mixing sequences  we make use of a generalized version of mcdiarmid s inequality  CITATION  that holds for   mixing sequences|26|arxiv|0|introduction|BASE
this leads to stability based generalization bounds with the standard exponential form|27|arxiv|0|introduction|BASE
our generalization bounds for stationary   mixing sequences cover a more general non independent and identically distributed   scenario and use the standard mcdiarmid s inequality  however  unlike the   mixing case  the   mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient|28|arxiv|0|introduction|OWN
we also illustrate the application of our   mixing generalization bounds to general classes of learning algorithms  including support vector regression  svr   CITATION   kernel ridge regression  CITATION   and support vector machines  svms   CITATION|29|arxiv|0|introduction|OWN
algorithms such as support vector regression  svr   CITATION  have been used in the context of time series prediction in which the independent and identically distributed   assumption does not hold  some with good experimental results  CITATION|30|arxiv|0|introduction|CONT
to our knowledge  the use of these algorithms in non independent and identically distributed   scenarios has not been previously supported by any theoretical analysis|31|arxiv|0|introduction|CONT
the stability bounds we give for svr  svms  and many other kernel regularization based and relative entropy based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios|32|arxiv|0|introduction|CONT
the following sections are organized as follows|33|arxiv|0|introduction|OWN
in section   we introduce the necessary definitions for the non independent and identically distributed   problems that we are considering and discuss the learning scenarios in that context|34|arxiv|0|introduction|OWN
section  gives our main generalization bounds for stationary   mixing sequences based on stability  as well as the illustration of its applications to general kernel regularization based algorithms  including svr  krr  and svms  as well as to relative entropy based regularization algorithms|35|arxiv|0|introduction|OWN
finally  section  presents the first known stability bounds for the more general stationary   mixing scenario|36|arxiv|0|introduction|OWN
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm|0|arxiv|0|abstract|MISC
 In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties|1|arxiv|0|abstract|MISC
 However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed|2|arxiv|0|abstract|MISC
 In many machine learning applications, however, this assumption does not hold|3|arxiv|0|abstract|MISC
 The observations received by the learning algorithm often have some inherent temporal dependence|4|arxiv|0|abstract|MISC
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid  processes that implies a dependence between observations weakening over time|5|arxiv|0|abstract|AIM
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences|6|arxiv|0|abstract|AIM
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-iid scenarios|7|arxiv|0|abstract|OWN
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms|8|arxiv|0|abstract|OWN
 These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-iid scenarios|9|arxiv|0|abstract|OWN
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, such as the VC-dimension, covering numbers, or Rademacher complexity|10|arxiv|0|introduction|MISC
 These measures characterize a class of hypotheses, independently of any algorithm|11|arxiv|0|introduction|MISC
 In contrast, the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties|12|arxiv|0|introduction|MISC
 A learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set|13|arxiv|0|introduction|MISC
 Algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION|14|arxiv|0|introduction|MISC
 But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (iid)|15|arxiv|0|introduction|MISC
 In many machine learning applications, this assumption, however, does not hold; in fact, the iid assumption is not tested or derived from any data analysis|16|arxiv|0|introduction|MISC
 The observations received by the learning algorithm often have some inherent temporal dependence|17|arxiv|0|introduction|MISC
 This is clear in system diagnosis or time series prediction problems|18|arxiv|0|introduction|MISC
 Clearly, prices of different stocks on the same day, or of the same stock on different days, may be dependent|19|arxiv|0|introduction|MISC
 But, a less apparent time dependency may affect data sampled in many other tasks as well|20|arxiv|0|introduction|MISC
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid processes that implies a dependence between observations weakening over time  CITATION|21|arxiv|0|introduction|AIM
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences|22|arxiv|0|introduction|AIM
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the usefulness of stability-bounds to non-iid scenarios|23|arxiv|0|introduction|OWN
 Our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION , which is commonly used in such contexts|24|arxiv|0|introduction|BASE
 However, our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size|25|arxiv|0|introduction|CONT
 For our analysis of stationary  SYMBOL -mixing sequences, we make use of a generalized version of McDiarmid's inequality  CITATION  that holds for  SYMBOL -mixing sequences|26|arxiv|0|introduction|BASE
 This leads to stability-based generalization bounds with the standard exponential form|27|arxiv|0|introduction|BASE
 Our generalization bounds for stationary  SYMBOL -mixing sequences cover a more general non-iid scenario and use the standard McDiarmid's inequality, however, unlike the  SYMBOL -mixing case, the  SYMBOL -mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient|28|arxiv|0|introduction|CONT
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression (SVR)  CITATION , Kernel Ridge Regression  CITATION , and Support Vector Machines (SVMs)  CITATION|29|arxiv|0|introduction|OWN
 Algorithms such as support vector regression (SVR)  CITATION  have been used in the context of time series prediction in which the iid assumption does not hold, some with good experimental results  CITATION|30|arxiv|0|introduction|OWN
 To our knowledge, the use of these algorithms in non-iid scenarios has not been previously supported by any theoretical analysis|31|arxiv|0|introduction|CONT
 The stability bounds we give for SVR, SVMs, and many other kernel regularization-based and relative entropy-based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios|32|arxiv|0|introduction|OWN
 The following sections are organized as follows|33|arxiv|0|introduction|OWN
 In Section~, we introduce the necessary definitions for the non-iid problems that we are considering and discuss the learning scenarios in that context|34|arxiv|0|introduction|OWN
 Section~ gives our main generalization bounds for stationary  SYMBOL -mixing sequences based on stability, as well as the illustration of its applications to general kernel regularization-based algorithms, including SVR, KRR, and SVMs, as well as to relative entropy-based regularization algorithms|35|arxiv|0|introduction|OWN
 Finally, Section~ presents the first known stability bounds for the more general stationary  SYMBOL -mixing scenario |36|arxiv|0|introduction|OWN
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm|0|arxiv|1|abstract|MISC
 In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties|1|arxiv|1|abstract|MISC
 However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed|2|arxiv|1|abstract|MISC
 In many machine learning applications, however, this assumption does not hold|3|arxiv|1|abstract|MISC
 The observations received by the learning algorithm often have some inherent temporal dependence|4|arxiv|1|abstract|MISC
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid  processes that implies a dependence between observations weakening over time|5|arxiv|1|abstract|AIM
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences|6|arxiv|1|abstract|OWN
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-iid scenarios|7|arxiv|1|abstract|OWN
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms|8|arxiv|1|abstract|OWN
 These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-iid scenarios|9|arxiv|1|abstract|OWN
 Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, such as the VC-dimension, covering numbers, or Rademacher complexity|10|arxiv|1|introduction|MISC
 These measures characterize a class of hypotheses, independently of any algorithm|11|arxiv|1|introduction|MISC
 In contrast, the notion of algorithmic stability can be used to derive bounds that are tailored to specific learning algorithms and exploit their particular properties|12|arxiv|1|introduction|MISC
 A learning algorithm is stable if the hypothesis it outputs varies in a limited way in response to small changes made to the training set|13|arxiv|1|introduction|MISC
 Algorithmic stability has been used effectively in the past to derive tight generalization bounds  CITATION|14|arxiv|1|introduction|MISC
 But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (iid)|15|arxiv|1|introduction|MISC
 In many machine learning applications, this assumption, however, does not hold; in fact, the iid assumption is not tested or derived from any data analysis|16|arxiv|1|introduction|MISC
 The observations received by the learning algorithm often have some inherent temporal dependence|17|arxiv|1|introduction|MISC
 This is clear in system diagnosis or time series prediction problems|18|arxiv|1|introduction|MISC
 Clearly, prices of different stocks on the same day, or of the same stock on different days, may be dependent|19|arxiv|1|introduction|MISC
 But, a less apparent time dependency may affect data sampled in many other tasks as well|20|arxiv|1|introduction|MISC
 This paper studies the scenario where the observations are drawn from a stationary  SYMBOL -mixing or  SYMBOL -mixing sequence, a widely adopted assumption in the study of non-iid processes that implies a dependence between observations weakening over time  CITATION|21|arxiv|1|introduction|AIM
 We prove novel and distinct stability-based generalization bounds for stationary  SYMBOL -mixing and  SYMBOL -mixing sequences|22|arxiv|1|introduction|OWN
 These bounds strictly generalize the bounds given in the iid case and apply to all stable learning algorithms, thereby extending the usefulness of stability-bounds to non-iid scenarios|23|arxiv|1|introduction|OWN
 Our proofs are based on the independent block technique described by  CITATION  and attributed to  CITATION , which is commonly used in such contextsMISC|24|arxiv|1|introduction|BASE
 However, our analysis differs from previous uses of this technique in that the blocks of points considered are not of equal size|25|arxiv|1|introduction|CONT
 For our analysis of stationary  SYMBOL -mixing sequences, we make use of a generalized version of McDiarmid's inequality  CITATION  that holds for  SYMBOL -mixing sequences|26|arxiv|1|introduction|BASE
 This leads to stability-based generalization bounds with the standard exponential form|27|arxiv|1|introduction|OWN
 Our generalization bounds for stationary  SYMBOL -mixing sequences cover a more general non-iid scenario and use the standard McDiarmid's inequality, however, unlike the  SYMBOL -mixing case, the  SYMBOL -mixing bound presented here is not a purely exponential bound and contains an additive term depending on the mixing coefficient|28|arxiv|1|introduction|OWN
 We also illustrate the application of our  SYMBOL -mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression (SVR)  CITATION , Kernel Ridge Regression  CITATION , and Support Vector Machines (SVMs)  CITATION|29|arxiv|1|introduction|OWN
 Algorithms such as support vector regression (SVR)  CITATION  have been used in the context of time series prediction in which the iid assumption does not hold, some with good experimental results  CITATION|30|arxiv|1|introduction|MISC
 To our knowledge, the use of these algorithms in non-iid scenarios has not been previously supported by any theoretical analysis|31|arxiv|1|introduction|MISC
 The stability bounds we give for SVR, SVMs, and many other kernel regularization-based and relative entropy-based regularization algorithms can thus be viewed as the first theoretical basis for their use in such scenarios|32|arxiv|1|introduction|OWN
 The following sections are organized as follows|33|arxiv|1|introduction|MISC
 In Section~, we introduce the necessary definitions for the non-iid problems that we are considering and discuss the learning scenarios in that context|34|arxiv|1|introduction|OWN
 Section~ gives our main generalization bounds for stationary  SYMBOL -mixing sequences based on stability, as well as the illustration of its applications to general kernel regularization-based algorithms, including SVR, KRR, and SVMs, as well as to relative entropy-based regularization algorithms|35|arxiv|1|introduction|OWN
 Finally, Section~ presents the first known stability bounds for the more general stationary  SYMBOL -mixing scenario|36|arxiv|1|introduction|OWN
this paper studies quantum annealing  qa  for clustering  which can be seen as an extension of simulated annealing  sa |0|arxiv|0|abstract|AIM
we derive a qa algorithm for clustering and propose an annealing schedule  which is crucial in practice|1|arxiv|0|abstract|OWN
experiments show the proposed qa algorithm finds better clustering assignments than sa|2|arxiv|0|abstract|CONT
furthermore  qa is as easy as sa to implement|3|arxiv|0|abstract|CONT
clustering is one of the most popular methods in data mining|4|arxiv|0|introduction|MISC
typically  clustering problems are formulated as optimization problems  which are solved by algorithms  for example the em algorithm or convex relaxation|5|arxiv|0|introduction|MISC
however  clustering is typically np hard|6|arxiv|0|introduction|MISC
the simulated annealing  sa   CITATION  is a promising candidate|7|arxiv|0|introduction|MISC
CITATION  proved sa was able to find the global optimum with a slow cooling schedule of temperature|8|arxiv|0|introduction|MISC
although their schedule is in practice too slow for clustering of a large amount of data  it is well known that sa still finds a reasonably good solution even with a faster schedule than what  citeauthor geman NUMBER   proposed|9|arxiv|0|introduction|CONT
in statistical mechanics  quantum annealing  qa  has been proposed as a novel alternative to sa  CITATION|10|arxiv|0|introduction|CONT
qa adds another dimension     to sa for annealing  see fig|11|arxiv|0|introduction|CONT
thus  it can be seen as an extension of sa|12|arxiv|0|introduction|BASE
qa has succeeded in specific problems  e g the ising model in statistical mechanics  and it is still unclear that qa works better than sa in general|13|arxiv|0|introduction|MISC
we do not actually think qa intuitively helps clustering  but we apply qa to clustering just as procedure to derive an algorithm|14|arxiv|0|introduction|OWN
a derived qa algorithm depends on the definition of quantum effect|15|arxiv|0|introduction|MISC
we propose quantum effect    which leads to a search strategy fit to clustering|16|arxiv|0|introduction|OWN
our contribution is 1) to propose a qa based optimization algorithm for clustering in particular quantum effect for clustering and a good annealing schedule  which is crucial for applications 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than sa|17|arxiv|0|introduction|AIM
we also show the proposed algorithm is as easy as sa to implement |18|arxiv|0|introduction|OWN
 	the algorithm we propose is a markov chain monte carlo  mcmc  sampler  which we call qa st sampler|19|arxiv|0|introduction|OWN
as we explain later  a naive qa sampler is intractable even with mcmc|20|arxiv|0|introduction|OWN
thus  we approximate qa by the suzuki trotter  st  expansion  CITATION  to derive a tractable sampler  which is the qa st sampler|21|arxiv|0|introduction|OWN
qa st looks like parallel   sas with interaction    see fig  |22|arxiv|0|introduction|OWN
at the beginning of the annealing process  qa st is almost the same as   sas|23|arxiv|0|introduction|OWN
hence  qa st finds    local  optima independently|24|arxiv|0|introduction|OWN
as the annealing process continues  interaction   in fig becomes stronger to move   states closer|25|arxiv|0|introduction|OWN
qa st at the end picks up the state with the lowest energy in   states as the final solution|26|arxiv|0|introduction|OWN
qa st with the proposed quantum effect   works well for clustering|27|arxiv|0|introduction|OWN
fig is an example where data points are grouped into four clusters|28|arxiv|0|introduction|OWN
SYMBOL and SYMBOL are locally optimal and   is globally optimal|29|arxiv|0|introduction|OWN
suppose SYMBOL is equal to two and SYMBOL and SYMBOL in fig correspond to SYMBOL and SYMBOL in fig|30|arxiv|0|introduction|OWN
although   and   are local optima  the interaction   in fig allows   and   to search for a better clustering assignment between   and|31|arxiv|0|introduction|OWN
quantum effect   defines the distance metric of clustering assignments|32|arxiv|0|introduction|OWN
in this case  the proposed   locates   between   and|33|arxiv|0|introduction|OWN
thus  the interaction   gives good chance to go to   because   makes   and   closer  see fig  |34|arxiv|0|introduction|OWN
the proposed algorithm actually finds   from   and|35|arxiv|0|introduction|OWN
fig is just an example|36|arxiv|0|introduction|OWN
however  a similar situation often occurs in clustering|37|arxiv|0|introduction|MISC
clustering algorithms in most cases give   almost   globally optimal solutions like   and    where the majority of data points are well clustered  but some of them are not|38|arxiv|0|introduction|MISC
thus  a better clustering assignment can be constructed by picking up well clustered data points from many sub optimal clustering assignments|39|arxiv|0|introduction|MISC
note an assignment constructed in such a way is located between the sub optimal ones by the proposed quantum effect   so that qa st can find a better assignment between sub optimal ones|40|arxiv|0|introduction|MISC
This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA)|0|arxiv|0|abstract|AIM
We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice|1|arxiv|0|abstract|AIM
Experiments show the proposed QA algorithm finds better clustering assignments than SA|2|arxiv|0|abstract|OWN
Furthermore, QA is as easy as SA to implement|3|arxiv|0|abstract|OWN
Clustering is one of the most popular methods in data mining|4|arxiv|0|introduction|MISC
Typically, clustering problems are formulated as optimization problems, which are solved by algorithms, for example the EM algorithm or convex relaxation|5|arxiv|0|introduction|MISC
However, clustering is typically NP-hard|6|arxiv|0|introduction|MISC
The simulated annealing (SA)  CITATION  is a promising candidate|7|arxiv|0|introduction|MISC
CITATION  proved SA was able to find the global optimum with a slow cooling schedule of temperature  SYMBOL|8|arxiv|0|introduction|MISC
Although their schedule is in practice too slow for clustering of a large amount of data, it is well known that SA still finds a reasonably good solution even with a faster schedule than what CITATION proposed|9|arxiv|0|introduction|MISC
In statistical mechanics, quantum annealing (QA) has been proposed as a novel alternative to SA  CITATION|10|arxiv|0|introduction|MISC
QA adds another dimension,  SYMBOL , to SA for annealing, see Fig|11|arxiv|0|introduction|MISC
Thus, it can be seen as an extension of SA|12|arxiv|0|introduction|MISC
QA has succeeded in specific problems, e g the Ising model in statistical mechanics, and it is still unclear that QA works better than SA in general|13|arxiv|0|introduction|MISC
We do not actually think QA intuitively helps clustering, but we apply QA to clustering just as procedure to derive an algorithm|14|arxiv|0|introduction|OWN
A derived QA algorithm depends on the definition of quantum effect  SYMBOL|15|arxiv|0|introduction|MISC
We propose quantum effect  SYMBOL , which leads to a search strategy fit to clustering|16|arxiv|0|introduction|AIM
Our contribution is, 1) to propose a QA-based optimization algorithm for clustering, in particular quantum effect  SYMBOL  for clustering  and a good annealing schedule, which is crucial for applications, 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than SA|17|arxiv|0|introduction|AIM
We also show the proposed algorithm is as easy as SA to implement|18|arxiv|0|introduction|OWN
The algorithm we propose is a Markov chain Monte Carlo (MCMC) sampler, which we call QA-ST sampler|19|arxiv|0|introduction|OWN
As we explain later, a naive QA sampler is intractable even with MCMC|20|arxiv|0|introduction|OWN
Thus, we approximate QA by the Suzuki-Trotter (ST) expansion  CITATION  to derive a tractable sampler, which is the QA-ST sampler|21|arxiv|0|introduction|AIM
QA-ST looks like parallel  SYMBOL  SAs with interaction  SYMBOL  (see Fig )|22|arxiv|0|introduction|BASE
At the beginning of the annealing process, QA-ST is almost the same as  SYMBOL  SAs|23|arxiv|0|introduction|BASE
Hence, QA-ST finds  SYMBOL  (local) optima independently|24|arxiv|0|introduction|OWN
As the annealing process continues, interaction  SYMBOL  in Fig becomes stronger to move  SYMBOL  states closer|25|arxiv|0|introduction|OWN
QA-ST at the end picks up the state with the lowest energy in  SYMBOL  states as the final solution|26|arxiv|0|introduction|OWN
QA-ST with the proposed quantum effect  SYMBOL  works well for clustering|27|arxiv|0|introduction|OWN
Fig is an example where data points are grouped into four clusters|28|arxiv|0|introduction|OWN
SYMBOL and  SYMBOL are locally optimal and  SYMBOL  is globally optimal|29|arxiv|0|introduction|OWN
Suppose  SYMBOL  is equal to two and  SYMBOL  and  SYMBOL  in Fig correspond to  SYMBOL  and  SYMBOL  in Fig|30|arxiv|0|introduction|OWN
Although  SYMBOL  and  SYMBOL  are local optima, the interaction  SYMBOL  in Fig allows  SYMBOL  and  SYMBOL  to search for a better clustering assignment between  SYMBOL  and  SYMBOL|31|arxiv|0|introduction|OWN
Quantum effect  SYMBOL  defines the distance metric of clustering assignments|32|arxiv|0|introduction|OWN
In this case, the proposed  SYMBOL  locates  SYMBOL  between  SYMBOL  and  SYMBOL|33|arxiv|0|introduction|OWN
Thus, the interaction  SYMBOL  gives good chance to go to  SYMBOL  because  SYMBOL  makes  SYMBOL  and  SYMBOL  closer (see Fig )|34|arxiv|0|introduction|OWN
The proposed algorithm actually finds  SYMBOL  from  SYMBOL  and  SYMBOL|35|arxiv|0|introduction|OWN
Fig is just an example|36|arxiv|0|introduction|OWN
However, a similar situation often occurs in clustering|37|arxiv|0|introduction|MISC
Clustering algorithms in most cases give ``almost'' globally optimal solutions like  SYMBOL  and  SYMBOL , where the majority of data points are well-clustered, but some of them are not|38|arxiv|0|introduction|CONT
Thus, a better clustering assignment can be constructed by picking up well-clustered data points from many sub-optimal clustering assignments|39|arxiv|0|introduction|CONT
Note an assignment constructed in such a way is located between the sub-optimal ones by the proposed quantum effect  SYMBOL  so that QA-ST can find a better assignment between sub-optimal ones|40|arxiv|0|introduction|CONT
 This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA)|0|arxiv|1|abstract|OWN
 We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice|1|arxiv|1|abstract|AIM
 Experiments show the proposed QA algorithm finds better clustering assignments than SA|2|arxiv|1|abstract|OWN
 Furthermore, QA is as easy as SA to implement|3|arxiv|1|abstract|OWN
 Clustering is one of the most popular methods in data mining|4|arxiv|1|introduction|MISC
 Typically, clustering problems are formulated as optimization problems, which are solved by algorithms, for example the EM algorithm or convex relaxation|5|arxiv|1|introduction|MISC
 However, clustering is typically NP-hard|6|arxiv|1|introduction|MISC
 The simulated annealing (SA)  CITATION  is a promising candidate|7|arxiv|1|introduction|MISC
 CITATION  proved SA was able to find the global optimum with a slow cooling schedule of temperature  SYMBOL|8|arxiv|1|introduction|MISC
 Although their schedule is in practice too slow for clustering of a large amount of data, it is well known that SA still finds a reasonably good solution even with a faster schedule than what CITATION proposed|9|arxiv|1|introduction|MISC
 In statistical mechanics, quantum annealing (QA) has been proposed as a novel alternative to SA  CITATION|10|arxiv|1|introduction|MISC
 QA adds another dimension,  SYMBOL , to SA for annealing, see Fig|11|arxiv|1|introduction|MISC
 Thus, it can be seen as an extension of SA|12|arxiv|1|introduction|MISC
 QA has succeeded in specific problems, e g the Ising model in statistical mechanics, and it is still unclear that QA works better than SA in general|13|arxiv|1|introduction|MISC
 We do not actually think QA intuitively helps clustering, but we apply QA to clustering just as procedure to derive an algorithm|14|arxiv|1|introduction|OWN
 A derived QA algorithm depends on the definition of quantum effect  SYMBOL|15|arxiv|1|introduction|MISC
 We propose quantum effect  SYMBOL , which leads to a search strategy fit to clustering|16|arxiv|1|introduction|AIM
 Our contribution is, 1) to propose a QA-based optimization algorithm for clustering, in particular quantum effect  SYMBOL  for clustering  and a good annealing schedule, which is crucial for applications, 2) and to experimentally show the proposed algorithm optimizes clustering assignments better than SA|17|arxiv|1|introduction|AIM
 We also show the proposed algorithm is as easy as SA to implement|18|arxiv|1|introduction|OWN
 The algorithm we propose is a Markov chain Monte Carlo (MCMC) sampler, which we call QA-ST sampler|19|arxiv|1|introduction|OWN
 As we explain later, a naive QA sampler is intractable even with MCMC|20|arxiv|1|introduction|MISC
 Thus, we approximate QA by the Suzuki-Trotter (ST) expansion  CITATION  to derive a tractable sampler, which is the QA-ST sampler|21|arxiv|1|introduction|OWN
 QA-ST looks like parallel  SYMBOL  SAs with interaction  SYMBOL  (see Fig )|22|arxiv|1|introduction|OWN
 At the beginning of the annealing process, QA-ST is almost the same as  SYMBOL  SAs|23|arxiv|1|introduction|OWN
 Hence, QA-ST finds  SYMBOL  (local) optima independently|24|arxiv|1|introduction|OWN
 As the annealing process continues, interaction  SYMBOL  in Fig becomes stronger to move  SYMBOL  states closer|25|arxiv|1|introduction|OWN
 QA-ST at the end picks up the state with the lowest energy in  SYMBOL  states as the final solution|26|arxiv|1|introduction|OWN
 QA-ST with the proposed quantum effect  SYMBOL  works well for clustering|27|arxiv|1|introduction|OWN
 Fig is an example where data points are grouped into four clusters|28|arxiv|1|introduction|MISC
 SYMBOL and  SYMBOL are locally optimal and  SYMBOL  is globally optimal|29|arxiv|1|introduction|MISC
 Suppose  SYMBOL  is equal to two and  SYMBOL  and  SYMBOL  in Fig correspond to  SYMBOL  and  SYMBOL  in Fig|30|arxiv|1|introduction|MISC
 Although  SYMBOL  and  SYMBOL  are local optima, the interaction  SYMBOL  in Fig allows  SYMBOL  and  SYMBOL  to search for a better clustering assignment between  SYMBOL  and  SYMBOL|31|arxiv|1|introduction|MISC
 Quantum effect  SYMBOL  defines the distance metric of clustering assignments|32|arxiv|1|introduction|MISC
 In this case, the proposed  SYMBOL  locates  SYMBOL  between  SYMBOL  and  SYMBOL|33|arxiv|1|introduction|MISC
 Thus, the interaction  SYMBOL  gives good chance to go to  SYMBOL  because  SYMBOL  makes  SYMBOL  and  SYMBOL  closer (see Fig )|34|arxiv|1|introduction|MISC
 The proposed algorithm actually finds  SYMBOL  from  SYMBOL  and  SYMBOL|35|arxiv|1|introduction|OWN
 Fig is just an example|36|arxiv|1|introduction|MISC
 However, a similar situation often occurs in clustering|37|arxiv|1|introduction|MISC
 Clustering algorithms in most cases give ``almost'' globally optimal solutions like  SYMBOL  and  SYMBOL , where the majority of data points are well-clustered, but some of them are not|38|arxiv|1|introduction|MISC
 Thus, a better clustering assignment can be constructed by picking up well-clustered data points from many sub-optimal clustering assignments|39|arxiv|1|introduction|MISC
 Note an assignment constructed in such a way is located between the sub-optimal ones by the proposed quantum effect  SYMBOL  so that QA-ST can find a better assignment between sub-optimal ones|40|arxiv|1|introduction|OWN
we introduce a new principle for model selection in regression and classification|0|arxiv|0|abstract|AIM
many regression models are controlled by some smoothness or flexibility or complexity parameter    e g   the number of neighbors to be averaged over in k nearest neighbor  knn  regression or the polynomial degree in regression with polynomials|1|arxiv|0|abstract|MISC
let   be the  best  regressor of complexity   on data|2|arxiv|0|abstract|OWN
a more flexible regressor can fit more data   well than a more rigid one|3|arxiv|0|abstract|MISC
if something  here small loss  is easy to achieve it s typically worth less|4|arxiv|0|abstract|MISC
we define the loss rank of   as the number of other  fictitious  data   that are fitted better by   than   is fitted by|5|arxiv|0|abstract|OWN
we suggest selecting the model complexity   that has minimal loss rank  lorp |6|arxiv|0|abstract|OWN
unlike most penalized maximum likelihood variants  aic bic mdl   lorp only depends on the regression functions and the loss function|7|arxiv|0|abstract|MISC
it works without a stochastic noise model  and is directly applicable to any non parametric regressor  like knn|8|arxiv|0|abstract|MISC
in this paper we formalize  discuss  and motivate lorp  study it for specific regression problems  in particular linear ones  and compare it to other model selection schemes|9|arxiv|0|abstract|AIM
consider a regression or classification problem in which we want to determine the functional relationship   from data    i e   we seek a function   such that   is close to the unknown   for all|10|arxiv|0|introduction|OWN
one may define regressor   directly  e g    average the   values of the   nearest neighbors  knn  of   in     or select the   from a class of functions   that has smallest  training  error on|11|arxiv|0|introduction|MISC
if the class   is not too large  e g the polynomials of fixed reasonable degree    this often works well|12|arxiv|0|introduction|MISC
what remains is to select the right model complexity    like   or|13|arxiv|0|introduction|MISC
this selection cannot be based on the training error  since the more complex the model  large    small    the better the fit on    perfect for   and   |14|arxiv|0|introduction|MISC
this problem is called overfitting  for which various remedies have been suggested   we will not discuss empirical test set methods like cross validation  but only training set based methods|15|arxiv|0|introduction|OWN
see e g    CITATION  for a comparison of cross validation with bayesian model selection|16|arxiv|0|introduction|MISC
training set based model selection methods allow using all data   for regression|17|arxiv|0|introduction|MISC
the most popular ones can be regarded as penalized versions of maximum likelihood  ml |18|arxiv|0|introduction|MISC
in addition to the function class    one has to specify a sampling model    e g   that the   have independent gaussian distribution with mean|19|arxiv|0|introduction|MISC
ml chooses    penalized ml  pml  then chooses  penalty   where the penalty depends on the used approach  mdl  CITATION   bic  CITATION   aic  CITATION  |20|arxiv|0|introduction|MISC
in particular  modern mdl  CITATION  has sound exact foundations and works very well in practice|21|arxiv|0|introduction|MISC
all pml variants rely on a proper sampling model  which may be difficult to establish   ignore  or at least do not tell how to incorporate  a potentially given loss function  and are typically limited to  semi parametric models|22|arxiv|0|introduction|CONT
the main goal of the paper is to establish a criterion for selecting the   best   model complexity   % based on regressors   given as a black box without insight into the origin or inner structure of    % that does not depend on things often not given  like a stochastic noise model   % and that exploits what is given  like the loss function |23|arxiv|0|introduction|AIM
the key observation we exploit is that large classes   or more flexible regressors   can fit more data   well than more rigid ones  e g   many   can be fit well with high order polynomials|24|arxiv|0|introduction|OWN
we define the  loss rank  of   as the number of other  fictitious  data   that are fitted better by   than   is fitted by    as measured by some loss function|25|arxiv|0|introduction|OWN
the loss rank is large for regressors fitting   not well  and  for too flexible regressors  in both cases the regressor fits many other   better |26|arxiv|0|introduction|MISC
the loss rank has a minimum for not too flexible regressors which fit   not too bad|27|arxiv|0|introduction|MISC
we claim that minimizing the loss rank is a suitable model selection criterion  since it trades off the quality of fit with the flexibility of the model|28|arxiv|0|introduction|OWN
unlike pml  our new loss rank principle  lorp  works without a noise  stochastic sampling  model  and is directly applicable to any non parametric regressor  like knn|29|arxiv|0|introduction|CONT
in section   after giving a brief introduction to regression  we formally state lorp for model selection|30|arxiv|0|introduction|OWN
to make it applicable to real problems  we have to generalize it to continuous spaces and regularize infinite loss ranks|31|arxiv|0|introduction|OWN
in section  we derive explicit expressions for the loss rank for the important class of linear regressors  which includes knn  polynomial  linear basis function  lbfr   kernel  and projective regression|32|arxiv|0|introduction|OWN
in section  we compare linear lorp to bayesian model selection for linear regression with gaussian noise and prior  and in section  to pml  in particular mdl  bic  aic  and mackay s  CITATION  and hastie s et al    CITATION  trace formulas for the effective dimension|33|arxiv|0|introduction|OWN
in this paper we just scratch at the surface of lorp|34|arxiv|0|introduction|AIM
section  contains further considerations  to be elaborated on in the future|35|arxiv|0|introduction|OWN
 We introduce a new principle for model selection in regression and classification|0|arxiv|0|abstract|OWN
 Many regression models are controlled by some smoothness or flexibility or complexity parameter  SYMBOL , eg the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials|1|arxiv|0|abstract|MISC
 Let  SYMBOL  be the (best) regressor of complexity  SYMBOL  on data  SYMBOL|2|arxiv|0|abstract|MISC
 A more flexible regressor can fit more data  SYMBOL  well than a more rigid one|3|arxiv|0|abstract|MISC
 If something (here small loss) is easy to achieve it's typically worth less|4|arxiv|0|abstract|MISC
 We define the loss rank of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL|5|arxiv|0|abstract|OWN
 We suggest selecting the model complexity  SYMBOL  that has minimal loss rank (LoRP)|6|arxiv|0|abstract|OWN
 Unlike most penalized maximum likelihood variants (AIC,BIC,MDL), LoRP only depends on the regression functions and the loss function|7|arxiv|0|abstract|CONT
 It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN|8|arxiv|0|abstract|CONT
 In this paper we formalize, discuss, and motivate LoRP, study it for specific regression problems, in particular linear ones, and compare it to other model selection schemes|9|arxiv|0|abstract|AIM
 Consider a regression or classification problem in which we want to determine the functional relationship  SYMBOL  from data  SYMBOL , ie we seek a function  SYMBOL  such that  SYMBOL  is close to the unknown  SYMBOL  for all  SYMBOL|10|arxiv|0|introduction|OWN
 One may define regressor  SYMBOL  directly, eg `average the  SYMBOL  values of the  SYMBOL  nearest neighbors (kNN) of  SYMBOL  in  SYMBOL ', or select the  SYMBOL  from a class of functions  SYMBOL  that has smallest (training) error on  SYMBOL|11|arxiv|0|introduction|OWN
 If the class  SYMBOL  is not too large, e g the polynomials of fixed reasonable degree  SYMBOL , this often works well|12|arxiv|0|introduction|OWN
 What remains is to select the right model complexity  SYMBOL , like  SYMBOL  or  SYMBOL|13|arxiv|0|introduction|OWN
 This selection cannot be based on the training error, since the more complex the model (large  SYMBOL , small  SYMBOL ) the better the fit on  SYMBOL  (perfect for  SYMBOL  and  SYMBOL )|14|arxiv|0|introduction|OWN
 This problem is called overfitting, for which various remedies have been suggested:  We will not discuss empirical test set methods like cross-validation, but only training set based methods|15|arxiv|0|introduction|MISC
 See eg CITATION  for a comparison of cross-validation with Bayesian model selection|16|arxiv|0|introduction|MISC
 Training set based model selection methods allow using all data  SYMBOL  for regression|17|arxiv|0|introduction|MISC
 The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML)|18|arxiv|0|introduction|MISC
 In addition to the function class  SYMBOL , one has to specify a sampling model  SYMBOL , eg that the  SYMBOL  have independent Gaussian distribution with mean  SYMBOL|19|arxiv|0|introduction|OWN
 ML chooses  SYMBOL , Penalized ML (PML) then chooses  SYMBOL Penalty SYMBOL , where the penalty depends on the used approach (MDL  CITATION , BIC  CITATION , AIC  CITATION )|20|arxiv|0|introduction|OWN
 In particular, modern MDL  CITATION  has sound exact foundations and works very well in practice|21|arxiv|0|introduction|OWN
 All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function, and are typically limited to (semi)parametric models|22|arxiv|0|introduction|MISC
 The main goal of the paper is to establish a criterion for selecting the ``best'' model complexity  SYMBOL based on regressors  SYMBOL  given as a black box without insight into the origin or inner structure of  SYMBOL , that does not depend on things often not given (like a stochastic noise model),  and that exploits what is given (like the loss function)|23|arxiv|0|introduction|AIM
 The key observation we exploit is that large classes  SYMBOL  or more flexible   regressors  SYMBOL  can fit more data  SYMBOL  well than more rigid ones, eg many  SYMBOL  can be fit well with high order polynomials|24|arxiv|0|introduction|OWN
 We define the  loss rank  of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL , as measured by some loss function|25|arxiv|0|introduction|OWN
 The loss rank is large for regressors fitting  SYMBOL  not well  and  for too flexible regressors (in both cases the regressor fits many other  SYMBOL  better)|26|arxiv|0|introduction|OWN
 The loss rank has a minimum for not too flexible regressors which fit  SYMBOL  not too bad|27|arxiv|0|introduction|OWN
 We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model|28|arxiv|0|introduction|OWN
 Unlike PML, our new Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN|29|arxiv|0|introduction|OWN
 In Section , after giving a brief introduction to regression, we formally state LoRP for model selection|30|arxiv|0|introduction|OWN
 To make it applicable to real problems, we have to generalize it to continuous spaces and regularize infinite loss ranks|31|arxiv|0|introduction|OWN
 In Section  we derive explicit expressions for the loss rank for the important class of linear regressors, which includes kNN, polynomial, linear basis function (LBFR), Kernel, and projective regression|32|arxiv|0|introduction|OWN
 In Section  we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section  to PML, in particular MDL, BIC, AIC, and MacKay's  CITATION  and Hastie's et al  CITATION  trace formulas for the effective dimension|33|arxiv|0|introduction|OWN
 In this paper we just scratch at the surface of LoRP|34|arxiv|0|introduction|OWN
 Section  contains further considerations, to be elaborated on in the future|35|arxiv|0|introduction|OWN
 We introduce a new principle for model selection in regression and classification|0|arxiv|1|abstract|AIM
 Many regression models are controlled by some smoothness or flexibility or complexity parameter  SYMBOL , eg the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials|1|arxiv|1|abstract|MISC
 Let  SYMBOL  be the (best) regressor of complexity  SYMBOL  on data  SYMBOL|2|arxiv|1|abstract|MISC
 A more flexible regressor can fit more data  SYMBOL  well than a more rigid one|3|arxiv|1|abstract|MISC
 If something (here small loss) is easy to achieve it's typically worth less|4|arxiv|1|abstract|MISC
 We define the loss rank of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL|5|arxiv|1|abstract|OWN
 We suggest selecting the model complexity  SYMBOL  that has minimal loss rank (LoRP)|6|arxiv|1|abstract|OWN
 Unlike most penalized maximum likelihood variants (AIC,BIC,MDL), LoRP only depends on the regression functions and the loss function|7|arxiv|1|abstract|CONT
 It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN|8|arxiv|1|abstract|CONT
 In this paper we formalize, discuss, and motivate LoRP, study it for specific regression problems, in particular linear ones, and compare it to other model selection schemes|9|arxiv|1|abstract|OWN
 Consider a regression or classification problem in which we want to determine the functional relationship  SYMBOL  from data  SYMBOL , ie we seek a function  SYMBOL  such that  SYMBOL  is close to the unknown  SYMBOL  for all  SYMBOL|10|arxiv|1|introduction|MISC
 One may define regressor  SYMBOL  directly, eg `average the  SYMBOL  values of the  SYMBOL  nearest neighbors (kNN) of  SYMBOL  in  SYMBOL ', or select the  SYMBOL  from a class of functions  SYMBOL  that has smallest (training) error on  SYMBOL|11|arxiv|1|introduction|MISC
 If the class  SYMBOL  is not too large, e g the polynomials of fixed reasonable degree  SYMBOL , this often works well|12|arxiv|1|introduction|MISC
 What remains is to select the right model complexity  SYMBOL , like  SYMBOL  or  SYMBOL|13|arxiv|1|introduction|MISC
 This selection cannot be based on the training error, since the more complex the model (large  SYMBOL , small  SYMBOL ) the better the fit on  SYMBOL  (perfect for  SYMBOL  and  SYMBOL )|14|arxiv|1|introduction|MISC
 This problem is called overfitting, for which various remedies have been suggested:  We will not discuss empirical test set methods like cross-validation, but only training set based methods|15|arxiv|1|introduction|MISC
 See eg CITATION  for a comparison of cross-validation with Bayesian model selection|16|arxiv|1|introduction|MISC
 Training set based model selection methods allow using all data  SYMBOL  for regression|17|arxiv|1|introduction|MISC
 The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML)|18|arxiv|1|introduction|MISC
 In addition to the function class  SYMBOL , one has to specify a sampling model  SYMBOL , eg that the  SYMBOL  have independent Gaussian distribution with mean  SYMBOL|19|arxiv|1|introduction|MISC
 ML chooses  SYMBOL , Penalized ML (PML) then chooses  SYMBOL Penalty SYMBOL , where the penalty depends on the used approach (MDL  CITATION , BIC  CITATION , AIC  CITATION )|20|arxiv|1|introduction|MISC
 In particular, modern MDL  CITATION  has sound exact foundations and works very well in practice|21|arxiv|1|introduction|MISC
 All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function, and are typically limited to (semi)parametric models|22|arxiv|1|introduction|CONT
 The main goal of the paper is to establish a criterion for selecting the ``best'' model complexity  SYMBOL based on regressors  SYMBOL  given as a black box without insight into the origin or inner structure of  SYMBOL , that does not depend on things often not given (like a stochastic noise model),  and that exploits what is given (like the loss function)|23|arxiv|1|introduction|AIM
 The key observation we exploit is that large classes  SYMBOL  or more flexible regressors  SYMBOL  can fit more data  SYMBOL  well than more rigid ones, eg many  SYMBOL  can be fit well with high order polynomials|24|arxiv|1|introduction|OWN
 We define the  loss rank  of  SYMBOL  as the number of other (fictitious) data  SYMBOL  that are fitted better by  SYMBOL  than  SYMBOL  is fitted by  SYMBOL , as measured by some loss function|25|arxiv|1|introduction|OWN
 The loss rank is large for regressors fitting  SYMBOL  not well  and  for too flexible regressors (in both cases the regressor fits many other  SYMBOL  better)|26|arxiv|1|introduction|OWN
 The loss rank has a minimum for not too flexible regressors which fit  SYMBOL  not too bad|27|arxiv|1|introduction|OWN
 We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model|28|arxiv|1|introduction|OWN
 Unlike PML, our new Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN|29|arxiv|1|introduction|CONT
 In Section , after giving a brief introduction to regression, we formally state LoRP for model selection|30|arxiv|1|introduction|OWN
 To make it applicable to real problems, we have to generalize it to continuous spaces and regularize infinite loss ranks|31|arxiv|1|introduction|OWN
 In Section  we derive explicit expressions for the loss rank for the important class of linear regressors, which includes kNN, polynomial, linear basis function (LBFR), Kernel, and projective regression|32|arxiv|1|introduction|OWN
 In Section  we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section  to PML, in particular MDL, BIC, AIC, and MacKay's  CITATION  and Hastie's et al  CITATION  trace formulas for the effective dimension|33|arxiv|1|introduction|OWN
 In this paper we just scratch at the surface of LoRP|34|arxiv|1|introduction|OWN
 Section  contains further considerations, to be elaborated on in the future|35|arxiv|1|introduction|OWN
we propose a nonparametric bayesian factor regression model that accounts for uncertainty in the number of factors  and the relationship between factors|0|arxiv|0|abstract|AIM
to accomplish this  we propose a sparse variant of the indian buffet process and couple this with a hierarchical model over factors  based on kingman s coalescent|1|arxiv|0|abstract|OWN
we apply this model to two problems  factor analysis and factor regression  in gene expression data analysis|2|arxiv|0|abstract|AIM
factor analysis is the task of explaining data by means of a set of  latent factors|3|arxiv|0|introduction|MISC
factor  regression  couples this analysis with a prediction task  where the predictions are made solely on the basis of the factor representation|4|arxiv|0|introduction|MISC
the latent factor representation achieves two fold benefits    NUMBER   discovering the latent  process  underlying the data    NUMBER   simpler predictive modeling through a compact data representation|5|arxiv|0|introduction|MISC
in particular    NUMBER   is motivated by the problem of prediction in the    large p small n    paradigm  CITATION   where the number of features   greatly exceeds the number of examples    potentially resulting in overfitting|6|arxiv|0|introduction|MISC
we address three fundamental shortcomings of standard factor analysis approaches  CITATION     NUMBER   we do not assume a known number of factors    NUMBER   we do not assume factors are independent    NUMBER   we do not assume all features are relevant to the factor analysis|7|arxiv|0|introduction|CONT
our motivation for this work stems from the task of reconstructing regulatory structure from gene expression data|8|arxiv|0|introduction|AIM
in this context  factors correspond to regulatory pathways|9|arxiv|0|introduction|MISC
our contributions thus parallel the needs of gene pathway modeling|10|arxiv|0|introduction|AIM
in addition  we couple predictive modeling  for factor regression  within the factor analysis framework itself  instead of having to model it separately|11|arxiv|0|introduction|OWN
our factor regression model is fundamentally nonparametric|12|arxiv|0|introduction|OWN
in particular  we treat the gene to factor relationship nonparametrically by proposing a sparse variant of the indian buffet process  ibp   CITATION   designed to account for the sparsity of relevant genes  features |13|arxiv|0|introduction|OWN
we  couple  this ibp with a hierarchical prior over the factors|14|arxiv|0|introduction|OWN
this prior explains the fact that pathways are fundamentally related  some are involved in transcription  some in signaling  some in synthesis|15|arxiv|0|introduction|OWN
the nonparametric nature of our sparse ibp requires that the hierarchical prior  also  be nonparametric|16|arxiv|0|introduction|OWN
a natural choice is kingman s coalescent  CITATION   a popular distribution over infinite binary trees|17|arxiv|0|introduction|BASE
since our motivation is an application in bioinformatics  our notation and terminology will be drawn from that area|18|arxiv|0|introduction|OWN
in particular   genes  are  features    samples  are  examples   and  pathways  are  factors|19|arxiv|0|introduction|OWN
however  our model is more general|20|arxiv|0|introduction|CONT
an alternative application might be to a collaborative filtering problem  in which case our genes might correspond to movies  our samples might correspond to users and our pathways might correspond to genres|21|arxiv|0|introduction|OWN
in this context  all three contributions of our model still make sense  we do not know how many movie genres there are  some genres are closely related  romance to comedy versus to action   many movies may be spurious|22|arxiv|0|introduction|OWN
We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors|0|arxiv|0|abstract|AIM
To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent|1|arxiv|0|abstract|OWN
We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis|2|arxiv|0|abstract|OWN
Factor analysis is the task of explaining data by means of a set of  latent factors|3|arxiv|0|introduction|MISC
Factor  regression  couples this analysis with a prediction task, where the predictions are made solely on the basis of the factor representation|4|arxiv|0|introduction|MISC
The latent factor representation achieves two-fold benefits: (1) discovering the latent  process  underlying the data; (2) simpler predictive modeling through a compact data representation|5|arxiv|0|introduction|MISC
In particular, (2) is motivated by the problem of prediction in the  ``large P small N''  paradigm  CITATION , where the number of features  SYMBOL  greatly exceeds the number of examples  SYMBOL , potentially resulting in overfitting|6|arxiv|0|introduction|MISC
We address three fundamental shortcomings of standard factor analysis approaches  CITATION : (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis|7|arxiv|0|introduction|AIM
Our motivation for this work stems from the task of reconstructing regulatory structure from gene-expression data|8|arxiv|0|introduction|OWN
In this context, factors correspond to regulatory pathways|9|arxiv|0|introduction|OWN
Our contributions thus parallel the needs of gene pathway modeling|10|arxiv|0|introduction|OWN
In addition, we couple predictive modeling (for factor regression) within the factor analysis framework itself, instead of having to model it separately|11|arxiv|0|introduction|OWN
Our factor regression model is fundamentally nonparametric|12|arxiv|0|introduction|OWN
In particular, we treat the gene-to-factor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP)  CITATION , designed to account for the sparsity of relevant genes (features)|13|arxiv|0|introduction|OWN
We  couple  this IBP with a hierarchical prior over the factors|14|arxiv|0|introduction|OWN
This prior explains the fact that pathways are fundamentally related: some are involved in transcription, some in signaling, some in synthesis|15|arxiv|0|introduction|OWN
The nonparametric nature of our sparse IBP requires that the hierarchical prior  also  be nonparametric|16|arxiv|0|introduction|OWN
A natural choice is Kingman's coalescent  CITATION , a popular distribution over infinite binary trees|17|arxiv|0|introduction|BASE
Since our motivation is an application in bioinformatics, our notation and terminology will be drawn from that area|18|arxiv|0|introduction|OWN
In particular,  genes  are  features ,  samples  are  examples , and  pathways  are  factors|19|arxiv|0|introduction|OWN
However, our model is more general|20|arxiv|0|introduction|OWN
An alternative application might be to a collaborative filtering problem, in which case our genes might correspond to movies, our samples might correspond to users and our pathways might correspond to genres|21|arxiv|0|introduction|OWN
In this context, all three contributions of our model still make sense: we do not know how many movie genres there are; some genres are closely related (romance to comedy versus to action); many movies may be spurious|22|arxiv|0|introduction|OWN
 We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors|0|arxiv|1|abstract|AIM
 To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent|1|arxiv|1|abstract|BASE
 We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis|2|arxiv|1|abstract|OWN
 Factor analysis is the task of explaining data by means of a set of  latent factors|3|arxiv|1|introduction|MISC
 Factor  regression  couples this analysis with a prediction task, where the predictions are made solely on the basis of the factor representation|4|arxiv|1|introduction|MISC
 The latent factor representation achieves two-fold benefits: (1) discovering the latent  process  underlying the data; (2) simpler predictive modeling through a compact data representation|5|arxiv|1|introduction|MISC
 In particular, (2) is motivated by the problem of prediction in the  ``large P small N''  paradigm  CITATION , where the number of features  SYMBOL  greatly exceeds the number of examples  SYMBOL , potentially resulting in overfitting|6|arxiv|1|introduction|MISC
 We address three fundamental shortcomings of standard factor analysis approaches  CITATION : (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis|7|arxiv|1|introduction|CONT
 Our motivation for this work stems from the task of reconstructing regulatory structure from gene-expression data|8|arxiv|1|introduction|OWN
 In this context, factors correspond to regulatory pathways|9|arxiv|1|introduction|OWN
 Our contributions thus parallel the needs of gene pathway modeling|10|arxiv|1|introduction|OWN
 In addition, we couple predictive modeling (for factor regression) within the factor analysis framework itself, instead of having to model it separately|11|arxiv|1|introduction|OWN
 Our factor regression model is fundamentally nonparametric|12|arxiv|1|introduction|OWN
 In particular, we treat the gene-to-factor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP)  CITATION , designed to account for the sparsity of relevant genes (features)|13|arxiv|1|introduction|OWN
 We  couple  this IBP with a hierarchical prior over the factors|14|arxiv|1|introduction|OWN
 This prior explains the fact that pathways are fundamentally related: some are involved in transcription, some in signaling, some in synthesis|15|arxiv|1|introduction|OWN
 The nonparametric nature of our sparse IBP requires that the hierarchical prior  also  be nonparametric|16|arxiv|1|introduction|OWN
 A natural choice is Kingman's coalescent  CITATION , a popular distribution over infinite binary trees|17|arxiv|1|introduction|MISC
 Since our motivation is an application in bioinformatics, our notation and terminology will be drawn from that area|18|arxiv|1|introduction|OWN
 In particular,  genes  are  features ,  samples  are  examples , and  pathways  are  factors|19|arxiv|1|introduction|OWN
 However, our model is more general|20|arxiv|1|introduction|OWN
 An alternative application might be to a collaborative filtering problem, in which case our genes might correspond to movies, our samples might correspond to users and our pathways might correspond to genres|21|arxiv|1|introduction|OWN
 In this context, all three contributions of our model still make sense: we do not know how many movie genres there are; some genres are closely related (romance to comedy versus to action); many movies may be spurious|22|arxiv|1|introduction|OWN
in the constraint satisfaction problem      the aim is to find an assignment of values to a set of variables subject to specified constraints|0|arxiv|0|abstract|MISC
in the minimum cost homomorphism problem      one is additionally given weights   for every variable   and value    and the aim is to find an assignment   to the variables that minimizes|1|arxiv|0|abstract|MISC
let   denote the   problem parameterized by the set of predicates allowed for constraints|2|arxiv|0|abstract|MISC
is related to many well studied combinatorial optimization problems  and concrete applications can be found in  for instance  defence logistics and machine learning|3|arxiv|0|abstract|MISC
we show that   can be studied by using algebraic methods similar to those used for csps|4|arxiv|0|abstract|AIM
with the aid of algebraic techniques  we classify the computational complexity of   for all choices of|5|arxiv|0|abstract|OWN
our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs   gutin  hell  rafiey  yeo  european j of combinatorics   NUMBER  |6|arxiv|0|abstract|CONT
constraint satisfaction problems     are a natural way of formalizing a large number of computational problems arising in combinatorial optimization  artificial intelligence  and database theory|7|arxiv|0|introduction|MISC
this problem has the following two equivalent formulations    NUMBER   to find an assignment of values to a given set of variables  subject to constraints on the values that can be assigned simultaneously to specified subsets of variables  and   NUMBER   to find a homomorphism between two finite relational structures   and|8|arxiv|0|introduction|MISC
applications of  s arise in the propositional logic  database and graph theory  scheduling and many other areas|9|arxiv|0|introduction|MISC
during the past  NUMBER  years    and its subproblems has been intensively studied by computer scientists and mathematicians|10|arxiv|0|introduction|MISC
considerable attention has been given to the case where the constraints are restricted to a given finite set of relations    called a constraint language  CITATION|11|arxiv|0|introduction|MISC
for example  when   is a constraint language over the boolean set   with four ternary predicates            we obtain  NUMBER  sat|12|arxiv|0|introduction|MISC
this direction of research has been mainly concerned with the computational complexity of   as a function of|13|arxiv|0|introduction|MISC
it has been shown that the complexity of   is highly connected with relational clones of universal algebra  CITATION|14|arxiv|0|introduction|MISC
for every constraint language    it has been conjectured that   is either in p or np complete  CITATION|15|arxiv|0|introduction|MISC
in the minimum cost homomorphism problem      we are given variables subject to constraints and  additionally  costs on variable value pairs|16|arxiv|0|introduction|MISC
now  the task is not just to find any satisfying assignment to the variables  but one that minimizes the total cost|17|arxiv|0|introduction|MISC
was introduced in  CITATION  where it was motivated by a real world problem in defence logistics|18|arxiv|0|introduction|MISC
the question for which directed graphs   the problem   is polynomial time solvable was considered in  CITATION|19|arxiv|0|introduction|MISC
in this paper  we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages|20|arxiv|0|introduction|AIM
from this characterization  we obtain a dichotomy for    i e   if   is not polynomial time solvable  then it is np hard|21|arxiv|0|introduction|OWN
of course  this dichotomy implies the dichotomy for directed graphs|22|arxiv|0|introduction|OWN
in section  NUMBER   we present some preliminaries together with results connecting the complexity of   with conservative algebras|23|arxiv|0|introduction|OWN
the main dichotomy theorem is stated in section  NUMBER  and its proof is divided into several parts which can be found in sections  NUMBER   NUMBER |24|arxiv|0|introduction|OWN
the np hardness results are collected in section  NUMBER  followed by the building blocks for the tractability result  existence of majority polymorphisms  section  NUMBER   and connections with optimization in perfect graphs  section  NUMBER  |25|arxiv|0|introduction|OWN
section  NUMBER  introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in section  NUMBER |26|arxiv|0|introduction|OWN
in section  NUMBER  we reformulate our main result in terms of relational clones|27|arxiv|0|introduction|OWN
finally  in section  NUMBER  we explain the relation of our results to previous research and present directions for future research|28|arxiv|0|introduction|BASE
In the constraint satisfaction problem ( SYMBOL ), the aim is to find an assignment of values to a set of variables subject to specified constraints|0|arxiv|0|abstract|MISC
In the minimum cost homomorphism problem ( SYMBOL ), one is additionally given weights  SYMBOL  for every variable  SYMBOL  and value  SYMBOL , and the aim is to find an assignment  SYMBOL  to the variables that minimizes  SYMBOL|1|arxiv|0|abstract|MISC
Let  SYMBOL  denote the  SYMBOL  problem parameterized by the set of predicates allowed for constraints|2|arxiv|0|abstract|OWN
SYMBOL  is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning|3|arxiv|0|abstract|OWN
We show that  SYMBOL  can be studied by using algebraic methods similar to those used for CSPs|4|arxiv|0|abstract|OWN
With the aid of algebraic techniques, we classify the computational complexity of  SYMBOL  for all choices of  SYMBOL|5|arxiv|0|abstract|OWN
Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs CITATION|6|arxiv|0|abstract|AIM
Constraint satisfaction problems ( SYMBOL ) are a natural way of formalizing a large number of computational problems arising in combinatorial optimization, artificial intelligence, and database theory|7|arxiv|0|introduction|MISC
This problem has the following two equivalent formulations: (1) to find an assignment of values to a given set of variables, subject to constraints on the values that can be assigned simultaneously to specified subsets of variables, and (2) to find a homomorphism between two finite relational structures  SYMBOL  and  SYMBOL|8|arxiv|0|introduction|MISC
Applications of  SYMBOL s arise in the propositional logic, database and graph theory, scheduling and many other areas|9|arxiv|0|introduction|MISC
During the past 30 years,  SYMBOL  and its subproblems has been intensively studied by computer scientists and mathematicians|10|arxiv|0|introduction|MISC
Considerable attention has been given to the case where the constraints are restricted to a given finite set of relations  SYMBOL , called a constraint language  CITATION|11|arxiv|0|introduction|MISC
For example, when  SYMBOL  is a constraint language over the boolean set  SYMBOL  with four ternary predicates  SYMBOL ,  SYMBOL ,  SYMBOL ,  SYMBOL  we obtain 3-SAT|12|arxiv|0|introduction|MISC
This direction of research has been mainly concerned with the computational complexity of  SYMBOL  as a function of  SYMBOL|13|arxiv|0|introduction|MISC
It has been shown that the complexity of  SYMBOL  is highly connected with relational clones of universal algebra  CITATION|14|arxiv|0|introduction|MISC
For every constraint language  SYMBOL , it has been conjectured that  SYMBOL  is either in P or NP-complete  CITATION|15|arxiv|0|introduction|MISC
In the minimum cost homomorphism problem ( SYMBOL ), we are given variables subject to constraints and, additionally, costs on variable/value pairs|16|arxiv|0|introduction|MISC
Now, the task is not just to find any satisfying assignment to the variables, but one that minimizes the total cost|17|arxiv|0|introduction|MISC
SYMBOL  was introduced in  CITATION  where it was motivated by a real-world problem in defence logistics|18|arxiv|0|introduction|MISC
The question for which directed graphs  SYMBOL  the problem  SYMBOL  is polynomial-time solvable was considered in  CITATION|19|arxiv|0|introduction|MISC
In this paper, we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages|20|arxiv|0|introduction|AIM
From this characterization, we obtain a dichotomy for  SYMBOL , i e , if  SYMBOL  is not polynomial-time solvable, then it is NP-hard|21|arxiv|0|introduction|OWN
Of course, this dichotomy implies the dichotomy for directed graphs|22|arxiv|0|introduction|OWN
In Section 2, we present some preliminaries together with results connecting the complexity of  SYMBOL  with conservative algebras|23|arxiv|0|introduction|OWN
The main dichotomy theorem is stated in Section 3 and its proof is divided into several parts which can be found in Sections 4-8|24|arxiv|0|introduction|OWN
The NP-hardness results are collected in Section 4 followed by the building blocks for the tractability result: existence of majority polymorphisms (Section 5) and connections with optimization in perfect graphs (Section 6)|25|arxiv|0|introduction|OWN
Section 7 introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in Section 8|26|arxiv|0|introduction|OWN
In Section 9 we reformulate our main result in terms of relational clones|27|arxiv|0|introduction|OWN
Finally, in Section 10 we explain the relation of our results to previous research and present directions for future research|28|arxiv|0|introduction|OWN
 In the constraint satisfaction problem ( SYMBOL ), the aim is to find an assignment of values to a set of variables subject to specified constraints|0|arxiv|1|abstract|MISC
 In the minimum cost homomorphism problem ( SYMBOL ), one is additionally given weights  SYMBOL  for every variable  SYMBOL  and value  SYMBOL , and the aim is to find an assignment  SYMBOL  to the variables that minimizes  SYMBOL|1|arxiv|1|abstract|MISC
 Let  SYMBOL  denote the  SYMBOL  problem parameterized by the set of predicates allowed for constraints|2|arxiv|1|abstract|MISC
 SYMBOL  is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning|3|arxiv|1|abstract|MISC
 We show that  SYMBOL  can be studied by using algebraic methods similar to those used for CSPs|4|arxiv|1|abstract|AIM
 With the aid of algebraic techniques, we classify the computational complexity of  SYMBOL  for all choices of  SYMBOL|5|arxiv|1|abstract|OWN
 Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs CITATION|6|arxiv|1|abstract|OWN
 Constraint satisfaction problems ( SYMBOL ) are a natural way of formalizing a large number of computational problems arising in combinatorial optimization, artificial intelligence, and database theory|7|arxiv|1|introduction|MISC
 This problem has the following two equivalent formulations: (1) to find an assignment of values to a given set of variables, subject to constraints on the values that can be assigned simultaneously to specified subsets of variables, and (2) to find a homomorphism between two finite relational structures  SYMBOL  and  SYMBOL|8|arxiv|1|introduction|MISC
 Applications of  SYMBOL s arise in the propositional logic, database and graph theory, scheduling and many other areas|9|arxiv|1|introduction|MISC
 During the past 30 years,  SYMBOL  and its subproblems has been intensively studied by computer scientists and mathematicians|10|arxiv|1|introduction|MISC
 Considerable attention has been given to the case where the constraints are restricted to a given finite set of relations  SYMBOL , called a constraint language  CITATION|11|arxiv|1|introduction|MISC
 For example, when  SYMBOL  is a constraint language over the boolean set  SYMBOL  with four ternary predicates  SYMBOL ,  SYMBOL ,  SYMBOL ,  SYMBOL  we obtain 3-SAT|12|arxiv|1|introduction|MISC
 This direction of research has been mainly concerned with the computational complexity of  SYMBOL  as a function of  SYMBOL|13|arxiv|1|introduction|MISC
 It has been shown that the complexity of  SYMBOL  is highly connected with relational clones of universal algebra  CITATION|14|arxiv|1|introduction|MISC
 For every constraint language  SYMBOL , it has been conjectured that  SYMBOL  is either in P or NP-complete  CITATION|15|arxiv|1|introduction|MISC
 In the minimum cost homomorphism problem ( SYMBOL ), we are given variables subject to constraints and, additionally, costs on variable/value pairs|16|arxiv|1|introduction|MISC
 Now, the task is not just to find any satisfying assignment to the variables, but one that minimizes the total cost|17|arxiv|1|introduction|MISC
 SYMBOL  was introduced in  CITATION  where it was motivated by a real-world problem in defence logistics|18|arxiv|1|introduction|MISC
 The question for which directed graphs  SYMBOL  the problem  SYMBOL  is polynomial-time solvable was considered in  CITATION|19|arxiv|1|introduction|MISC
 In this paper, we approach the problem in its most general form by algebraic methods and give a complete algebraic characterization of tractable constraint languages|20|arxiv|1|introduction|AIM
 From this characterization, we obtain a dichotomy for  SYMBOL , i e , if  SYMBOL  is not polynomial-time solvable, then it is NP-hard|21|arxiv|1|introduction|OWN
 Of course, this dichotomy implies the dichotomy for directed graphs|22|arxiv|1|introduction|OWN
 In Section 2, we present some preliminaries together with results connecting the complexity of  SYMBOL  with conservative algebras|23|arxiv|1|introduction|OWN
 The main dichotomy theorem is stated in Section 3 and its proof is divided into several parts which can be found in Sections 4-8|24|arxiv|1|introduction|OWN
 The NP-hardness results are collected in Section 4 followed by the building blocks for the tractability result: existence of majority polymorphisms (Section 5) and connections with optimization in perfect graphs (Section 6)|25|arxiv|1|introduction|OWN
 Section 7 introduces the concept of  arithmetical deadlocks  which lay the foundation for the final proof in Section 8|26|arxiv|1|introduction|OWN
 In Section 9 we reformulate our main result in terms of relational clones|27|arxiv|1|introduction|OWN
 Finally, in Section 10 we explain the relation of our results to previous research and present directions for future research|28|arxiv|1|introduction|OWN
Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems|0|arxiv|0|abstract|MISC
Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem|1|arxiv|0|abstract|MISC
It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution|2|arxiv|0|abstract|MISC
In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved|3|arxiv|0|abstract|AIM
We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations|4|arxiv|0|abstract|OWN
Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels|5|arxiv|0|introduction|MISC
Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn i i d from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     |6|arxiv|0|introduction|MISC
Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set|7|arxiv|0|introduction|MISC
The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting|8|arxiv|0|introduction|MISC
Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function|9|arxiv|0|introduction|MISC
Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    |10|arxiv|0|introduction|MISC
Recently, a number of solvers have been proposed for the regularized risk minimization problem|11|arxiv|0|introduction|MISC
The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution|12|arxiv|0|introduction|MISC
The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION|13|arxiv|0|introduction|MISC
In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)|14|arxiv|0|introduction|MISC
At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL|15|arxiv|0|introduction|MISC
Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL|16|arxiv|0|introduction|MISC
The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL|17|arxiv|0|introduction|MISC
Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL|18|arxiv|0|introduction|CONT
Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems|19|arxiv|0|introduction|CONT
It was therefore conjectured that the rates of convergence of BMRM could be improved|20|arxiv|0|introduction|MISC
In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations|21|arxiv|0|introduction|AIM
One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual|22|arxiv|0|introduction|OWN
Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work|23|arxiv|0|introduction|OWN
Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces|24|arxiv|0|introduction|OWN
Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses|25|arxiv|0|introduction|OWN
 Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems|0|arxiv|0|abstract|MISC
 Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem|1|arxiv|0|abstract|MISC
 It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution|2|arxiv|0|abstract|MISC
 In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved|3|arxiv|0|abstract|OWN
 We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations|4|arxiv|0|abstract|AIM
 Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels|5|arxiv|0|introduction|MISC
 Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn iid from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     |6|arxiv|0|introduction|MISC
 Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set|7|arxiv|0|introduction|MISC
 The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting|8|arxiv|0|introduction|MISC
 Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function|9|arxiv|0|introduction|MISC
 Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    |10|arxiv|0|introduction|MISC
 Recently, a number of solvers have been proposed for the regularized risk minimization problem|11|arxiv|0|introduction|MISC
 The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution|12|arxiv|0|introduction|MISC
 The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION|13|arxiv|0|introduction|MISC
 In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)|14|arxiv|0|introduction|MISC
 At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL|15|arxiv|0|introduction|MISC
 Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL|16|arxiv|0|introduction|MISC
 The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL|17|arxiv|0|introduction|MISC
 Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL|18|arxiv|0|introduction|MISC
 Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems|19|arxiv|0|introduction|MISC
 It was therefore conjectured that the rates of convergence of BMRM could be improved|20|arxiv|0|introduction|MISC
 In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations|21|arxiv|0|introduction|AIM
 One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual|22|arxiv|0|introduction|OWN
 Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work|23|arxiv|0|introduction|BASE
 Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces|24|arxiv|0|introduction|OWN
 Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses|25|arxiv|0|introduction|OWN
 Regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems|0|arxiv|1|abstract|MISC
 Bundle methods for regularized risk minimization (BMRM) and the closely related SVMStruct are considered the best general purpose solvers to tackle this problem|1|arxiv|1|abstract|MISC
 It was recently shown that BMRM requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution|2|arxiv|1|abstract|MISC
 In the first part of the paper we use the Hadamard matrix to construct a regularized risk minimization problem and show that these rates cannot be improved|3|arxiv|1|abstract|AIM
 We then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an  SYMBOL  accurate solution in  SYMBOL  iterations|4|arxiv|1|abstract|AIM
 Let  SYMBOL  denote samples and  SYMBOL  be the corresponding labels|5|arxiv|1|introduction|MISC
 Given a training set of  SYMBOL  sample label pairs  SYMBOL , drawn iid from a joint probability distribution on  SYMBOL , many machine learning algorithms solve the following regularized risk minimization problem:     |6|arxiv|1|introduction|MISC
 Here  SYMBOL  denotes the loss on instance  SYMBOL  using the current model  SYMBOL  and  SYMBOL , the empirical risk, is the average loss on the training set|7|arxiv|1|introduction|MISC
 The regularizer  SYMBOL  acts as a penalty on the complexity of the classifier and prevents overfitting|8|arxiv|1|introduction|MISC
 Usually the loss is convex in  SYMBOL  but can be nonsmooth while the regularizer is usually a smooth strongly convex function|9|arxiv|1|introduction|MISC
 Binary Support Vector Machines (SVMs) are a prototypical example of such regularized risk minimization problems where  SYMBOL  and the loss considered is the binary hinge loss:    |10|arxiv|1|introduction|MISC
 Recently, a number of solvers have been proposed for the regularized risk minimization problem|11|arxiv|1|introduction|MISC
 The first and perhaps the best known solver is SVMStruct  CITATION , which was shown to converge in  SYMBOL  iterations to an  SYMBOL  accurate solution|12|arxiv|1|introduction|MISC
 The convergence analysis of SVMStruct was improved to  SYMBOL  iterations by  CITATION|13|arxiv|1|introduction|MISC
 In fact,  CITATION  showed that their convergence analysis holds for a more general solver than SVMStruct namely BMRM (Bundle method for regularized risk minimization)|14|arxiv|1|introduction|MISC
 At every iteration BMRM replaces  SYMBOL  by a piecewise linear lower bound  SYMBOL  and optimizes    to obtain the next iterate  SYMBOL|15|arxiv|1|introduction|MISC
 Here  SYMBOL  denotes an arbitrary subgradient of  SYMBOL  at  SYMBOL  (see Section ) and  SYMBOL|16|arxiv|1|introduction|MISC
 The piecewise linear lower bound is successively tightened until the gap     falls below a predefined tolerance  SYMBOL|17|arxiv|1|introduction|MISC
 Even though BMRM solves an expensive optimization problem at every iteration, the convergence analysis only uses a simple one-dimensional line search to bound the decrease in  SYMBOL|18|arxiv|1|introduction|MISC
 Furthermore, the empirical convergence behavior of BMRM is much better than the theoretically predicted rates on a number of real life problems|19|arxiv|1|introduction|MISC
 It was therefore conjectured that the rates of convergence of BMRM could be improved|20|arxiv|1|introduction|MISC
 In this paper we answer this question in the negative by explicitly constructing a regularized risk minimization problem for which BMRM takes at least  SYMBOL  iterations|21|arxiv|1|introduction|AIM
 One possible way to circumvent the  SYMBOL  lower bound is to solve the problem in the dual|22|arxiv|1|introduction|MISC
 Using a very old result of Nesterov  CITATION  we obtain an algorithm for SVMs which only requires  SYMBOL  iterations to converge to an  SYMBOL  accurate solution; each iteration of the algorithm requires  SYMBOL  work|23|arxiv|1|introduction|OWN
 Although we primarily focus on the regularized risk minimization with the binary hinge loss, our algorithm can also be used whenever the empirical risk is piecewise linear and contains a small number of pieces|24|arxiv|1|introduction|OWN
 Examples of this include multiclass, multi-label, and ordinal regression hinge loss and other related losses|25|arxiv|1|introduction|MISC
this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty|0|jdm|0|abstract|OWN
 	a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty|1|jdm|0|abstract|OWN
the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory|2|jdm|0|abstract|OWN
during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices|3|jdm|0|abstract|OWN
women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk|4|jdm|0|abstract|OWN
this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation|5|jdm|0|abstract|OWN
males and females seem to differ in spatial abilities and styles  CITATION|6|jdm|0|introduction|MISC
generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION|7|jdm|0|introduction|MISC
evolutionary mechanisms could potentially account for sex differences in spatial behavior|8|jdm|0|introduction|MISC
for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION|9|jdm|0|introduction|MISC
mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates|10|jdm|0|introduction|MISC
typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION|11|jdm|0|introduction|MISC
another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION|12|jdm|0|introduction|MISC
however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates|13|jdm|0|introduction|MISC
in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring|14|jdm|0|introduction|MISC
this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems|15|jdm|0|introduction|MISC
such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER |16|jdm|0|introduction|MISC
thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior|17|jdm|0|introduction|MISC
taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION|18|jdm|0|introduction|MISC
in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation|19|jdm|0|introduction|MISC
hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex|20|jdm|0|introduction|MISC
experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION|21|jdm|0|introduction|CONT
this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies|22|jdm|0|introduction|MISC
the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty|23|jdm|0|introduction|OWN
following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems|24|jdm|0|introduction|OWN
to test this  i used a simple spontaneous two-dimensional exploratory task|25|jdm|0|introduction|OWN
this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge|26|jdm|0|introduction|OWN
it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment|27|jdm|0|introduction|MISC
moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION|28|jdm|0|introduction|MISC
exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty|29|jdm|0|introduction|MISC
indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions|30|jdm|0|introduction|MISC
voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use|31|jdm|0|introduction|MISC
in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used|32|jdm|0|introduction|OWN
the results were analyzed with signal detection theory|33|jdm|0|introduction|OWN
 this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty|0|jdm|0|abstract|AIM
 a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty|1|jdm|0|abstract|OWN
 the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory|2|jdm|0|abstract|OWN
 during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices|3|jdm|0|abstract|OWN
 women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk|4|jdm|0|abstract|OWN
 this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation|5|jdm|0|abstract|OWN
 males and females seem to differ in spatial abilities and styles  CITATION|6|jdm|0|introduction|MISC
 generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION|7|jdm|0|introduction|MISC
 evolutionary mechanisms could potentially account for sex differences in spatial behavior|8|jdm|0|introduction|MISC
 for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION|9|jdm|0|introduction|MISC
 mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates|10|jdm|0|introduction|MISC
 typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION|11|jdm|0|introduction|MISC
 another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION|12|jdm|0|introduction|MISC
 however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates|13|jdm|0|introduction|MISC
 in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring|14|jdm|0|introduction|MISC
 this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems|15|jdm|0|introduction|MISC
 such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER |16|jdm|0|introduction|MISC
 thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior|17|jdm|0|introduction|MISC
 taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION|18|jdm|0|introduction|MISC
 in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation|19|jdm|0|introduction|MISC
 hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex|20|jdm|0|introduction|MISC
 experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION|21|jdm|0|introduction|CONT
 this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies|22|jdm|0|introduction|MISC
 the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty|23|jdm|0|introduction|AIM
 following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems|24|jdm|0|introduction|AIM
 to test this  i used a simple spontaneous two-dimensional exploratory task|25|jdm|0|introduction|OWN
 this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge|26|jdm|0|introduction|MISC
 it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment|27|jdm|0|introduction|MISC
 moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION|28|jdm|0|introduction|MISC
 exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty|29|jdm|0|introduction|MISC
 indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions|30|jdm|0|introduction|MISC
 voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use|31|jdm|0|introduction|BASE
 in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used|32|jdm|0|introduction|OWN
 the results were analyzed with signal detection theory|33|jdm|0|introduction|OWN
 this study was designed to assess sex-related differences in the selection of an appropriate strategy when facing novelty|0|jdm|1|abstract|AIM
 a simple visuo-spatial task was used to investigate exploratory behavior as a specific response to novelty|1|jdm|1|abstract|OWN
 the exploration task was followed by a visual discrimination task  and the responses were analyzed using signal detection theory|2|jdm|1|abstract|OWN
 during exploration women selected a local searching strategy in which the metric distance between what is already known and what is unknown was reduced  whereas men adopted a global strategy based on an approximately uniform distribution of choices|3|jdm|1|abstract|OWN
 women's exploratory behavior gives rise to a notion of a secure base warranting a sense of safety while men's behavior does not appear to be influenced by risk|4|jdm|1|abstract|OWN
 this sex-related difference was interpreted as a difference in beliefs concerning the likelihood of uncertain events influencing risk evaluation|5|jdm|1|abstract|OWN
 males and females seem to differ in spatial abilities and styles  CITATION|6|jdm|1|introduction|MISC
 generally  studies involving navigational problems showed that female cognitive style relies more on detailed information  while male style relies more on global information  CITATION|7|jdm|1|introduction|MISC
 evolutionary mechanisms could potentially account for sex differences in spatial behavior|8|jdm|1|introduction|MISC
 for example  these behavioral differences may be due to mating patterns that induced a selection of large-range navigation in males  CITATION|9|jdm|1|introduction|MISC
 mating patterns or mating strategies are linked to the dynamics of reproduction and sexual selection  CITATION   and sexual selection is restricted to characteristics that influence mate choice and competition for mates|10|jdm|1|introduction|MISC
 typically  males have to compete through extensive ranging for access to mates while females have to choose mating partners according to reproductive success  CITATION|11|jdm|1|introduction|MISC
 another proposition  but exclusively directed at humans  suggested that the division of labor game hunting and plant gathering would have put greater selection pressure on females' spatial memory because females sustained gathering duties  CITATION|12|jdm|1|introduction|MISC
 however  as argued by ecuyer-dab and robert  CITATION    the selection of male characteristics depends on females' choice for mates|13|jdm|1|introduction|MISC
 in females  however  spatial cognition would have been primarily shaped by the natural selection of a strong concern for survival both of self and of offspring|14|jdm|1|introduction|MISC
 this concern would have compelled them to favor low-risk strategies  like concentrating on proximal spatial cues  when coping with space-related problems|15|jdm|1|introduction|MISC
 such focusing would have enabled secure navigation based on detailed landmark encoding  as well as  in certain species  regular feeding based on remembering the exact locations of potential resources  p  NUMBER |16|jdm|1|introduction|MISC
 thus  the hypothesis of labor division would be a by-product of sexual selection and not the cause of sex differences in spatial behavior|17|jdm|1|introduction|MISC
 taken together  the literature seems to indicate that the key to understanding the evolution of behavioral sex differences relies on the relative costs and benefits of producing offspring  CITATION|18|jdm|1|introduction|MISC
 in that context spatial skills play a crucial role since they increase reproductive success and the accessibility to food resource but  at the same time  multiply the risks of getting lost  being killed or consumed by other animals predation|19|jdm|1|introduction|MISC
 hence  the survival of mobile species depends on their ability to balance costs and benefits induced by locomotion and this balancing should differ according to sex|20|jdm|1|introduction|MISC
 experimental investigations of sex differences in spatial abilities yield apparently disparate results  CITATION|21|jdm|1|introduction|MISC
 this might be partly due to the complexity of contemporary experimental designs  but also to a lack of investigations concerning decision-making processes involved in the selection of strategies|22|jdm|1|introduction|MISC
 the current study investigates sex differences in basic behaviors like exploration  detection and discrimination involving the selection of strategies when coping with uncertainty|23|jdm|1|introduction|AIM
 following the above quotation from ecuyer-dab and robert's  the hypothesis is that women  compared to men  should favor low-risk strategies when coping with space-related problems|24|jdm|1|introduction|OWN
 to test this  i used a simple spontaneous two-dimensional exploratory task|25|jdm|1|introduction|OWN
 this choice relies on the fact that exploration is a natural behavior and that it is fundamental in acquiring spatial knowledge|26|jdm|1|introduction|MISC
 it seems to be based on driving factors such as curiosity  comfort or mastery over one's environment|27|jdm|1|introduction|MISC
 moreover  it is commonly defined as serving to reduce uncertainty and thus allow coping with fear  CITATION|28|jdm|1|introduction|MISC
 exploration is mainly characterized by a succession of progressions and stops  CITATION   and the selection of exploration could rely on its capacity to act as a regulator of uncertainty|29|jdm|1|introduction|MISC
 indeed  progressions are based on decisions taken during stops  and stops correspond to choice points allowing decisions|30|jdm|1|introduction|MISC
 voss  CITATION  refers to the exploration process as the generation and testing of hypotheses concerning the object's meaning and potential use|31|jdm|1|introduction|MISC
 in order to assess risk-taking  a classical visual discrimination task based on the stimuli observed during the exploration task was used|32|jdm|1|introduction|OWN
 the results were analyzed with signal detection theory|33|jdm|1|introduction|OWN
similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities|0|jdm|0|abstract|MISC
the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option|1|jdm|0|abstract|MISC
we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox|2|jdm|0|abstract|AIM
in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION|3|jdm|0|introduction|MISC
in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired|4|jdm|0|introduction|MISC
the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products|5|jdm|0|introduction|MISC
that is, va = sigma  ux ft|6|jdm|0|introduction|MISC
an option a will be preferred to an option b if and only if va  greater than  vb|7|jdm|0|introduction|MISC
however, a large body of empirical evidence demonstrates that people systematically violate this theory|8|jdm|0|introduction|MISC
this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION|9|jdm|0|introduction|MISC
this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data|10|jdm|0|introduction|MISC
the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION|11|jdm|0|introduction|MISC
this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future|12|jdm|0|introduction|MISC
numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION|13|jdm|0|introduction|MISC
however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses|14|jdm|0|introduction|MISC
when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption|15|jdm|0|introduction|MISC
with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION|16|jdm|0|introduction|MISC
the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION|17|jdm|0|introduction|MISC
because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION|18|jdm|0|introduction|MISC
both lines of research have spawned a large number of variant models|19|jdm|0|introduction|MISC
although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility|20|jdm|0|introduction|MISC
a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges|21|jdm|0|introduction|CONT
for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses|22|jdm|0|introduction|CONT
examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION|23|jdm|0|introduction|CONT
the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option|24|jdm|0|introduction|MISC
the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION|25|jdm|0|introduction|MISC
since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION|26|jdm|0|introduction|MISC
most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption|27|jdm|0|introduction|MISC
these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements|28|jdm|0|introduction|MISC
table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom|29|jdm|0|introduction|MISC
in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings|30|jdm|0|introduction|MISC
the violation of cancellation would be observed if the preference orderings were different between problem i and problem i|31|jdm|0|introduction|MISC
however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox|32|jdm|0|introduction|MISC
we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom|33|jdm|0|introduction|AIM
 similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities|0|jdm|0|abstract|MISC
 the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option|1|jdm|0|abstract|MISC
 we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox|2|jdm|0|abstract|AIM
 in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION|3|jdm|0|introduction|MISC
 in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired|4|jdm|0|introduction|MISC
 the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products|5|jdm|0|introduction|MISC
 that is, va = sigma  ux ft|6|jdm|0|introduction|MISC
 an option a will be preferred to an option b if and only if va  greater than  vb|7|jdm|0|introduction|MISC
 however, a large body of empirical evidence demonstrates that people systematically violate this theory|8|jdm|0|introduction|MISC
 this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION|9|jdm|0|introduction|MISC
 this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data|10|jdm|0|introduction|MISC
 the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION|11|jdm|0|introduction|MISC
 this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future|12|jdm|0|introduction|MISC
 numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION|13|jdm|0|introduction|MISC
 however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses|14|jdm|0|introduction|MISC
 when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption|15|jdm|0|introduction|MISC
 with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION|16|jdm|0|introduction|MISC
 the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION|17|jdm|0|introduction|MISC
 because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION|18|jdm|0|introduction|MISC
 both lines of research have spawned a large number of variant models|19|jdm|0|introduction|MISC
 although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility|20|jdm|0|introduction|MISC
 a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges|21|jdm|0|introduction|MISC
 for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses|22|jdm|0|introduction|MISC
 examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION|23|jdm|0|introduction|MISC
 the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option|24|jdm|0|introduction|MISC
 the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION|25|jdm|0|introduction|MISC
 since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION|26|jdm|0|introduction|MISC
 most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption|27|jdm|0|introduction|MISC
 these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements|28|jdm|0|introduction|MISC
 table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom|29|jdm|0|introduction|OWN
 in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings|30|jdm|0|introduction|OWN
 the violation of cancellation would be observed if the preference orderings were different between problem i and problem i|31|jdm|0|introduction|OWN
 however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox|32|jdm|0|introduction|OWN
 we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom|33|jdm|0|introduction|OWN
 similar to research on risky choice, the traditional analysis of intertemporal choice takes the view that an individual behaves so as to maximize the discounted sum of all future utilities|0|jdm|1|abstract|MISC
 the well-known allais paradox contradicts the fundamental postulates of maximizing the expected value or utility of a risky option|1|jdm|1|abstract|MISC
 we describe a violation of the law of diminishing marginal utility as well as an intertemporal version of the allais paradox|2|jdm|1|abstract|AIM
 in the field of intertemporal choice, the discounted-utility du theory proposed by paul samuelson in 1937 was presented not only as a valid normative standard but also as a descriptive theory of actual intertemporal choice behavior  CITATION|3|jdm|1|introduction|MISC
 in its general form, the du theory proposes that the value of an option, x; t, is the product of its present utility, ux, and an exponential temporal discounting function, ft, where t is the time at which x is acquired|4|jdm|1|introduction|MISC
 the overall value of a mixed option, a = {x, t, x, t, }, denoted va, is simply the sum of these products|5|jdm|1|introduction|MISC
 that is, va = sigma  ux ft|6|jdm|1|introduction|MISC
 an option a will be preferred to an option b if and only if va  greater than  vb|7|jdm|1|introduction|MISC
 however, a large body of empirical evidence demonstrates that people systematically violate this theory|8|jdm|1|introduction|CONT
 this includes the common difference effect, the magnitude effect, the gain-loss asymmetry, the delay-speedup asymmetry, and so on  CITATION|9|jdm|1|introduction|MISC
 this situation has led researchers to consider extensions and modifications of the du theory to reconcile it with the experimental data|10|jdm|1|introduction|MISC
 the most prominent idea to account for these anomalies is the hyperbolic discounting model  CITATION|11|jdm|1|introduction|MISC
 this model suggests that the discount rate is not dynamically consistent but that the rate is higher between the present and near future and lower between the near and far distant future|12|jdm|1|introduction|MISC
 numerous theories have been developed by transforming the discount function to other forms, from one-parameter hyperbolic discounting  CITATION  to generalized hyperbolic discounting  CITATION , to proportional discounting  CITATION , and to quasi-hyperbolic discounting  CITATION|13|jdm|1|introduction|MISC
 however, these models focus on intertemporal choice between pairs of single-dated outcomes represented as pure gains or losses|14|jdm|1|introduction|MISC
 when these models are applied to intertemporal choice between pairs of multiple-dated outcomes in mixed contexts, there is general agreement on the additive assumption and the independence assumption|15|jdm|1|introduction|MISC
 with an apt transformation of the discounting rate, the additive assumption means that preferences for outcome sequences are based on a simple aggregation of their individual components within intertemporal choice  CITATION|16|jdm|1|introduction|MISC
 the independence assumption means that the value or utility of an outcome in one period is independent of outcomes in other periods  CITATION|17|jdm|1|introduction|MISC
 because risk and delay might be psychologically equivalent, or at least analogous, and because similar psychological processes might underlie risk and intertemporal choice  CITATION , theoretical development in intertemporal choice has progressed steadily along a similar route as that of risky choice  CITATION|18|jdm|1|introduction|MISC
 both lines of research have spawned a large number of variant models|19|jdm|1|introduction|MISC
 although the functional forms differ, most theories assume a maximization principle; that is, people calculate the mathematical expectation of each outcome and add them together before choosing the option that maximizes overall value or utility|20|jdm|1|introduction|MISC
 a minor difference is that the existing models of intertemporal choice are relatively underdeveloped and are less flexible in dealing with empirical challenges|21|jdm|1|introduction|MISC
 for example, research on risky decision making does not treat risky choice as limited to pure gains or pure losses but has been extended to include mixed outcomes involving both gains and losses|22|jdm|1|introduction|MISC
 examples include the sign-dependent utility model  CITATION , the rank- and sign-dependent utility model  CITATION , and the transfer of attention exchange model  CITATION|23|jdm|1|introduction|MISC
 the well-known allais paradox  CITATION  contradicts the fundamental postulates of maximizing the expected utility of a risky option|24|jdm|1|introduction|MISC
 the paradox presents a violation of the cancellation axiom, which asserts that, if two options have a common consequence under a particular event, the preference order of the options should be independent of the value of that consequence  CITATION|25|jdm|1|introduction|MISC
 since then, many new descriptive theories of risky choice have abandoned the maximization assumption  CITATION|26|jdm|1|introduction|MISC
 most models of intertemporal choice have not yet abandoned the additive assumption and the independence assumption|27|jdm|1|introduction|CONT
 these two assumptions would lead to the cancellation axiom, which indicates that a preference between two sequences with elements in common does not depend on the nature of the common elements|28|jdm|1|introduction|MISC
 table 1 illustrates an example of the multiple-dated outcomes problem, which would be used to test the cancellation axiom|29|jdm|1|introduction|MISC
 in problem i, the additive models predict that adding a common element x at time 2 to both option a and option b would not change the preference orderings|30|jdm|1|introduction|MISC
 the violation of cancellation would be observed if the preference orderings were different between problem i and problem i|31|jdm|1|introduction|MISC
 however, if allais's proposition applies to intertemporal choice, we will eventually encounter an intertemporal version of the allais paradox|32|jdm|1|introduction|MISC
 we first illustrate our point with a paradox that is an intertemporal-type violation of the cancellation axiom|33|jdm|1|introduction|OWN
the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented|0|jdm|0|abstract|AIM
we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003|1|jdm|0|abstract|OWN
the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords|2|jdm|0|abstract|OWN
we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement|3|jdm|0|abstract|OWN
the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal|4|jdm|0|abstract|OWN
normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION|5|jdm|0|introduction|MISC
descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION|6|jdm|0|introduction|CONT
for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION|7|jdm|0|introduction|MISC
the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party|8|jdm|0|introduction|MISC
this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit|9|jdm|0|introduction|MISC
a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION|10|jdm|0|introduction|MISC
in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict|11|jdm|0|introduction|BASE
the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented|0|jdm|0|abstract|AIM
we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003|1|jdm|0|abstract|OWN
the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords|2|jdm|0|abstract|OWN
we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement|3|jdm|0|abstract|AIM
the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal|4|jdm|0|abstract|OWN
normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION|5|jdm|0|introduction|MISC
descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION|6|jdm|0|introduction|CONT
for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION|7|jdm|0|introduction|MISC
the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party|8|jdm|0|introduction|MISC
this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit|9|jdm|0|introduction|CONT
a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION|10|jdm|0|introduction|MISC
in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict|11|jdm|0|introduction|AIM
 the paper extends research on fixed-pie perceptions by suggesting that disputants may prefer proposals that are perceived to be equally attractive to both parties i e , balanced rather than one-sided, because balanced agreements are seen as more likely to be successfully implemented|0|jdm|1|abstract|AIM
 we test our predictions using data on israeli support for the geneva accords, an agreement for a two state solution negotiated by unofficial delegations of israel and the palestinian authority in 2003|1|jdm|1|abstract|OWN
 the results demonstrate that israelis are more likely to support agreements that are seen favorably by other israelis, but - contrary to fixed-pie predictions - israeli support for the accords does not diminish simply because a majority of palestinians favors rather than opposes the accords|2|jdm|1|abstract|OWN
 we show that implementation concerns create a demand among israelis for balance in the degree to which each side favors or opposes the agreement|3|jdm|1|abstract|OWN
 the effect of balance is noteworthy in that it creates considerable support for proposals even when a majority of israelis and palestinians oppose the deal|4|jdm|1|abstract|OWN
 normative models of bargaining and negotiation suggest that if there is potential for mutual benefit, conflicting parties should be able to achieve it  CITATION|5|jdm|1|introduction|MISC
 descriptive accounts and empirical investigations of negotiation behavior  CITATION , however, suggest that a number of psychological barriers to conflict resolution are likely to make efficient deal making difficult  CITATION|6|jdm|1|introduction|MISC
 for example, research on cognitive biases associated with egocentric perceptions suggests that negotiators and evaluators of negotiated agreements are likely to exhibit a "fixed-pie bias"  CITATION|7|jdm|1|introduction|MISC
 the fixed-pie bias refers to the belief that any gain for one party will be associated with an equivalent loss to the other party|8|jdm|1|introduction|MISC
 this belief is a "bias" when it persists even in contexts where there is a possibility of compatible interests or mutual benefit|9|jdm|1|introduction|MISC
 a large body of research finds that negotiators are susceptible to the fixed-pie bias prior to, during, and even after negotiations  CITATION|10|jdm|1|introduction|MISC
 in the current paper we investigate and extend research on fixed pie bias in the context of protracted intergroup conflict|11|jdm|1|introduction|AIM
we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially|0|jdm|0|abstract|AIM
we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies|1|jdm|0|abstract|OWN
our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood|2|jdm|0|abstract|OWN
the magnitude of the effect is of economic significance|3|jdm|0|abstract|OWN
we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially|4|jdm|0|abstract|OWN
lured by temptation, individuals may find themselves acting against their better judgment|5|jdm|0|introduction|MISC
self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences|6|jdm|0|introduction|MISC
for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure|7|jdm|0|introduction|MISC
the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study|8|jdm|0|introduction|MISC
and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget|9|jdm|0|introduction|MISC
perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms|10|jdm|0|introduction|MISC
this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations|11|jdm|0|introduction|MISC
that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION|12|jdm|0|introduction|MISC
for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so|13|jdm|0|introduction|MISC
nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities|14|jdm|0|introduction|MISC
that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly|15|jdm|0|introduction|MISC
self-control-our capacity to overrule temptation-is no less complex than it is important|16|jdm|0|introduction|MISC
a multitude of conceptualizations exist, many of which are complementary|17|jdm|0|introduction|MISC
typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION|18|jdm|0|introduction|MISC
willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION|19|jdm|0|introduction|MISC
such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens|20|jdm|0|introduction|MISC
our conceptualization of self-control mirrors these|21|jdm|0|introduction|MISC
only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control|22|jdm|0|introduction|MISC
loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation|23|jdm|0|introduction|MISC
o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health|24|jdm|0|introduction|MISC
at present, there is but indirect evidence for this idea|25|jdm|0|introduction|CONT
for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"|26|jdm|0|introduction|MISC
albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future|27|jdm|0|introduction|MISC
moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good|28|jdm|0|introduction|MISC
that is, more "impatient" individuals contributed less to the public good than did "patient" ones|29|jdm|0|introduction|MISC
arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource|30|jdm|0|introduction|MISC
furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma|31|jdm|0|introduction|MISC
however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma|32|jdm|0|introduction|MISC
an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control|33|jdm|0|introduction|MISC
achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic|34|jdm|0|introduction|MISC
moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"|35|jdm|0|introduction|MISC
halali et al CITATION  report the same for responders, but with a different depletion task|36|jdm|0|introduction|MISC
crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION|37|jdm|0|introduction|MISC
using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region|38|jdm|0|introduction|MISC
trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself|39|jdm|0|introduction|MISC
closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods|40|jdm|0|introduction|MISC
they find both across and within participants that lower response times are associated with more selfish choices|41|jdm|0|introduction|MISC
one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time|42|jdm|0|introduction|MISC
such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games|43|jdm|0|introduction|MISC
in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem|44|jdm|0|introduction|AIM
we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION|45|jdm|0|introduction|OWN
further, we explore the conditions under which we expect an association between self-control and pro-social behavior|46|jdm|0|introduction|OWN
in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1|47|jdm|0|introduction|BASE
critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict|48|jdm|0|introduction|MISC
therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not|49|jdm|0|introduction|MISC
determinants of conflict identification in the face of temptation have been explored only recently|50|jdm|0|introduction|MISC
in some contexts, the question is almost trivial and identification of conflict virtually obvious|51|jdm|0|introduction|MISC
for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs|52|jdm|0|introduction|MISC
however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate|53|jdm|0|introduction|MISC
having this one chocolate alone will not incur major costs, but doing so regularly might|54|jdm|0|introduction|MISC
similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter|55|jdm|0|introduction|MISC
myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively|56|jdm|0|introduction|MISC
they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION|57|jdm|0|introduction|MISC
that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode|58|jdm|0|introduction|MISC
similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation|59|jdm|0|introduction|MISC
if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not|60|jdm|0|introduction|MISC
however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation|61|jdm|0|introduction|MISC
because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation|62|jdm|0|introduction|MISC
myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation|63|jdm|0|introduction|MISC
they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid|64|jdm|0|introduction|MISC
they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities|65|jdm|0|introduction|MISC
consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health|66|jdm|0|introduction|MISC
indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar|67|jdm|0|introduction|MISC
furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict|68|jdm|0|introduction|MISC
that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips|69|jdm|0|introduction|MISC
to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION|70|jdm|0|introduction|OWN
the game thus pits pro-social motivations against self-interest|71|jdm|0|introduction|OWN
if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with|72|jdm|0|introduction|OWN
the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood|73|jdm|0|introduction|OWN
in the case of low likelihood, the slope is expected to be weakly positive|74|jdm|0|introduction|OWN
in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood|75|jdm|0|introduction|OWN
this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not|76|jdm|0|introduction|OWN
  we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially|0|jdm|0|abstract|AIM
  we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies|1|jdm|0|abstract|OWN
  our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood|2|jdm|0|abstract|OWN
  the magnitude of the effect is of economic significance|3|jdm|0|abstract|MISC
  we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially|4|jdm|0|abstract|OWN
  lured by temptation, individuals may find themselves acting against their better judgment|5|jdm|0|introduction|MISC
  self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences|6|jdm|0|introduction|MISC
  for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure|7|jdm|0|introduction|MISC
  the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study|8|jdm|0|introduction|MISC
  and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget|9|jdm|0|introduction|MISC
  perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms|10|jdm|0|introduction|MISC
  this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations|11|jdm|0|introduction|MISC
  that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION|12|jdm|0|introduction|MISC
  for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so|13|jdm|0|introduction|MISC
  nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities|14|jdm|0|introduction|MISC
  that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly|15|jdm|0|introduction|MISC
  self-control-our capacity to overrule temptation-is no less complex than it is important|16|jdm|0|introduction|MISC
  a multitude of conceptualizations exist, many of which are complementary|17|jdm|0|introduction|MISC
  typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION|18|jdm|0|introduction|MISC
  willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION|19|jdm|0|introduction|MISC
  such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens|20|jdm|0|introduction|MISC
  our conceptualization of self-control mirrors these|21|jdm|0|introduction|OWN
  only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control|22|jdm|0|introduction|MISC
  loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation|23|jdm|0|introduction|MISC
  o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health|24|jdm|0|introduction|MISC
  at present, there is but indirect evidence for this idea|25|jdm|0|introduction|CONT
  for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"|26|jdm|0|introduction|MISC
  albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future|27|jdm|0|introduction|MISC
  moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good|28|jdm|0|introduction|MISC
  that is, more "impatient" individuals contributed less to the public good than did "patient" ones|29|jdm|0|introduction|MISC
  arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource|30|jdm|0|introduction|MISC
  furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma|31|jdm|0|introduction|MISC
  however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma|32|jdm|0|introduction|MISC
  an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control|33|jdm|0|introduction|MISC
  achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic|34|jdm|0|introduction|MISC
  moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"|35|jdm|0|introduction|MISC
  halali et al CITATION  report the same for responders, but with a different depletion task|36|jdm|0|introduction|MISC
  crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION|37|jdm|0|introduction|MISC
  using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region|38|jdm|0|introduction|MISC
  trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself|39|jdm|0|introduction|MISC
  closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods|40|jdm|0|introduction|BASE
  they find both across and within participants that lower response times are associated with more selfish choices|41|jdm|0|introduction|MISC
  one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time|42|jdm|0|introduction|MISC
  such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games|43|jdm|0|introduction|MISC
  in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem|44|jdm|0|introduction|AIM
  we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION|45|jdm|0|introduction|OWN
  further, we explore the conditions under which we expect an association between self-control and pro-social behavior|46|jdm|0|introduction|OWN
  in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1|47|jdm|0|introduction|OWN
  critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict|48|jdm|0|introduction|MISC
  therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not|49|jdm|0|introduction|MISC
  determinants of conflict identification in the face of temptation have been explored only recently|50|jdm|0|introduction|MISC
  in some contexts, the question is almost trivial and identification of conflict virtually obvious|51|jdm|0|introduction|MISC
  for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs|52|jdm|0|introduction|MISC
  however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate|53|jdm|0|introduction|MISC
  having this one chocolate alone will not incur major costs, but doing so regularly might|54|jdm|0|introduction|MISC
  similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter|55|jdm|0|introduction|MISC
  myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively|56|jdm|0|introduction|MISC
  they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION|57|jdm|0|introduction|MISC
  that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode|58|jdm|0|introduction|MISC
  similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation|59|jdm|0|introduction|MISC
  if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not|60|jdm|0|introduction|MISC
  however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation|61|jdm|0|introduction|MISC
  because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation|62|jdm|0|introduction|MISC
  myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation|63|jdm|0|introduction|MISC
  they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid|64|jdm|0|introduction|MISC
  they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities|65|jdm|0|introduction|MISC
  consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health|66|jdm|0|introduction|MISC
  indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar|67|jdm|0|introduction|MISC
  furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict|68|jdm|0|introduction|MISC
  that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips|69|jdm|0|introduction|MISC
  to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION|70|jdm|0|introduction|BASE
  the game thus pits pro-social motivations against self-interest|71|jdm|0|introduction|OWN
  if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with|72|jdm|0|introduction|OWN
  the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood|73|jdm|0|introduction|OWN
  in the case of low likelihood, the slope is expected to be weakly positive|74|jdm|0|introduction|OWN
  in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood|75|jdm|0|introduction|OWN
  this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not|76|jdm|0|introduction|OWN
 we test in the context of a dictator game the proposition that individuals may experience a self-control conflict between the temptation to act selfishly and the better judgment to act pro-socially|0|jdm|1|abstract|AIM
 we manipulated the likelihood that individuals would identify self-control conflict, and we measured their trait ability to implement self-control strategies|1|jdm|1|abstract|OWN
 our analysis reveals a positive and significant correlation between trait self-control and pro-social behavior in the treatment where we expected a relatively high likelihood of conflict identification-but not in the treatment where we expected a low likelihood|2|jdm|1|abstract|OWN
 the magnitude of the effect is of economic significance|3|jdm|1|abstract|OWN
 we conclude that subtle cues might prove sufficient to alter individuals' perception of allocation opportunities, thereby prompting individuals to draw on their own cognitive resources to act pro-socially|4|jdm|1|abstract|OWN
 lured by temptation, individuals may find themselves acting against their better judgment|5|jdm|1|introduction|MISC
 self-control failure, famously termed akrasia in plato's protagoras  CITATION , persists throughout domains of daily life and represents a central issue of both philosophy and modern-day social sciences|6|jdm|1|introduction|MISC
 for example, the dieter faced with the opportunity to indulge in a delicious creamy cake may perceive a conflict between indulging and maintaining a good figure|7|jdm|1|introduction|MISC
 the student may feel conflicted between the desire to go to the cinema and her better judgment to stay home and study|8|jdm|1|introduction|MISC
 and, similarly, the fashionista might feel conflicted between the temptation to purchase new boots and her better judgment to maintain a responsible budget|9|jdm|1|introduction|MISC
 perhaps less intuitively, but no less importantly, the question of pro-social versus selfish behavior may be understood in similar terms|10|jdm|1|introduction|MISC
 this conceptualization may help reconcile conflicting notions in economics of selfish and pro-social motivations|11|jdm|1|introduction|MISC
 that individuals should care much about their own self-interest seems almost tautological and requires little further exposition, but that individuals also should care about the interest of others-at the expense of that of their own-has attracted considerable attention  CITATION|12|jdm|1|introduction|MISC
 for example, many individuals voluntarily contribute to charity or to public goods e g , recycling, and they pay their taxes despite low likelihood of punishment for failing to do so|13|jdm|1|introduction|MISC
 nonetheless, one could imagine that even individuals of generally pro-social inclination on occasion may feel tempted to act selfishly and hence underreport income to the authorities|14|jdm|1|introduction|MISC
 that is, pro-social preferences potentially fly in the face of basic urges for personal gain-or greed-and the individual may thus experience a self-control conflict between better judgment to act pro-socially and the temptation to act selfishly|15|jdm|1|introduction|MISC
 self-control-our capacity to overrule temptation-is no less complex than it is important|16|jdm|1|introduction|MISC
 a multitude of conceptualizations exist, many of which are complementary|17|jdm|1|introduction|MISC
 typically, and in line with classic ideas of the conflict between reason and passion, authors view self-control as a "cold" executive function that guides behavior in the face of "hot" impulses to act against better judgment  CITATION|18|jdm|1|introduction|MISC
 willpower, then, represents the combined resources that the executive function-or the planner, in the parlance of thaler and shefrin  CITATION -brings to bear in a deliberate struggle against temptation  CITATION|19|jdm|1|introduction|MISC
 such resources may include cognitive strategies to divert attention away from temptation  CITATION , strategies of pre-commitment  CITATION , or possibly the sheer strength of mind to hold back from the song of the sirens|20|jdm|1|introduction|MISC
 our conceptualization of self-control mirrors these|21|jdm|1|introduction|OWN
 only recently has the psychological literature started to explore how the question of pro-social versus selfish behavior relates to that of self-control|22|jdm|1|introduction|MISC
 loewenstein  CITATION  suggests that selfish behavior may be motivated by visceral urges or drive-states, resembling cravings for relief from hunger, pain, and sexual deprivation|23|jdm|1|introduction|MISC
 o'donoghue and loewenstein  CITATION  argue that such selfish urges may conflict with the "colder", more abstract preferences for altruism, as visceral urges for sweets may conflict with more abstract preferences for a fine figure or good health|24|jdm|1|introduction|MISC
 at present, there is but indirect evidence for this idea|25|jdm|1|introduction|MISC
 for example, pronin et al CITATION  show that decisions about others resemble decisions about "future selves", both classes of which contrast to decisions about less abstract "present selves"|26|jdm|1|introduction|MISC
 albrecht et al CITATION  report consistent results; individuals who choose between immediate and delayed rewards for themselves exhibit less patience and more affective involvement activation in the dopaminergic reward system than do individuals who make such choices for others-or for themselves in the future|27|jdm|1|introduction|MISC
 moreover, curry et al CITATION  find in a standard public goods game that individuals' discount rates are negatively associated with their contributions to the public good|28|jdm|1|introduction|MISC
 that is, more "impatient" individuals contributed less to the public good than did "patient" ones|29|jdm|1|introduction|MISC
 arriving at similar results, fehr and leibbrandt  CITATION  report that patient vs impatient fishermen, whose time preferences were elicited in the lab, exhibited more cooperative behavior in a common resource problem and were in the field less likely to over-exploit the common pool resource|30|jdm|1|introduction|MISC
 furthermore, burks et al CITATION  find that "short-term" patience-the  beta   in the  beta  -&#x3b4;  model-is positively associated with cooperative behavior in a sequential prisoner's dilemma|31|jdm|1|introduction|MISC
 however, duffy and smith  CITATION  report no effect of cognitive load-a manipulation intended to deplete cognitive resources and thereby impair self-control-on outcomes across treatments in a repeated multi-player prisoner's dilemma|32|jdm|1|introduction|MISC
 an emerging literature on the "default" response in games of trust and reciprocity lends further credence to the notion that altruistic responses require self-control|33|jdm|1|introduction|MISC
 achtziger et al CITATION  subjected players in an ultimatum game to cognitive resource depletion, and show that depleted proposers made lower offers-they became less altruistic|34|jdm|1|introduction|MISC
 moreover, depleted responders were more likely to reject offers that were unfair to themselves-they exhibited "altruistic punishment"|35|jdm|1|introduction|MISC
 halali et al CITATION  report the same for responders, but with a different depletion task|36|jdm|1|introduction|MISC
 crockett et al CITATION  subjected responders to acute tryptophan depletion-a procedure that temporarily reduces serotonin levels in the brain and thereby impairs self-control  CITATION ; reduced serotonin levels raised rejection rates and this reduction is positively correlated with impulsive choice in a delay-discounting task  CITATION|37|jdm|1|introduction|MISC
 using a trust game, knoch et al CITATION  subjected trustees' right lateral prefrontal cortex to transcranial magnetic stimulation, which reduces functioning in the targeted brain region|38|jdm|1|introduction|MISC
 trustees, though cognizant that returning a share of the investments was both strategic and norm-compliant, were unable to do so under impaired executive functioning; self-control seems necessary to act on the better judgment to resist the temptation to keep the received investment entirely for oneself|39|jdm|1|introduction|MISC
 closest to our domain of inquiry, piovesan and wengstrom  CITATION  measured response times of participants in a repeated dictator game, lasting 24 periods|40|jdm|1|introduction|MISC
 they find both across and within participants that lower response times are associated with more selfish choices|41|jdm|1|introduction|MISC
 one interpretation of their results is that the default behavior is to act selfishly and that pro-social behavior requires the successful resolution of a self-control conflict, which slows the response time|42|jdm|1|introduction|MISC
 such successful resolution of conflict would require cognitive resources, but hauge et al CITATION  report no effect of cognitive load on players in one-shot dictator games|43|jdm|1|introduction|MISC
 in this paper we attempt a more direct test of the hypothesis that pro-social versus selfish behavior may represent a self-control problem|44|jdm|1|introduction|AIM
 we employ a standard measure of pure pro-social behavior, the one-shot dictator game, which invokes neither concerns for strategy nor for reciprocity; and a well-grounded psychometric measure of self-control, the rosenbaum self-control schedule  CITATION|45|jdm|1|introduction|OWN
 further, we explore the conditions under which we expect an association between self-control and pro-social behavior|46|jdm|1|introduction|OWN
 in so doing, we rely on two conditions necessary for successfully exercising restraint in the face of temptation; myrseth and fishbach  CITATION  propose a two-stage model of self-control, which postulates that an individual in the face of temptation first identifies conflict or not between indulging and pursuing a higher-order goal and, second, that the individual next employs self-control strategies if and only if conflict was identified at the first stage see figure 1|47|jdm|1|introduction|BASE
 critically, self-control strategies are relevant to the decision to indulge only when the individual has identified self-control conflict|48|jdm|1|introduction|MISC
 therefore, one strategy for investigating whether the problem of pro-social versus selfish behavior resembles one of self-control is to test whether the tendency to apply self-control strategies is positively associated with pro-social behavior when individuals have identified self-control conflict, but less so or not at all when individuals have not|49|jdm|1|introduction|MISC
 determinants of conflict identification in the face of temptation have been explored only recently|50|jdm|1|introduction|MISC
 in some contexts, the question is almost trivial and identification of conflict virtually obvious|51|jdm|1|introduction|MISC
 for example, the diabetic dieter probably knows that having even a single, tempting chocolate may incur major costs|52|jdm|1|introduction|MISC
 however, the question of self-control conflict is more ambiguous for the non-diabetic dieter, who faces the same chocolate|53|jdm|1|introduction|MISC
 having this one chocolate alone will not incur major costs, but doing so regularly might|54|jdm|1|introduction|MISC
 similarly, the good citizen may find that a general failure to act generously would represent a major threat to his self-image, but being stingy on just a couple of occasions is a more ambiguous matter|55|jdm|1|introduction|MISC
 myrseth and fishbach  CITATION  use the term epsilon cost temptation to denote tempting opportunities that incur nothing but trivial costs when consumed in small amounts, but potentially serious costs when consumed extensively|56|jdm|1|introduction|MISC
 they argue that individuals identify self-control conflict in the face of epsilon cost temptation if and only if two conditions are met: a the focal consumption opportunity must be viewed in relation to multiple additional opportunities, and b the decision maker must assume that similar choices are made for each opportunity  CITATION|57|jdm|1|introduction|MISC
 that is, considering the question of whether or not to have a delicious creamy cake will evoke self-control conflict in the dieter if the dining opportunity is viewed in relation to future opportunities for dessert consumption, but not if the dining opportunity is viewed in isolation, as a singular episode|58|jdm|1|introduction|MISC
 similarly, the question of whether or not to be generous-to donate to a charitable organization-may elicit self-control conflict if the decision is viewed in relation to future decisions, but not if the decision is viewed in isolation|59|jdm|1|introduction|MISC
 if viewed in relation to future decisions, the question of how much to donate on a single occasion may have bearing on the decision maker's self-image; donating now-and in the future-indicates a generous character, whereas keeping the money for oneself does not|60|jdm|1|introduction|MISC
 however, if viewed in isolation, the question of how much to donate has little bearing on self-image; the present decision of how much to donate is considered only in light of immediate consequences, leaving self-image out of the equation|61|jdm|1|introduction|MISC
 because a consistent self-image represents an important motivator for pro-social behavior  CITATION , we expect that individuals are more likely to identify self-control conflict between selfish and pro-social behavior if the allocation decision is seen in relation to future opportunities than if it is seen in isolation|62|jdm|1|introduction|OWN
 myrseth and fishbach  CITATION  show that subtle framing manipulations are sufficient to influence identification of self-control conflict in the face of epsilon cost temptation|63|jdm|1|introduction|MISC
 they find that presenting a calendar displaying the current month, with a grid separating the dates, raised participants' subsequent consumption of potato chips relative to that of participants whom were presented a calendar without a grid|64|jdm|1|introduction|MISC
 they argue that the gridded calendar activated an isolated versus interrelated frame of the choice opportunity; it made participants more likely to isolate the date in question and thus less likely to see the decision task in relation to similar future opportunities|65|jdm|1|introduction|MISC
 consequently, the grid reduced the likelihood that participants would identify a conflict between the temptation to have chips and the better judgment to maintain a fine figure and good health|66|jdm|1|introduction|MISC
 indeed, participants who viewed the gridded calendar reported experiencing less conflict during their decision to have chips or not than did those who viewed the non-gridded calendar|67|jdm|1|introduction|MISC
 furthermore, participants' trait ability to implement self-control strategies, measured by rosenbaum's  CITATION  psychometric scale, was positively associated with chips consumption for those who viewed the calendar without the grid and who were more likely to identify conflict, but not for others who viewed the calendar with and who were less likely to identify conflict|68|jdm|1|introduction|MISC
 that is, participants who viewed the calendar without the grid, more likely than those who viewed the calendar with, identified self-control conflict and therefore leveraged their self-control strategies to resist the tempting chips|69|jdm|1|introduction|MISC
 to explore our hypothesis that the problem of pro-social versus selfish behavior may represent one of self-control, we have applied the empirical strategy from myrseth and fishbach  CITATION  in the dictator game-a participant is granted an endowment and asked to split it between herself and a recipient  CITATION , and in our case the red cross featured as recipient  CITATION|70|jdm|1|introduction|BASE
 the game thus pits pro-social motivations against self-interest|71|jdm|1|introduction|OWN
 if pro-social versus selfish behavior could represent a self-control conflict, we would expect participants' trait self-control, as measured by rosenbaum's  CITATION  scale, to correlate positively with pro-social behavior for participants who have just previously viewed a calendar without a grid, but less so or not at all for participants who have viewed a calendar with|72|jdm|1|introduction|OWN
 the graph in figure 2 displays donation, as a function of level of self-control, for two different levels of identification likelihood|73|jdm|1|introduction|OWN
 in the case of low likelihood, the slope is expected to be weakly positive|74|jdm|1|introduction|OWN
 in the case of the higher likelihood, however, the slope is expected to be strictly greater than that in the case of low likelihood|75|jdm|1|introduction|OWN
 this means that for a given level of self-control, one might observe substantially different donation behavior depending on whether conflict was identified or not|76|jdm|1|introduction|OWN
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease|0|jdm|0|abstract|MISC
--the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test|1|jdm|0|abstract|MISC
--on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points|2|jdm|0|abstract|AIM
--after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions|3|jdm|0|abstract|OWN
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease|4|jdm|0|introduction|MISC
--for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy|5|jdm|0|introduction|MISC
--the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test|6|jdm|0|introduction|MISC
--the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION|7|jdm|0|introduction|MISC
--specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses|8|jdm|0|introduction|MISC
--on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains|9|jdm|0|introduction|MISC
--several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION|10|jdm|0|introduction|MISC
--on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself|11|jdm|0|introduction|AIM
--these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing|12|jdm|0|introduction|CONT
--we instead derive the potential impact of directly refocusing the decision maker's reference point|13|jdm|0|introduction|OWN
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease|0|jdm|0|abstract|MISC
--the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test|1|jdm|0|abstract|MISC
--on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points|2|jdm|0|abstract|AIM
--after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions|3|jdm|0|abstract|OWN
--in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease|4|jdm|0|introduction|MISC
--for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy|5|jdm|0|introduction|MISC
--the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test|6|jdm|0|introduction|MISC
--the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION|7|jdm|0|introduction|MISC
--specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses|8|jdm|0|introduction|MISC
--on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains|9|jdm|0|introduction|MISC
--several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION|10|jdm|0|introduction|MISC
--on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself|11|jdm|0|introduction|AIM
--these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing|12|jdm|0|introduction|CONT
--we instead derive the potential impact of directly refocusing the decision maker's reference point|13|jdm|0|introduction|OWN
 in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its perceived disutility relative to not testing is greater than the utility associated with prevention of possible disease|0|jdm|1|abstract|MISC
 the prospect theory editing operation, by which a decision maker's reference point is determined, can have important effects on the disutility of the test|1|jdm|1|abstract|MISC
 on the basis of the prospect theory value function, this paper develops two approaches to reducing disutility by directing the decision maker's attention to either actual past or expected future losses that result in shifted reference points|2|jdm|1|abstract|AIM
 after providing a graphical description of the approaches and a mathematical proof of the direction of their effect on judgment, we briefly illustrate the potential value of these approaches with examples from qualitative research on prostate cancer treatment decisions|3|jdm|1|abstract|OWN
 in preventative health decisions, such as the decision to undergo an invasive screening test or treatment, people may be deterred from selecting the test because its disutility relative to not testing is greater than the utility associated with prevention of possible disease|4|jdm|1|introduction|MISC
 for example, people may feel that the anticipated disutility of a colonoscopy for colorectal cancer screening is great enough relative to the expected utilty of prevention of possible colorectal cancer to dissuade them from seeking colonoscopy|5|jdm|1|introduction|MISC
 the prospect theory editing operation  CITATION , by which a decision maker's reference point is determined, can have important impacts on the perceived disutility of the test|6|jdm|1|introduction|MISC
 the work of rothman, salovey, and colleagues on message framing has tested prospect theory predictions of how the description of test outcomes as gains or losses as well as the conceptualization of the purpose of the test as preventative vs diagnostic and the consequent perception of whether the test is "safe" or "risky" can affect test rates  CITATION|7|jdm|1|introduction|MISC
 specifically, message framing theories predict that when a procedure is perceived as risky e g , cancer screening tests may cause a patient to find out that they have cancer, loss-framed messages will promote testing more strongly than gain-framed messages, because people favor risky prospects over sure prospects in the domain of losses|8|jdm|1|introduction|MISC
 on the other hand, when a procedure is perceived as safe e g , sunscreen prevents sunburn and skin cancer, gain-framed messages are predicted to be more effective because people prefer sure prospects to risky prospects in the domain of gains|9|jdm|1|introduction|MISC
 several public health intervention studies have examined message framing and generally found evidence favoring the predictions  CITATION|10|jdm|1|introduction|MISC
 on the basis of the prospect theory value function, this paper develops two approaches to reducing perceived disutility by directing the decision maker's attention to either actual past or expected future losses that can serve as reference points and are not consequences of the test itself|11|jdm|1|introduction|AIM
 these approaches thus differ from message framing, which focuses on how the test outcomes are described and manipulates gain and loss framing|12|jdm|1|introduction|CONT
 we instead derive the potential impact of directly refocusing the decision maker's reference point|13|jdm|1|introduction|OWN
previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making.|0|jdm|0|abstract|MISC
however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics.|1|jdm|0|abstract|CONT
this paper presents new tests that avoid those characteristics.|2|jdm|0|abstract|AIM
expected values of the gambles are nearly equal in each choice.|3|jdm|0|abstract|OWN
in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu|4|jdm|0|abstract|OWN
in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction|5|jdm|0|abstract|OWN
results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters|6|jdm|0|abstract|OWN
new tests of probability-consequence interaction were also conducted|7|jdm|0|abstract|OWN
strong interactions were observed, contrary to ph|8|jdm|0|abstract|OWN
these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making|9|jdm|0|abstract|OWN
this paper compares three models that attempt to describe risky decision making|10|jdm|0|introduction|AIM
these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION|11|jdm|0|introduction|AIM
the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes|12|jdm|0|introduction|MISC
in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model|13|jdm|0|introduction|BASE
birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu|14|jdm|0|introduction|MISC
birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic|15|jdm|0|introduction|MISC
for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION|16|jdm|0|introduction|MISC
some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference|17|jdm|0|introduction|MISC
in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it|18|jdm|0|introduction|MISC
brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox|19|jdm|0|introduction|MISC
 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule|20|jdm|0|introduction|MISC
the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters|21|jdm|0|introduction|CONT
brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice|22|jdm|0|introduction|MISC
in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev|23|jdm|0|introduction|MISC
brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal|24|jdm|0|introduction|CONT
from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev|25|jdm|0|introduction|MISC
brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev|26|jdm|0|introduction|MISC
they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal|27|jdm|0|introduction|MISC
however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly|28|jdm|0|introduction|CONT
brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION|29|jdm|0|introduction|MISC
to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability|30|jdm|0|introduction|MISC
ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability|31|jdm|0|introduction|CONT
in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic|32|jdm|0|introduction|MISC
the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well|33|jdm|0|introduction|CONT
this paper devises a new type of test that avoids the exceptions stated above|34|jdm|0|introduction|AIM
in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal|35|jdm|0|introduction|OWN
in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt|36|jdm|0|introduction|CONT
that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt|37|jdm|0|introduction|OWN
however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph|38|jdm|0|introduction|OWN
previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making.|0|jdm|0|abstract|MISC
however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics.|1|jdm|0|abstract|CONT
this paper presents new tests that avoid those characteristics.|2|jdm|0|abstract|AIM
expected values of the gambles are nearly equal in each choice.|3|jdm|0|abstract|OWN
in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu|4|jdm|0|abstract|OWN
in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction|5|jdm|0|abstract|OWN
results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters|6|jdm|0|abstract|OWN
new tests of probability-consequence interaction were also conducted|7|jdm|0|abstract|OWN
strong interactions were observed, contrary to ph|8|jdm|0|abstract|OWN
these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making|9|jdm|0|abstract|OWN
this paper compares three models that attempt to describe risky decision making|10|jdm|0|introduction|AIM
these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION|11|jdm|0|introduction|AIM
the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes|12|jdm|0|introduction|MISC
in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model|13|jdm|0|introduction|BASE
birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu|14|jdm|0|introduction|MISC
birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic|15|jdm|0|introduction|MISC
for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION|16|jdm|0|introduction|MISC
some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference|17|jdm|0|introduction|MISC
in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it|18|jdm|0|introduction|MISC
brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox|19|jdm|0|introduction|MISC
 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule|20|jdm|0|introduction|MISC
the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters|21|jdm|0|introduction|CONT
brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice|22|jdm|0|introduction|MISC
in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev|23|jdm|0|introduction|MISC
brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal|24|jdm|0|introduction|CONT
from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev|25|jdm|0|introduction|MISC
brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev|26|jdm|0|introduction|MISC
they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal|27|jdm|0|introduction|MISC
however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly|28|jdm|0|introduction|CONT
brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION|29|jdm|0|introduction|MISC
to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability|30|jdm|0|introduction|MISC
ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability|31|jdm|0|introduction|CONT
in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic|32|jdm|0|introduction|MISC
the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well|33|jdm|0|introduction|CONT
this paper devises a new type of test that avoids the exceptions stated above|34|jdm|0|introduction|AIM
in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal|35|jdm|0|introduction|OWN
in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt|36|jdm|0|introduction|CONT
that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt|37|jdm|0|introduction|OWN
however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph|38|jdm|0|introduction|OWN
 previous tests of cumulative prospect theory cpt and of the priority heuristic ph found evidence contradicting these two models of risky decision making|0|jdm|1|abstract|MISC
 however, those tests were criticized because they had characteristics that might "trigger" use of other heuristics|1|jdm|1|abstract|MISC
 this paper presents new tests that avoid those characteristics|2|jdm|1|abstract|AIM
 expected values of the gambles are nearly equal in each choice|3|jdm|1|abstract|OWN
 in addition, if a person followed expected value ev, expected utility eu, cpt, or ph in these tests, she would shift her preferences in the same direction as shifts in ev or eu|4|jdm|1|abstract|OWN
 in contrast, the transfer of attention exchange model tax and a similarity model predict that people will reverse preferences in the opposite direction|5|jdm|1|abstract|MISC
 results contradict the ph, even when ph is modified to include a preliminary similarity evaluation using the ph parameters|6|jdm|1|abstract|CONT
 new tests of probability-consequence interaction were also conducted|7|jdm|1|abstract|OWN
 strong interactions were observed, contrary to ph|8|jdm|1|abstract|OWN
 these results add to the growing bodies of evidence showing that neither cpt nor ph is an accurate description of risky decision making|9|jdm|1|abstract|OWN
 this paper compares three models that attempt to describe risky decision making|10|jdm|1|introduction|AIM
 these models are cumulative prospect theory cpt  CITATION , birnbaum's  CITATION  transfer of attention exchange model tax, and the priority heuristic ph of brandstatter, gigerenzer, and hertwig  CITATION|11|jdm|1|introduction|MISC
 the ph model is based on the idea that people compare one attribute at a time, such as the minimum prizes|12|jdm|1|introduction|MISC
 in addition, the similarity model of rubinstein  CITATION  as modified by leland  CITATION  is also relevant to these studies, although these studies were not designed to test that model|13|jdm|1|introduction|MISC
 birnbaum  CITATION  reviewed a number of critical tests that refute any rank dependent utility rdu model  CITATION  including rank and sign-dependent utility  CITATION , cpt, and expected utility eu|14|jdm|1|introduction|MISC
 birnbaum  CITATION  noted that many of the same tests that refute cpt also contradict the priority heuristic|15|jdm|1|introduction|MISC
 for example, the priority heuristic predicted fewer than half of the modal choices analyzed by birnbaum  CITATION , by birnbaum  CITATION , and by birnbaum and navarrete  CITATION|16|jdm|1|introduction|MISC
 some of these choices included cases where 90% or more of the participants satisfied stochastic dominance but the priority heuristic predicts indifference|17|jdm|1|introduction|MISC
 in other choices, significantly more than half of the participants about 70% of undergraduates violated stochastic dominance, but the priority heuristic predicts that people should satisfy it|18|jdm|1|introduction|MISC
 brandstatter, gigerenzer, and hertwig  CITATION  responded that properties of these choices may have induced people to use other heuristics drawn from a person's "adaptive toolbox"|19|jdm|1|introduction|MISC
 presumably, decision makers first decide what rule to use, then they either apply that rule or choose to use another rule|20|jdm|1|introduction|MISC
 the mechanism that decides what rule to use has not yet been specified; it is described instead with lists of "triggering conditions," which are estimated from data like parameters|21|jdm|1|introduction|MISC
 brandstatter et al CITATION  concluded that the priority heuristic does not apply when there is a stochastic dominance relation in the choice|22|jdm|1|introduction|MISC
 in addition, brandstatter et al CITATION  argued that certain choices reviewed by birnbaum  CITATION  used gambles that differed in expected value ev|23|jdm|1|introduction|MISC
 brandstatter et al CITATION  presented a figure to show that the priority heuristic is not accurate when expected values evs differ, which led them to suppose that two strategies are at work, one for "easy" choices that differ in ev and one for "harder" choices where evs are nearly equal|24|jdm|1|introduction|MISC
 from the data, brandstatter et al CITATION  estimated that, when the ratio of ev exceeds 2, people act as if they choose the gamble with the higher ev|25|jdm|1|introduction|MISC
 brandstatter et al consider ev ratio as a proxy for the "difficulty" of a choice, but do not necessarily hold that people actually compute ratios of ev|26|jdm|1|introduction|MISC
 they argued that the priority heuristic is accurate for "difficult" choices in which evs are nearly equal|27|jdm|1|introduction|MISC
 however, birnbaum  CITATION  noted that ev ratios in birnbaum and navarrete  CITATION  had been inside the region where ph is supposed to apply; in that study, the priority heuristic failed to reproduce even half of the modal choices correctly|28|jdm|1|introduction|MISC
 brandstatter et al CITATION  replicated part of that study and their results confirmed that the priority heuristic reproduced fewer than half of the modal choices that they chose for replication  CITATION|29|jdm|1|introduction|MISC
 to account for the results, brandstatter et al noted that birnbaum and navarrete  CITATION  used many choices in which both gambles of a choice had the same probability distribution and in some choices two branches had the same probability|30|jdm|1|introduction|MISC
 ph was not accurate for such choices, so brandstatter, et al CITATION  theorized that people use a "toting up" heuristic for choices in which two branches had the same probability|31|jdm|1|introduction|MISC
 in some of the choices in birnbaum and navarrete  CITATION , there was a common probability-consequence branch in both choices, which was theorized to trigger editing rules and other heuristics that were called up to account for the failures of the priority heuristic|32|jdm|1|introduction|MISC
 the arguments of brandstatter et al CITATION  might also provide excuses for previous failures of cpt as well|33|jdm|1|introduction|MISC
 this paper devises a new type of test that avoids the exceptions stated above|34|jdm|1|introduction|AIM
 in these tests, one alternative does not stochastically dominate the other, there are no common probability-consequence branches, probabilities of the consequences are not equal, and expected values are nearly equal|35|jdm|1|introduction|OWN
 in addition, unlike previous tests, the new tests use shifts in expected value and expected utility to "help" predictions of ph and cpt|36|jdm|1|introduction|CONT
 that is, expected value and expected utility are both manipulated such that, if a person shifts his or her judgments in the same direction as the changes in eu or ev, his or her choices will appear consistent with ph and cpt|37|jdm|1|introduction|OWN
 however, the choices are designed so that the tax model with parameters typical of previous research predicts that people will shift their choices in the opposite direction of ev, eu, cpt, and ph|38|jdm|1|introduction|OWN
 this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course|0|jdm|0|abstract|AIM
 students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task|1|jdm|0|abstract|OWN
 respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments|2|jdm|0|abstract|OWN
 an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated|3|jdm|0|abstract|OWN
 the paper concludes with a discussion of implications of the present results and possible directions for future research|4|jdm|0|abstract|OWN
 according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind|5|jdm|0|introduction|MISC
 this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category  despite the fact that instances of the first category are more common in the world|6|jdm|0|introduction|MISC
 for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter|7|jdm|0|introduction|MISC
 in fact  it turns out that r appears more often as the third than first letter in english words|8|jdm|0|introduction|MISC
 schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind|9|jdm|0|introduction|MISC
 to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness|10|jdm|0|introduction|MISC
 participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior|11|jdm|0|introduction|MISC
 similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION|12|jdm|0|introduction|MISC
 for a review of this literature see schwarz  CITATION|13|jdm|0|introduction|MISC
 thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated|14|jdm|0|introduction|MISC
 this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION|15|jdm|0|introduction|MISC
 for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task|16|jdm|0|introduction|MISC
 some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade|17|jdm|0|introduction|MISC
 when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task|18|jdm|0|introduction|MISC
 previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results|19|jdm|0|introduction|MISC
 however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence|20|jdm|0|introduction|MISC
 more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale|21|jdm|0|introduction|MISC
 hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale|22|jdm|0|introduction|MISC
 the present investigation overcomes these limitations through a  field study  of students evaluating a course|23|jdm|0|introduction|OWN
 first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment|24|jdm|0|introduction|MISC
 moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course|25|jdm|0|introduction|MISC
 second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses|26|jdm|0|introduction|MISC
 the study of course evaluations is also interesting in its own right|27|jdm|0|introduction|MISC
 a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION|28|jdm|0|introduction|MISC
 thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload|29|jdm|0|introduction|MISC
 to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations|30|jdm|0|introduction|MISC
 the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students|31|jdm|0|introduction|AIM
this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course|0|jdm|0|abstract|BASE
students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task|1|jdm|0|abstract|AIM
respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments|2|jdm|0|abstract|OWN
an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated|3|jdm|0|abstract|OWN
the paper concludes with a discussion of implications of the present results and possible directions for future research|4|jdm|0|abstract|OWN
according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind|5|jdm|0|introduction|MISC
this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category  despite the fact that instances of the first category are more common in the world|6|jdm|0|introduction|MISC
for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter|7|jdm|0|introduction|MISC
in fact  it turns out that r appears more often as the third than first letter in english words|8|jdm|0|introduction|MISC
schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind|9|jdm|0|introduction|MISC
to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness|10|jdm|0|introduction|MISC
participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior|11|jdm|0|introduction|MISC
similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION|12|jdm|0|introduction|MISC
for a review of this literature see schwarz  CITATION|13|jdm|0|introduction|MISC
thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated|14|jdm|0|introduction|MISC
this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION|15|jdm|0|introduction|MISC
for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task|16|jdm|0|introduction|MISC
some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade|17|jdm|0|introduction|MISC
when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task|18|jdm|0|introduction|MISC
previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results|19|jdm|0|introduction|MISC
however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence|20|jdm|0|introduction|CONT
more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale|21|jdm|0|introduction|MISC
hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale|22|jdm|0|introduction|CONT
the present investigation overcomes these limitations through a  field study  of students evaluating a course|23|jdm|0|introduction|AIM
first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment|24|jdm|0|introduction|OWN
moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course|25|jdm|0|introduction|OWN
second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses|26|jdm|0|introduction|OWN
the study of course evaluations is also interesting in its own right|27|jdm|0|introduction|OWN
a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION|28|jdm|0|introduction|OWN
thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload|29|jdm|0|introduction|OWN
to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations|30|jdm|0|introduction|MISC
the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students|31|jdm|0|introduction|OWN
 this paper extends previous research showing that experienced difficulty of recall can influence evaluative judgments CITATION to a field study of university students rating a course|0|jdm|1|abstract|AIM
 students completed a mid-course evaluation form in which they were asked to list either  NUMBER  ways in which the course could be improved a relatively easy task or  NUMBER  ways in which the course could be improved a relatively difficult task|1|jdm|1|abstract|OWN
 respondents who had been asked for  NUMBER  critical comments subsequently rated the course more favorably than respondents who had been asked for  NUMBER  critical comments|2|jdm|1|abstract|OWN
 an internal analysis suggests that the number of critiques solicited provides a frame against which accessibility of instances is evaluated|3|jdm|1|abstract|OWN
 the paper concludes with a discussion of implications of the present results and possible directions for future research|4|jdm|1|abstract|OWN
 according to tversky and kahneman's  CITATION  availability heuristic  people sometimes judge the frequency of events in the world by the ease with which examples come to mind|5|jdm|1|introduction|MISC
 this process has generally been demonstrated by asking participants to assess the relative likelihood of two categories in which instances of the first category are more difficult to recall than instances of the second category  MISC despite the fact that instances of the first category are more common in the world|6|jdm|1|introduction|MISC
 for instance  kahneman and tversky  CITATION  found that most people think the letter r more often appears in english words as the first letter than the third letter  presumably because the first letter provides a better cue for recalling instances of words than does the third letter|7|jdm|1|introduction|MISC
 in fact  it turns out that r appears more often as the third than first letter in english words|8|jdm|1|introduction|MISC
 schwarz et al CITATION  observed that the classic studies demonstrating the availability heuristic failed to distinguish an interpretation based on ease of retrieval from an alternative interpretation based on content of retrieval in which an event is judged more common when a larger number of examples come to mind|9|jdm|1|introduction|MISC
 to tease apart these accounts  schwarz et al CITATION  asked participants in one   study to list either  NUMBER  or  NUMBER  examples of assertive or unassertive behavior that they have exhibited and then rate themselves on their overall degree of assertiveness|10|jdm|1|introduction|MISC
 participants rated themselves as more assertive after they had listed  NUMBER  examples of assertive behavior a relatively easy task rather than  NUMBER  examples a relatively difficult task  similarly  they rated themselves as less assertive i e   more unassertive after they had listed  NUMBER  rather than  NUMBER  examples of unassertive behavior|11|jdm|1|introduction|MISC
 similar patterns of results have been observed in many other studies of frequency-related judgments  including the rate at which a particular letter occurs in various positions of words  CITATION   the quality of one's own memory  CITATION   the frequency of one's own past behaviors  CITATION   one's susceptibility to heart disease  CITATION  and one's susceptibility to sexual assault  CITATION|12|jdm|1|introduction|MISC
 for a review of this literature see schwarz  CITATION|13|jdm|1|introduction|MISC
 thus  an abundance of data supports the original interpretation of the availability heuristic  categories are judged to be more common when instances more easily come to mind  even when a smaller absolute number of instances are generated|14|jdm|1|introduction|MISC
 this program has been extended from frequency-based judgments to  evaluative judgments of such targets as public transportation  CITATION   luxury automobiles  CITATION   and one's own childhood  CITATION|15|jdm|1|introduction|MISC
 for instance  winkielman and schwarz  CITATION  asked participants to recall either  NUMBER  childhood events an easy task or  NUMBER  childhood events a difficult task|16|jdm|1|introduction|MISC
 some participants were then led to believe that memories from pleasant periods tend to fade  while others were led to believe that memories from unpleasant periods tend to fade|17|jdm|1|introduction|MISC
 when later asked to evaluate their childhood  participants believed that pleasant memories fade rated their childhood more favorably when they completed the difficult task  NUMBER  events than the easy task  NUMBER  events  participants who believed that unpleasant memories fade rated their childhood more favorably when they completed the easy rather than difficult task|18|jdm|1|introduction|MISC
 previous studies of the availability heuristic using the paradigm of schwarz et al CITATION  have turned up impressive and robust results|19|jdm|1|introduction|MISC
 however  these demonstrations have been restricted primarily to laboratory surveys in which task of recalling examples then making an overall assessment may seem somewhat artificial to participants and the responses of little consequence|20|jdm|1|introduction|CONT
 more important  most participants in previous studies presumably had little prior experience with the particular likert scale that served as the dependent measure e g   most had never before rated their childhood or public transportation on a  NUMBER -point scale|21|jdm|1|introduction|CONT
 hence  ratings of respondents may be especially susceptible to superficial cues-such as the accessibility of instances-when mapping their beliefs and attitudes onto an unfamiliar response scale|22|jdm|1|introduction|MISC
 the present investigation overcomes these limitations through a  field study  of students evaluating a course|23|jdm|1|introduction|AIM
 first  evaluations are a normal facet of most university courses in which students are commonly asked to list specific suggestions and also provide a global assessment|24|jdm|1|introduction|MISC
 moreover  course evaluations are consequential  as they can influence future course offerings and course staffing  promotion and tenure decisions  and provide information to future prospective students of the target course|25|jdm|1|introduction|MISC
 second  students at universities quickly become familiar with standard course evaluation scales and how ratings are distributed across classes  often relying on these scores in choosing among elective courses|26|jdm|1|introduction|MISC
 the study of course evaluations is also interesting in its own right|27|jdm|1|introduction|MISC
 a number of recent papers have questioned the validity of these ratings  and a lively debate appeared some years ago in the american psychologist  CITATION|28|jdm|1|introduction|MISC
 thus far  questions of discriminant validity have mainly focused on the correlation between teaching ratings and apparently irrelevant factors such as the students' expected grades or the course workload|29|jdm|1|introduction|MISC
 to date there have been few published investigations of the relationship between the design of course feedback forms and summary course evaluations|30|jdm|1|introduction|MISC
 the present study attempts to answer the following provocative question  can one paradoxically obtain higher course ratings by soliciting a greater number of critical comments from students|31|jdm|1|introduction|AIM
 third-party punishment has recently received attention as an explanation for human altruism|0|jdm|0|abstract|MISC
 feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea|1|jdm|0|abstract|MISC
 we investigated the impact of both anger and guilt feelings on third-party sanctions|2|jdm|0|abstract|AIM
 in two studies both emotions were independently manipulated|3|jdm|0|abstract|OWN
 results show that anger and guilt independently constitute sufficient but not necessary causes of punishment|4|jdm|0|abstract|OWN
 low levels of punishment are observed only when neither emotion is elicited|5|jdm|0|abstract|OWN
 we discuss the implications of these findings for the functions of altruistic sanctions|6|jdm|0|abstract|OWN
 people often defend the interests of others|7|jdm|0|introduction|MISC
 they stand up for their friends if someone speaks ill about them in their absence|8|jdm|0|introduction|MISC
 they do not tolerate a colleague being bullied at work|9|jdm|0|introduction|MISC
 they boycott consumer products that are produced using child labor|10|jdm|0|introduction|MISC
 some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger|11|jdm|0|introduction|MISC
 in general, people retaliate against injustice even if they are not directly victimized|12|jdm|0|introduction|MISC
 sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION|13|jdm|0|introduction|MISC
 however, punishing norm-violations is costly in terms of time and energy|14|jdm|0|introduction|MISC
 it may even impose physical risks|15|jdm|0|introduction|MISC
 punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION|16|jdm|0|introduction|MISC
 this begs the question of what incites third-party sanctions, as they usually oppose self-interest|17|jdm|0|introduction|AIM
third-party punishment has recently received attention as an explanation for human altruism|0|jdm|0|abstract|MISC
feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea|1|jdm|0|abstract|CONT
we investigated the impact of both anger and guilt feelings on third-party sanctions|2|jdm|0|abstract|OWN
in two studies both emotions were independently manipulated|3|jdm|0|abstract|OWN
results show that anger and guilt independently constitute sufficient but not necessary causes of punishment|4|jdm|0|abstract|OWN
low levels of punishment are observed only when neither emotion is elicited|5|jdm|0|abstract|OWN
we discuss the implications of these findings for the functions of altruistic sanctions|6|jdm|0|abstract|AIM
people often defend the interests of others|7|jdm|0|introduction|MISC
they stand up for their friends if someone speaks ill about them in their absence|8|jdm|0|introduction|MISC
they do not tolerate a colleague being bullied at work|9|jdm|0|introduction|MISC
they boycott consumer products that are produced using child labor|10|jdm|0|introduction|MISC
some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger|11|jdm|0|introduction|MISC
in general, people retaliate against injustice even if they are not directly victimized|12|jdm|0|introduction|MISC
sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION|13|jdm|0|introduction|MISC
however, punishing norm-violations is costly in terms of time and energy|14|jdm|0|introduction|MISC
it may even impose physical risks|15|jdm|0|introduction|MISC
punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION|16|jdm|0|introduction|MISC
this begs the question of what incites third-party sanctions, as they usually oppose self-interest|17|jdm|0|introduction|AIM
 third-party punishment has recently received attention as an explanation for human altruism|0|jdm|1|abstract|MISC
 feelings of anger in response to norm violations are assumed to motivate third-party sanctions, yet there is only sparse and indirect support for this idea|1|jdm|1|abstract|CONT
 we investigated the impact of both anger and guilt feelings on third-party sanctions|2|jdm|1|abstract|AIM
 in two studies both emotions were independently manipulated|3|jdm|1|abstract|OWN
 results show that anger and guilt independently constitute sufficient but not necessary causes of punishment|4|jdm|1|abstract|OWN
 low levels of punishment are observed only when neither emotion is elicited|5|jdm|1|abstract|OWN
 we discuss the implications of these findings for the functions of altruistic sanctions|6|jdm|1|abstract|OWN
 people often defend the interests of others|7|jdm|1|introduction|MISC
 they stand up for their friends if someone speaks ill about them in their absence|8|jdm|1|introduction|MISC
 they do not tolerate a colleague being bullied at work|9|jdm|1|introduction|MISC
 they boycott consumer products that are produced using child labor|10|jdm|1|introduction|MISC
 some even come to the aid of a stranger who is being physically harassed, in spite of obvious personal danger|11|jdm|1|introduction|MISC
 in general, people retaliate against injustice even if they are not directly victimized|12|jdm|1|introduction|MISC
 sanctioning of norm-violations is vital for prosocial behavior to be sustained  CITATION|13|jdm|1|introduction|MISC
 however, punishing norm-violations is costly in terms of time and energy|14|jdm|1|introduction|MISC
 it may even impose physical risks|15|jdm|1|introduction|MISC
 punishing injustice is therefore considered to be a moral act, particularly when it is performed on behalf of others  CITATION|16|jdm|1|introduction|MISC
 this begs the question of what incites third-party sanctions, as they usually oppose self-interest|17|jdm|1|introduction|MISC
 when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce|0|jdm|0|abstract|MISC
 previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces|1|jdm|0|abstract|MISC
 however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play|2|jdm|0|abstract|AIM
 pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects|3|jdm|0|abstract|OWN
 participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback|4|jdm|0|abstract|OWN
 these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback|5|jdm|0|abstract|OWN
 competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates|6|jdm|0|introduction|MISC
 when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail|7|jdm|0|introduction|MISC
 however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION|8|jdm|0|introduction|MISC
 this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION|9|jdm|0|introduction|MISC
 egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category|10|jdm|0|introduction|MISC
 this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions|11|jdm|0|introduction|MISC
 in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations|12|jdm|0|introduction|MISC
 these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context|13|jdm|0|introduction|MISC
 however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes|14|jdm|0|introduction|MISC
 for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors|15|jdm|0|introduction|MISC
 to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories|16|jdm|0|introduction|MISC
 in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won|17|jdm|0|introduction|MISC
 in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones|18|jdm|0|introduction|MISC
 if they encountered the same hard and easy categories across rounds, the participants learned from feedback|19|jdm|0|introduction|MISC
 that is, the sce shrank-but slowly-across six rounds with the same categories|20|jdm|0|introduction|MISC
 the sce was never eliminated, even after six rounds|21|jdm|0|introduction|MISC
 also, for a seventh round, participants were told there would be new categories|22|jdm|0|introduction|MISC
 the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1|23|jdm|0|introduction|MISC
 these results provide a bleak view of how well people can learn from feedback and avoid sces|24|jdm|0|introduction|MISC
 moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view|25|jdm|0|introduction|MISC
 those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback|26|jdm|0|introduction|MISC
 CITATION are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest|27|jdm|0|introduction|MISC
 we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances|28|jdm|0|introduction|AIM
 by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way|29|jdm|0|introduction|OWN
 in the present study, we examined the influence of repeated feedback on sces|30|jdm|0|introduction|OWN
 however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared|31|jdm|0|introduction|CONT
 in a multi-round paradigm, participants competed in object-tossing competitions|32|jdm|0|introduction|OWN
 in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area|33|jdm|0|introduction|OWN
 there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation|34|jdm|0|introduction|OWN
 full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects|35|jdm|0|introduction|OWN
 we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play|36|jdm|0|introduction|OWN
 in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person|37|jdm|0|introduction|MISC
 also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category|38|jdm|0|introduction|MISC
 however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone|39|jdm|0|introduction|MISC
 for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it|40|jdm|0|introduction|MISC
 for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced|41|jdm|0|introduction|MISC
 consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1|42|jdm|0|introduction|OWN
 that is, they would show significantly reduced sces starting immediately after round 1|43|jdm|0|introduction|OWN
 we also expected that this learning would be generally transferable|44|jdm|0|introduction|OWN
when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce|0|jdm|0|abstract|MISC
previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces|1|jdm|0|abstract|MISC
however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play|2|jdm|0|abstract|AIM
pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects|3|jdm|0|abstract|OWN
participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback|4|jdm|0|abstract|OWN
these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback|5|jdm|0|abstract|OWN
competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates|6|jdm|0|introduction|MISC
when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail|7|jdm|0|introduction|MISC
however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION|8|jdm|0|introduction|MISC
this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION|9|jdm|0|introduction|MISC
egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category|10|jdm|0|introduction|MISC
this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions|11|jdm|0|introduction|MISC
in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations|12|jdm|0|introduction|MISC
these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context|13|jdm|0|introduction|MISC
however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes|14|jdm|0|introduction|MISC
for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors|15|jdm|0|introduction|MISC
to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories|16|jdm|0|introduction|MISC
in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won|17|jdm|0|introduction|MISC
in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones|18|jdm|0|introduction|MISC
if they encountered the same hard and easy categories across rounds, the participants learned from feedback|19|jdm|0|introduction|MISC
that is, the sce shrank-but slowly-across six rounds with the same categories|20|jdm|0|introduction|MISC
the sce was never eliminated, even after six rounds|21|jdm|0|introduction|MISC
also, for a seventh round, participants were told there would be new categories|22|jdm|0|introduction|MISC
the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1|23|jdm|0|introduction|MISC
these results provide a bleak view of how well people can learn from feedback and avoid sces|24|jdm|0|introduction|MISC
moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view|25|jdm|0|introduction|MISC
those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback|26|jdm|0|introduction|MISC
CITATION are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest|27|jdm|0|introduction|MISC
we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances|28|jdm|0|introduction|AIM
by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way|29|jdm|0|introduction|OWN
in the present study, we examined the influence of repeated feedback on sces|30|jdm|0|introduction|OWN
however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared|31|jdm|0|introduction|OWN
in a multi-round paradigm, participants competed in object-tossing competitions|32|jdm|0|introduction|OWN
in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area|33|jdm|0|introduction|OWN
there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation|34|jdm|0|introduction|OWN
full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects|35|jdm|0|introduction|OWN
we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play|36|jdm|0|introduction|OWN
in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person|37|jdm|0|introduction|OWN
also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category|38|jdm|0|introduction|OWN
however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone|39|jdm|0|introduction|OWN
for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it|40|jdm|0|introduction|OWN
for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced|41|jdm|0|introduction|OWN
consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1|42|jdm|0|introduction|OWN
that is, they would show significantly reduced sces starting immediately after round 1|43|jdm|0|introduction|OWN
we also expected that this learning would be generally transferable|44|jdm|0|introduction|OWN
 when judging their likelihood of success in competitive tasks, people tend to be overoptimistic for easy tasks and overpessimistic for hard tasks the shared circumstance effect; sce|0|jdm|1|abstract|MISC
 previous research has shown that feedback and experience from repeated-play competitions has a limited impact on sces|1|jdm|1|abstract|MISC
 however, in this paper, we suggest that competitive situations, in which the shared difficulty or easiness of the task is more transparent, will be more amenable to debiasing via repeated play|2|jdm|1|abstract|AIM
 pairs of participants competed in, made predictions about, and received feedback on, multiple rounds of a throwing task involving both easy- and hard-to-aim objects|3|jdm|1|abstract|OWN
 participants initially showed robust sces, but they also showed a significant reduction in bias after only one round of feedback|4|jdm|1|abstract|OWN
 these and other results support a more positive view than suggested from past research on the potential for sces to be debiased through outcome feedback|5|jdm|1|abstract|OWN
 competition abounds in everyday life, where we contend with others for top grades, jobs, trophies, and mates|6|jdm|1|introduction|MISC
 when resources are at a premium, it is optimal to enter into competitive environments in which we are certain to fare well and to avoid those in which we are doomed to fail|7|jdm|1|introduction|MISC
 however, when people evaluate their likelihood of success in competitions, they are subject to a robust bias: a competitor should consider the strengths and weaknesses of the self and the other competitors  CITATION , but people often give too much weight to evidence related to their own strengths and weaknesses and too little weight to such evidence about the competitor  CITATION|8|jdm|1|introduction|MISC
 this egocentrism results in overoptimism when the circumstances of the competition are favorable, such as when competitors in a trivia game learn that the questions will be from an easy category-even though they'll be easy for everyone  CITATION|9|jdm|1|introduction|MISC
 egocentrism also results in overpessimism when the circumstances are unfavorable e g , a difficult trivia category|10|jdm|1|introduction|MISC
 this phenomenon of being more optimistic when shared competitive circumstances are favorable than when they are not has been dubbed the shared-circumstance effect  CITATION , and it has been replicated across a variety of settings e g , general knowledge tasks, card games, athletic competitions|11|jdm|1|introduction|MISC
 in most previous studies on the sce, participants were presented with novel, non-repeated competitive situations|12|jdm|1|introduction|MISC
 these situations did not allow people to learn from past experiences or from feedback within the immediate competitive context|13|jdm|1|introduction|MISC
 however, in everyday contexts there are often opportunities to learn how a shared circumstance tends to affect the self, others, and outcomes|14|jdm|1|introduction|MISC
 for example, when a tennis tournament is played during a string of windy days, players can have several opportunities to observe how the weather affects themselves and their competitors|15|jdm|1|introduction|MISC
 to examine whether egocentrism and sces persist in repeated-play contexts, rose and windschitl  CITATION  had pairs of participants compete against each other in multiple rounds of a trivia contest that involved easy and hard categories|16|jdm|1|introduction|MISC
 in each round, participants estimated their likelihood of beating their competitor, answered trivia questions, and received feedback about who won|17|jdm|1|introduction|MISC
 in initial rounds, there were robust sces; participants were much more optimistic about winning easy categories than hard ones|18|jdm|1|introduction|MISC
 if they encountered the same hard and easy categories across rounds, the participants learned from feedback|19|jdm|1|introduction|MISC
 that is, the sce shrank-but slowly-across six rounds with the same categories|20|jdm|1|introduction|MISC
 the sce was never eliminated, even after six rounds|21|jdm|1|introduction|MISC
 also, for a seventh round, participants were told there would be new categories|22|jdm|1|introduction|MISC
 the sce for that round dramatically and fully rebounded; it was every bit as large as observed for round 1|23|jdm|1|introduction|MISC
 these results provide a bleak view of how well people can learn from feedback and avoid sces|24|jdm|1|introduction|MISC
 moreover, results from a study by moore and cain  CITATION , which also used repeated plays with feedback, suggest an even bleaker view|25|jdm|1|introduction|MISC
 those researchers also used easy and difficult quizzes as shared-circumstance manipulations, but found virtually no reduction in sces after numerous rounds with feedback CITATION|26|jdm|1|introduction|MISC
 are people's abilities to learn to avoid sces-based on feedback-really as bleak as this prior research might suggest|27|jdm|1|introduction|MISC
 we argue that some shared circumstances are more transparently shared than others, and this may affect how readily people learn to avoid the bias that creates sces-and how easily they can transfer this learning to a slightly new set of shared circumstances|28|jdm|1|introduction|AIM
 by transparently shared, we are referring to how obvious it is that a circumstance that is helpful or hindering to the self will affect others in largely the same way|29|jdm|1|introduction|OWN
 in the present study, we examined the influence of repeated feedback on sces|30|jdm|1|introduction|AIM
 however, unlike past research, we used a competition in which the difficulty of the shared circumstance relative to competitions, for example, involving easy and hard trivia categories is more transparently shared|31|jdm|1|introduction|OWN
 in a multi-round paradigm, participants competed in object-tossing competitions|32|jdm|1|introduction|OWN
 in each round, two competitors each had 8 throws per object-attempting to land the object inside a target area|33|jdm|1|introduction|OWN
 there was always one easy-to-aim object e g , a beanbag and one hard-to-aim object e g , a paper plate, which constituted our shared-circumstance manipulation|34|jdm|1|introduction|OWN
 full feedback was given during and after each round, and predictions were solicited before each round and also before a final round with novel objects|35|jdm|1|introduction|OWN
 we suspected that a tossing competition, rather than a trivia competition, would produce less bleak results about the debiasing of sces through repeated play|36|jdm|1|introduction|OWN
 in the case of trivia, watching one's competitor fail to answer trivia questions doesn't give any insight about why the category is difficult for that person|37|jdm|1|introduction|MISC
 also, knowing that one's competitor struggled on a difficult category does not provide obvious information about why he or she might struggle on another difficult category|38|jdm|1|introduction|MISC
 however, in the case of throwing, watching a competitor fail when tossing an object probably illuminates the relevance of specific shared, situational circumstances i e , the properties of the specific objects as well as a more general awareness of the relevance that object properties have on throwing success-for anyone|39|jdm|1|introduction|MISC
 for example, seeing a paper plate fly unpredictably will likely give an observer a clear impression that the object will fly unpredictably regardless of who is throwing it|40|jdm|1|introduction|MISC
 for participants, this enhances the appreciation that one's struggles are not primarily due to personal characteristics but to properties of the tossed objects; this would then be useful in mitigating egocentrism and sces, even when new objects are introduced|41|jdm|1|introduction|MISC
 consequently, we expected that, even though participants might reveal a robust sce at round 1 prior to any feedback or observations regarding their competitor, they would show a pronounced learning effect after the feedback and observations of round 1|42|jdm|1|introduction|OWN
 that is, they would show significantly reduced sces starting immediately after round 1|43|jdm|1|introduction|OWN
 we also expected that this learning would be generally transferable|44|jdm|1|introduction|OWN
previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making|0|jdm|0|abstract|MISC
despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them|1|jdm|0|abstract|CONT
this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments|2|jdm|0|abstract|AIM
our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait|3|jdm|0|abstract|OWN
these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis|4|jdm|0|abstract|OWN
the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor|5|jdm|0|introduction|MISC
the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION|6|jdm|0|introduction|MISC
further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION|7|jdm|0|introduction|MISC
anchoring thus appears to be a very robust psychological phenomenon|8|jdm|0|introduction|MISC
however  not all individuals may be equally influenced by anchoring cues|9|jdm|0|introduction|MISC
identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process|10|jdm|0|introduction|MISC
one avenue of approach is to investigate the role of individual difference factors|11|jdm|0|introduction|MISC
tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations|12|jdm|0|introduction|MISC
later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference|13|jdm|0|introduction|MISC
recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION|14|jdm|0|introduction|MISC
the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices|15|jdm|0|introduction|MISC
further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age|16|jdm|0|introduction|MISC
taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive|17|jdm|0|introduction|MISC
the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect|18|jdm|0|introduction|AIM
specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects|19|jdm|0|introduction|AIM
in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model|20|jdm|0|introduction|MISC
a great deal of research has supported this model's validity and reliability  CITATION|21|jdm|0|introduction|MISC
while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial  a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it|22|jdm|0|introduction|CONT
the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION|23|jdm|0|introduction|MISC
individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience|24|jdm|0|introduction|MISC
individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION|25|jdm|0|introduction|MISC
a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced|26|jdm|0|introduction|MISC
this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait|27|jdm|0|introduction|MISC
specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION|28|jdm|0|introduction|MISC
therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect  we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects|29|jdm|0|introduction|OWN
specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor|30|jdm|0|introduction|OWN
to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience|31|jdm|0|introduction|OWN
we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER |32|jdm|0|introduction|OWN
previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making.|0|jdm|0|abstract|MISC
despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them.|1|jdm|0|abstract|MISC
this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments.|2|jdm|0|abstract|AIM
our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait.|3|jdm|0|abstract|OWN
these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis|4|jdm|0|abstract|OWN
the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor. |5|jdm|0|introduction|MISC
the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION.|6|jdm|0|introduction|MISC
further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION.|7|jdm|0|introduction|MISC
anchoring thus appears to be a very robust psychological phenomenon.|8|jdm|0|introduction|MISC
however  not all individuals may be equally influenced by anchoring cues.|9|jdm|0|introduction|MISC
identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process.|10|jdm|0|introduction|MISC
one avenue of approach is to investigate the role of individual difference factors.|11|jdm|0|introduction|MISC
tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations.|12|jdm|0|introduction|MISC
later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference.|13|jdm|0|introduction|MISC
recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION.|14|jdm|0|introduction|MISC
the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices.|15|jdm|0|introduction|MISC
further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age.|16|jdm|0|introduction|MISC
taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive.|17|jdm|0|introduction|MISC
the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect.|18|jdm|0|introduction|AIM
specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects.|19|jdm|0|introduction|AIM
in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model.|20|jdm|0|introduction|MISC
a great deal of research has supported this model's validity and reliability  CITATION.|21|jdm|0|introduction|MISC
while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial; a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it. |22|jdm|0|introduction|MISC
the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION. |23|jdm|0|introduction|MISC
individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience.|24|jdm|0|introduction|MISC
individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION.|25|jdm|0|introduction|MISC
a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced.|26|jdm|0|introduction|MISC
this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait.|27|jdm|0|introduction|MISC
specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION.|28|jdm|0|introduction|MISC
therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects.|29|jdm|0|introduction|AIM
specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor.|30|jdm|0|introduction|AIM
to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience.|31|jdm|0|introduction|OWN
we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER |32|jdm|0|introduction|OWN
 previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making|0|jdm|1|abstract|MISC
 despite the prevalence of anchoring effects  researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them|1|jdm|1|abstract|MISC
 this paper examines how one such factor  the big-five personality trait of openness-to-experience  influences the effect of previously presented anchors on participants' judgments|2|jdm|1|abstract|AIM
 our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait|3|jdm|1|abstract|OWN
 these findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis|4|jdm|1|abstract|OWN
 the anchoring effect  CITATION  refers to the adjustment of one's assessment  higher or lower  based upon previously presented external information or an  anchor|5|jdm|1|introduction|MISC
 the anchoring heuristic appears to be prevalent throughout human decision processes and has been shown to reliably influence judgments in a variety of domains including probability estimates  CITATION   negotiation  CITATION   legal judgments  CITATION   and general knowledge  CITATION|6|jdm|1|introduction|MISC
 further  anchoring effects appear viable across most situations for both novices and experts  CITATION  and seem to be effective under conditions of monetary incentives  CITATION  and in real-world settings  CITATION|7|jdm|1|introduction|MISC
 anchoring thus appears to be a very robust psychological phenomenon|8|jdm|1|introduction|MISC
 however  not all individuals may be equally influenced by anchoring cues|9|jdm|1|introduction|MISC
 identification of factors that influence how and in what ways a person is susceptible to this heuristic should further the understanding of the process|10|jdm|1|introduction|MISC
 one avenue of approach is to investigate the role of individual difference factors|11|jdm|1|introduction|MISC
 tversky and kahneman  CITATION  pointed to the important role of  personal characteristics  of the decision maker in risky choice situations|12|jdm|1|introduction|MISC
 later work by stanovich and west  CITATION  suggested that intellectual traits influence decision making and consequential choice preference|13|jdm|1|introduction|MISC
 recently  individual differences have been found in numerical reliance  CITATION   ambiguity  CITATION   preference for actions or inactions  CITATION  and the optimistic bias  CITATION|14|jdm|1|introduction|MISC
 the big-five personality traits  CITATION  have proven to be important individual difference factors for understanding decision choices|15|jdm|1|introduction|MISC
 further  attesting to the importance of individual differences  levin and hart  CITATION  demonstrated that individual differences in preference appear to originate at a very early age|16|jdm|1|introduction|MISC
 taken together  these findings suggest that the impact of individual difference factors on decision-making is both profound and pervasive|17|jdm|1|introduction|MISC
 the purpose of the current study is to investigate how one individual difference factor may influence the strength of the anchoring effect|18|jdm|1|introduction|AIM
 specifically  we are interested in how individual differences in the personality trait of openness-to-experience influences anchoring effects|19|jdm|1|introduction|AIM
 in the last couple of decades the five-factor model of personality has become the most widely tested and well-regarded personality trait model|20|jdm|1|introduction|MISC
 a great deal of research has supported this model's validity and reliability  CITATION|21|jdm|1|introduction|MISC
 while most research has agreed on the nature of the first four factors  the nature of the fifth factor has been controversial  a controversy predominately based upon whether a lexical approach  derived from language frequency within the lexicon of a particular language  CITATION   or a questionnaire approach  CITATION  should be used to measure it|22|jdm|1|introduction|MISC
 the fifth factor is often labeled openness-to-experience  which refers to a propensity to adjust beliefs and behaviors when exposed to new types of information or ideas  CITATION|23|jdm|1|introduction|MISC
 individuals scoring high on this dimension are more open to new ideas  CITATION  and motivated to seek variety and external experience|24|jdm|1|introduction|MISC
 individuals scoring low tend to be less inclined to consider alternative opinions and are more steadfast in their own beliefs  CITATION  making them more likely to rely upon information that is familiar and conventional  CITATION|25|jdm|1|introduction|MISC
 a fundamental aspect of the anchoring effect is that individuals are sensitive to information which they have experienced|26|jdm|1|introduction|MISC
 this change in judgment  which is based upon external cues  seems particularly relevant and related to the openness-to-experience personality trait|27|jdm|1|introduction|MISC
 specifically  as research has shown  the openness trait reflects individual propensities to  adjust  one's beliefs  CITATION  and to consider external information  CITATION|28|jdm|1|introduction|MISC
 therefore  based upon the nature of the openness-to-experience trait and the processes involved in the anchoring effect  we hypothesize that individual differences in openness-to-experience will influence susceptibility to anchoring effects|29|jdm|1|introduction|OWN
 specifically  we hypothesize that the judgments of those individuals high in this trait will be more influenced by previously presented anchors whereas those individuals low in this trait will be less influenced by the anchor|30|jdm|1|introduction|OWN
 to test this hypothesis  we first measured individual levels of the personality trait of openness-to-experience|31|jdm|1|introduction|OWN
 we then provided participants with an anchoring task involving either the mississippi river study  NUMBER  or african nations in the un study  NUMBER |32|jdm|1|introduction|OWN
A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.|0|plos|0|abstract|AIM
The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.|1|plos|0|abstract|MISC
Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.|2|plos|0|abstract|MISC
The proteins are ranked according to the strength of the resistance.|3|plos|0|abstract|MISC
Most of the predicted top-strength proteins have not yet been studied experimentally.|4|plos|0|abstract|MISC
Architectures and folds which are likely to yield large forces are identified.|5|plos|0|abstract|MISC
New types of potent force clamps are discovered.|6|plos|0|abstract|MISC
They involve disulphide bridges and, in particular, cysteine slipknots.|7|plos|0|abstract|MISC
An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.|8|plos|0|abstract|MISC
These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.|9|plos|0|abstract|MISC
A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.|10|plos|0|abstract|MISC
This class is characterized through molecular dynamics simulations.|11|plos|0|abstract|MISC
Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.|12|plos|0|introduction|MISC
Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.|13|plos|0|introduction|MISC
Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.|14|plos|0|introduction|MISC
Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.|15|plos|0|introduction|MISC
They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.|16|plos|0|introduction|CONT
However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.|17|plos|0|introduction|MISC
All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.|18|plos|0|introduction|MISC
Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.|19|plos|0|introduction|MISC
Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.|20|plos|0|introduction|MISC
There are many ways, all phenomenological, to construct a structure-based model of a protein.|21|plos|0|introduction|MISC
504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.|22|plos|0|introduction|MISC
The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.|23|plos|0|introduction|MISC
The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.|24|plos|0|introduction|MISC
Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.|25|plos|0|introduction|MISC
Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.|26|plos|0|introduction|MISC
This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.|27|plos|0|introduction|MISC
The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.|28|plos|0|introduction|MISC
If they do, a contact is declared as native.|29|plos|0|introduction|MISC
Non-native contacts are considered repulsive.|30|plos|0|introduction|MISC
Application of this criterion frequently selects the FORMULA contacts as native.|31|plos|0|introduction|MISC
If the contact map includes these contacts the resulting model will be denoted here as FORMULA.|32|plos|0|introduction|MISC
On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.|33|plos|0|introduction|MISC
Thus the FORMULA couplings should better be removed from the contact map .|34|plos|0|introduction|MISC
The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.|35|plos|0|introduction|MISC
First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.|36|plos|0|introduction|MISC
The first survey also comes with many details of the methodology whereas the second just presents the outcomes.|37|plos|0|introduction|MISC
The two surveys are compared in more details in refs. CITATION, CITATION.|38|plos|0|introduction|MISC
The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.|39|plos|0|introduction|MISC
They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.|40|plos|0|introduction|MISC
Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.|41|plos|0|introduction|MISC
Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.|42|plos|0|introduction|MISC
The large forces most commonly originate in parallel FORMULA that are sheared CITATION.|43|plos|0|introduction|MISC
However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.|44|plos|0|introduction|MISC
The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.|45|plos|0|introduction|MISC
Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.|46|plos|0|introduction|MISC
Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.|47|plos|0|introduction|MISC
These structures are then analyzed through simulations based on the FORMULA model.|48|plos|0|introduction|MISC
The numerical code has been improved to allow for acceleration of calculations by a factor of 2.|49|plos|0|introduction|MISC
The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.|50|plos|0|introduction|MISC
As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.|51|plos|0|introduction|MISC
All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.|52|plos|0|introduction|MISC
In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.|53|plos|0|introduction|OWN
One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.|54|plos|0|introduction|OWN
In this motif, a slip-loop is pulled out of a cysteine knot-loop.|55|plos|0|introduction|OWN
Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.|56|plos|0|introduction|OWN
The two mechanisms are similar in spirit since both involve dragging of the backbone.|57|plos|0|introduction|OWN
However, in the CSK case, two fragments of the backbone are participating.|58|plos|0|introduction|OWN
We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.|59|plos|0|introduction|AIM
The previous surveys did not relate to the SCOP scheme.|60|plos|0|introduction|OWN
We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.|61|plos|0|introduction|OWN
A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.|62|plos|0|introduction|MISC
The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.|63|plos|0|introduction|MISC
On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.|64|plos|0|introduction|MISC
We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.|65|plos|0|introduction|OWN
The current third survey has been performed within the same FORMULA model as the second survey CITATION.|66|plos|0|introduction|MISC
However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.|67|plos|0|introduction|BASE
All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.|68|plos|0|introduction|OWN
A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.|0|plos|0|abstract|AIM
The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.|1|plos|0|abstract|MISC
Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.|2|plos|0|abstract|MISC
The proteins are ranked according to the strength of the resistance.|3|plos|0|abstract|MISC
Most of the predicted top-strength proteins have not yet been studied experimentally.|4|plos|0|abstract|MISC
Architectures and folds which are likely to yield large forces are identified.|5|plos|0|abstract|MISC
New types of potent force clamps are discovered.|6|plos|0|abstract|MISC
They involve disulphide bridges and, in particular, cysteine slipknots.|7|plos|0|abstract|MISC
An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.|8|plos|0|abstract|MISC
These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.|9|plos|0|abstract|MISC
A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.|10|plos|0|abstract|MISC
This class is characterized through molecular dynamics simulations.|11|plos|0|abstract|MISC
Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.|12|plos|0|introduction|MISC
Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.|13|plos|0|introduction|MISC
Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.|14|plos|0|introduction|MISC
Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.|15|plos|0|introduction|MISC
They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.|16|plos|0|introduction|CONT
However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.|17|plos|0|introduction|MISC
All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.|18|plos|0|introduction|MISC
Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.|19|plos|0|introduction|MISC
Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.|20|plos|0|introduction|MISC
There are many ways, all phenomenological, to construct a structure-based model of a protein.|21|plos|0|introduction|MISC
504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.|22|plos|0|introduction|MISC
The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.|23|plos|0|introduction|MISC
The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.|24|plos|0|introduction|MISC
Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.|25|plos|0|introduction|MISC
Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.|26|plos|0|introduction|MISC
This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.|27|plos|0|introduction|MISC
The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.|28|plos|0|introduction|MISC
If they do, a contact is declared as native.|29|plos|0|introduction|MISC
Non-native contacts are considered repulsive.|30|plos|0|introduction|MISC
Application of this criterion frequently selects the FORMULA contacts as native.|31|plos|0|introduction|MISC
If the contact map includes these contacts the resulting model will be denoted here as FORMULA.|32|plos|0|introduction|MISC
On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.|33|plos|0|introduction|MISC
Thus the FORMULA couplings should better be removed from the contact map .|34|plos|0|introduction|MISC
The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.|35|plos|0|introduction|MISC
First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.|36|plos|0|introduction|MISC
The first survey also comes with many details of the methodology whereas the second just presents the outcomes.|37|plos|0|introduction|MISC
The two surveys are compared in more details in refs. CITATION, CITATION.|38|plos|0|introduction|MISC
The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.|39|plos|0|introduction|MISC
They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.|40|plos|0|introduction|MISC
Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.|41|plos|0|introduction|MISC
Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.|42|plos|0|introduction|MISC
The large forces most commonly originate in parallel FORMULA that are sheared CITATION.|43|plos|0|introduction|MISC
However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.|44|plos|0|introduction|MISC
The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.|45|plos|0|introduction|MISC
Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.|46|plos|0|introduction|MISC
Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.|47|plos|0|introduction|MISC
These structures are then analyzed through simulations based on the FORMULA model.|48|plos|0|introduction|MISC
The numerical code has been improved to allow for acceleration of calculations by a factor of 2.|49|plos|0|introduction|MISC
The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.|50|plos|0|introduction|MISC
As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.|51|plos|0|introduction|MISC
All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.|52|plos|0|introduction|MISC
In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.|53|plos|0|introduction|OWN
One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.|54|plos|0|introduction|OWN
In this motif, a slip-loop is pulled out of a cysteine knot-loop.|55|plos|0|introduction|OWN
Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.|56|plos|0|introduction|OWN
The two mechanisms are similar in spirit since both involve dragging of the backbone.|57|plos|0|introduction|OWN
However, in the CSK case, two fragments of the backbone are participating.|58|plos|0|introduction|OWN
We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.|59|plos|0|introduction|AIM
The previous surveys did not relate to the SCOP scheme.|60|plos|0|introduction|OWN
We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.|61|plos|0|introduction|OWN
A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.|62|plos|0|introduction|MISC
The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.|63|plos|0|introduction|MISC
On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.|64|plos|0|introduction|MISC
We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.|65|plos|0|introduction|OWN
The current third survey has been performed within the same FORMULA model as the second survey CITATION.|66|plos|0|introduction|MISC
However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.|67|plos|0|introduction|BASE
All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.|68|plos|0|introduction|OWN
 A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model.|0|plos|1|abstract|AIM
 The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids.|1|plos|1|abstract|OWN
 Our previous studies have dealt with 7510 proteins of no more than 150 amino acids.|2|plos|1|abstract|MISC
 The proteins are ranked according to the strength of the resistance.|3|plos|1|abstract|OWN
 Most of the predicted top-strength proteins have not yet been studied experimentally.|4|plos|1|abstract|OWN
 Architectures and folds which are likely to yield large forces are identified.|5|plos|1|abstract|OWN
 New types of potent force clamps are discovered.|6|plos|1|abstract|OWN
 They involve disulphide bridges and, in particular, cysteine slipknots.|7|plos|1|abstract|OWN
 An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds.|8|plos|1|abstract|OWN
 These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications.|9|plos|1|abstract|OWN
 A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known.|10|plos|1|abstract|OWN
 This class is characterized through molecular dynamics simulations.|11|plos|1|abstract|OWN
 Atomic force microscopy, optical tweezers, and other tools of nanotechnology have enabled induction and monitoring of large conformational changes in biomolecules.|12|plos|1|introduction|MISC
 Such studies are performed to assess structure of the biomolecules, their elastic properties, and ability to act as nanomachines in a cell.|13|plos|1|introduction|MISC
 Stretching studies of proteins CITATION are of a particular current interest and they have been performed for under a hundred of systems.|14|plos|1|introduction|MISC
 Interpretation of some of these experiments has been helped by all-atom simulations, such as reported in refs. CITATION, CITATION.|15|plos|1|introduction|MISC
 They are limited by of order 100 ns time scales and thus require using unrealistically large constant pulling speeds.|16|plos|1|introduction|CONT
 However, they often elucidate the nature of the force clamp the region responsible for the largest force of resistance to pulling, FORMULA.|17|plos|1|introduction|MISC
 All of the experimental and all-atom simulational studies address merely a tiny fraction of proteins that are stored in the Protein Data Bank CITATION.|18|plos|1|introduction|CONT
 Thus it appears worthwhile to consider a large set of proteins and determine their FORMULA within an approximate model that allows for fast and yet reasonably accurate calculations.|19|plos|1|introduction|MISC
 Structure-based models of proteins, as pioneered by Go and his collaborators CITATION and used in several implementations CITATION CITATION, seem to be suited to this task especially well since they are defined in terms of the native structures away from which stretching is imposed.|20|plos|1|introduction|MISC
 There are many ways, all phenomenological, to construct a structure-based model of a protein.|21|plos|1|introduction|MISC
 504 of possible variants are enumerated and 62 are studied in details in ref. CITATION.|22|plos|1|introduction|MISC
 The variants differ by the choice of effective potentials, nature of the local backbone stiffness, energy-related parameters, and of the coarse-grained degrees of freedom.|23|plos|1|introduction|MISC
 The most crucial choice relates to making a decision about which interactions between amino acids count as native contacts.|24|plos|1|introduction|MISC
 Comparing FORMULA to the corresponding experimental values in 36 available cases selects several optimal models CITATION.|25|plos|1|introduction|MISC
 Among them, there is one which is very simple and which describes a protein in terms of its FORMULA atoms, as labeled by the sequential index FORMULA.|26|plos|1|introduction|MISC
 This model is denoted by FORMULA which stands for, respectively, the Lennard-Jones native contact potentials, local backbone stiffness represented by harmonic terms that favor the native values of local chiralities, the contact map in which there are no FORMULA contacts, and the amplitude of the Lennard-Jones potential, FORMULA, is uniform.|27|plos|1|introduction|MISC
 The contact map is determined by assigning the van der Waals spheres to the heavy atoms and by checking whether spheres belonging to different amino acids overlap in the native state CITATION, CITATION.|28|plos|1|introduction|MISC
 If they do, a contact is declared as native.|29|plos|1|introduction|MISC
 Non-native contacts are considered repulsive.|30|plos|1|introduction|MISC
 Application of this criterion frequently selects the FORMULA contacts as native.|31|plos|1|introduction|MISC
 If the contact map includes these contacts the resulting model will be denoted here as FORMULA.|32|plos|1|introduction|MISC
 On average, it performs worse than FORMULA because the FORMULA contacts usually correspond to the weak van der Waals couplings as can be demonstrated in a sample of proteins by using a software CITATION which analyses atomic configurations from the chemical perspective on molecular bonds.|33|plos|1|introduction|MISC
 Thus the FORMULA couplings should better be removed from the contact map .|34|plos|1|introduction|MISC
 The survey to determine FORMULA in 7510 model proteins with the number of amino acids, FORMULA, not exceeding 150 and 239 longer proteins has been accomplished twice.|35|plos|1|introduction|MISC
 First within the FORMULA model CITATION and soon afterwords within the FORMULA model CITATION.|36|plos|1|introduction|MISC
 The first survey also comes with many details of the methodology whereas the second just presents the outcomes.|37|plos|1|introduction|MISC
 The two surveys are compared in more details in refs. CITATION, CITATION.|38|plos|1|introduction|MISC
 The results differ, particularly when it comes to ranking of the proteins according to the value of FORMULA, but they mutually provide the error bars on the findings.|39|plos|1|introduction|MISC
 They both agree, however, on predicting that there are many proteins whose strength should be considerably larger than the frequently studied benchmark the sarcomere protein titin.|40|plos|1|introduction|MISC
 Near the top of the list, there is the scaffoldin protein c7A which has been recently measured to have FORMULA of about 480 pN CITATION.|41|plos|1|introduction|MISC
 Other findings include establishing correlations with the CATH hierarchical classification scheme CITATION, CITATION, such as that there are no strong FORMULA proteins, and identification of several types of the force clamps.|42|plos|1|introduction|MISC
 The large forces most commonly originate in parallel FORMULA that are sheared CITATION.|43|plos|1|introduction|MISC
 However, there are also clamps with antiparallel FORMULA, unstructured strands, and other kinds.|44|plos|1|introduction|MISC
 The two surveys have been based on the structure download made on July 26, 2005 when the PDB comprised 29 385 entries.|45|plos|1|introduction|MISC
 Many of them correspond to nucleic acids, complexes with nucleic acids and with other proteins, carbohydrates, or come with incomplete files and hence the much smaller number of proteins that could be used in the molecular dynamics studies.|46|plos|1|introduction|MISC
 Here, we present results of still another survey which is based on a download of December 18, 2008 which contains 54 807 structure files and leads to 17 134 acceptable structures with FORMULA not exceeding 250.|47|plos|1|introduction|AIM
 These structures are then analyzed through simulations based on the FORMULA model.|48|plos|1|introduction|OWN
 The numerical code has been improved to allow for acceleration of calculations by a factor of 2.|49|plos|1|introduction|OWN
 The 190 structures with the top values of FORMULA in units of FORMULA are shown in Table 1 and Table S1 of the SI, together with the values of titin and ubiquitin to provide a scale.|50|plos|1|introduction|OWN
 As argued in the Materials and Methods section section, the unit of force, FORMULA, is now estimated to be of order 110 pN.|51|plos|1|introduction|OWN
 All of the corresponding proteins are predicted to be much stronger than titin and none but two of them have been studied experimentally yet.|52|plos|1|introduction|OWN
 In addition to the types of force clamps identified before, we have discovered two new mechanisms of sturdiness.|53|plos|1|introduction|OWN
 One of them involves a cysteine slipknot and is found to be operational in all of the 13 top strength proteins.|54|plos|1|introduction|OWN
 In this motif, a slip-loop is pulled out of a cysteine knot-loop.|55|plos|1|introduction|OWN
 Another involves dragging of a single fragment of the main chain across a cysteine knot-loop.|56|plos|1|introduction|OWN
 The two mechanisms are similar in spirit since both involve dragging of the backbone.|57|plos|1|introduction|OWN
 However, in the CSK case, two fragments of the backbone are participating.|58|plos|1|introduction|OWN
 We make a more systematic identification of the CATH-classified architectures that are linked to mechanical strength and then analyze correlations of the data to the SCOP-based grouping CITATION CITATION.|59|plos|1|introduction|OWN
 The previous surveys did not relate to the SCOP scheme.|60|plos|1|introduction|CONT
 We identify the CATH-based architectures and SCOP-based folds that are associated with the occurrence of a strong resistance to pulling.|61|plos|1|introduction|OWN
 A general observation, however, is that each such group of structures may also include examples of proteins that unravel easily.|62|plos|1|introduction|OWN
 The dynamics of a protein are very sensitive to mechanical details that are largely captured by the contact map and not just by the appearance of a structure.|63|plos|1|introduction|OWN
 On the other hand, if one were to look for mechanically strong proteins then the architectures and folds identified by us should provide a good starting point.|64|plos|1|introduction|OWN
 We also study the dependence of FORMULA on the pulling velocity and characterize the dependence on FORMULA through distributions of the forces.|65|plos|1|introduction|OWN
 The current third survey has been performed within the same FORMULA model as the second survey CITATION.|66|plos|1|introduction|MISC
 However, we reuse and extend it here because the editors of Biophysical Journal retracted the second survey CITATION.|67|plos|1|introduction|BASE
 All of the values of FORMULA are deposited at the website LINK and can by accessed by through the PDB structure code.|68|plos|1|introduction|OWN
Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.|0|plos|0|abstract|MISC
Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.|1|plos|0|abstract|MISC
The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.|2|plos|0|abstract|MISC
While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.|3|plos|0|abstract|CONT
Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.|4|plos|0|abstract|AIM
This transition occurs selectively in peptides longer than 37 glutamines.|5|plos|0|abstract|MISC
In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.|6|plos|0|abstract|OWN
We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.|7|plos|0|abstract|OWN
The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.|8|plos|0|abstract|OWN
Our results provide a molecular mechanism for polyQ-mediated aggregation.|9|plos|0|abstract|OWN
The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.|10|plos|0|introduction|MISC
Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.|11|plos|0|introduction|MISC
The aggregates are known to have a characteristic amyloid topology CITATION.|12|plos|0|introduction|MISC
The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.|13|plos|0|introduction|MISC
Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.|14|plos|0|introduction|MISC
The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.|15|plos|0|introduction|MISC
Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.|16|plos|0|introduction|MISC
Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.|17|plos|0|introduction|MISC
Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .|18|plos|0|introduction|MISC
Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.|19|plos|0|introduction|MISC
The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.|20|plos|0|introduction|MISC
Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.|21|plos|0|introduction|MISC
The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .|22|plos|0|introduction|MISC
Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.|0|plos|0|abstract|MISC
Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.|1|plos|0|abstract|MISC
The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.|2|plos|0|abstract|MISC
While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.|3|plos|0|abstract|MISC
Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.|4|plos|0|abstract|AIM
This transition occurs selectively in peptides longer than 37 glutamines.|5|plos|0|abstract|OWN
In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.|6|plos|0|abstract|OWN
We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.|7|plos|0|abstract|OWN
The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.|8|plos|0|abstract|OWN
Our results provide a molecular mechanism for polyQ-mediated aggregation.|9|plos|0|abstract|OWN
The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.|10|plos|0|introduction|MISC
Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.|11|plos|0|introduction|MISC
The aggregates are known to have a characteristic amyloid topology CITATION.|12|plos|0|introduction|MISC
The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.|13|plos|0|introduction|MISC
Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.|14|plos|0|introduction|MISC
The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.|15|plos|0|introduction|MISC
Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.|16|plos|0|introduction|MISC
Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.|17|plos|0|introduction|MISC
Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .|18|plos|0|introduction|MISC
Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.|19|plos|0|introduction|MISC
The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.|20|plos|0|introduction|MISC
Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.|21|plos|0|introduction|MISC
The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .|22|plos|0|introduction|MISC
 Expansion of polyglutamine tracts in proteins results in protein aggregation and is associated with cell death in at least nine neurodegenerative diseases.|0|plos|1|abstract|MISC
 Disease age of onset is correlated with the polyQ insert length above a critical value of 35 40 glutamines.|1|plos|1|abstract|MISC
 The aggregation kinetics of isolated polyQ peptides in vitro also shows a similar critical-length dependence.|2|plos|1|abstract|MISC
 While recent experimental work has provided considerable insights into polyQ aggregation, the molecular mechanism of aggregation is not well understood.|3|plos|1|abstract|CONT
 Here, using computer simulations of isolated polyQ peptides, we show that a mechanism of aggregation is the conformational transition in a single polyQ peptide chain from random coil to a parallel -helix.|4|plos|1|abstract|AIM
 This transition occurs selectively in peptides longer than 37 glutamines.|5|plos|1|abstract|OWN
 In the -helices observed in simulations, all residues adopt -strand backbone dihedral angles, and the polypeptide chain coils around a central helical axis with 18.5 2 residues per turn.|6|plos|1|abstract|OWN
 We also find that mutant polyQ peptides with proline-glycine inserts show formation of antiparallel -hairpins in their ground state, in agreement with experiments.|7|plos|1|abstract|OWN
 The lower stability of mutant -helices explains their lower aggregation rates compared to wild type.|8|plos|1|abstract|OWN
 Our results provide a molecular mechanism for polyQ-mediated aggregation.|9|plos|1|abstract|OWN
 The appearance of polyglutamine -containing aggregates CITATION CITATION is a hallmark of disease progression in all diseases in which CAG-expansions occur in genes CITATION.|10|plos|1|introduction|MISC
 Intranuclear inclusion bodies containing polyQ aggregates have been found in vitro CITATION, CITATION, in cell cultures, animal models, and affected patients CITATION, CITATION.|11|plos|1|introduction|MISC
 The aggregates are known to have a characteristic amyloid topology CITATION.|12|plos|1|introduction|MISC
 The inhibition of oligomerization by the azo-dye Congo red, or by the Hsp70/Hsp40 chaperone system, exerts marked protective effects in vivo and in vitro CITATION, CITATION.|13|plos|1|introduction|MISC
 Aggregation and disease are observed if the number of glutamines in the expansion, n, exceeds a critical value, n C CITATION.|14|plos|1|introduction|MISC
 The nearly universal existence of this criticality in all polyQ-related diseases suggests that when the polyQ insert length exceeds a critical value, a pathological change, largely independent of the host protein, occurs in the polyQ insert itself.|15|plos|1|introduction|MISC
 Therefore, isolated polyQ peptides have been used as model systems for studying polyQ aggregation CITATION, CITATION, CITATION, and it is known that: The nuclear uptake of polyQ peptide aggregates prepared in vitro is cytotoxic in cell cultures CITATION, isolated polyQ peptides have in vitro aggregation properties similar to the corresponding full-length proteins containing the polyQ insert CITATION, CITATION, peptide aggregation follows a nucleated mechanism showing characteristic lag and growth phases CITATION, CITATION, and the glutamine tract-length dependence of the lag-time interval correlates well with the age of onset of disease CITATION.|16|plos|1|introduction|MISC
 Peptides of subcritical lengths have long lag times of aggregation and a corresponding age of onset later than the typical life span of a person.|17|plos|1|introduction|MISC
 Longer peptides have progressively smaller lag times of aggregation, and a correspondingly early age of onset of the disease CITATION .|18|plos|1|introduction|MISC
 Unaggregated polyQ peptides form random coil structures, whereas aggregates are composed of amyloid-like -strands CITATION.|19|plos|1|introduction|MISC
 The conversion of random coil to -strand occurs in an individual polyQ chain CITATION, and fibril formation occurs by addition of other polyQ chains to these monomeric -strand nuclei.|20|plos|1|introduction|MISC
 Therefore, the conformational dynamics of an individual polyQ chain determines both its aggregation mechanism and the structure of the final aggregates.|21|plos|1|introduction|MISC
 The details of the conformational dynamics of polyQ and the length dependence of the dynamics are not well understood CITATION .|22|plos|1|introduction|CONT
The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.|0|plos|0|abstract|MISC
Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.|1|plos|0|abstract|AIM
Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.|2|plos|0|abstract|OWN
For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.|3|plos|0|abstract|OWN
The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.|4|plos|0|abstract|OWN
These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.|5|plos|0|abstract|OWN
The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.|6|plos|0|introduction|MISC
In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.|7|plos|0|introduction|MISC
In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.|8|plos|0|introduction|MISC
However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.|9|plos|0|introduction|MISC
In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.|10|plos|0|introduction|MISC
This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.|11|plos|0|introduction|MISC
These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .|12|plos|0|introduction|MISC
The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.|13|plos|0|introduction|MISC
Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.|14|plos|0|introduction|CONT
Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.|15|plos|0|introduction|CONT
One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.|16|plos|0|introduction|CONT
Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.|17|plos|0|introduction|CONT
However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.|18|plos|0|introduction|MISC
We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.|19|plos|0|introduction|OWN
To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.|20|plos|0|introduction|OWN
We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.|21|plos|0|introduction|OWN
Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.|22|plos|0|introduction|OWN
For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.|23|plos|0|introduction|OWN
The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.|0|plos|0|abstract|MISC
Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.|1|plos|0|abstract|AIM
Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.|2|plos|0|abstract|OWN
For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.|3|plos|0|abstract|OWN
The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.|4|plos|0|abstract|OWN
These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.|5|plos|0|abstract|OWN
The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.|6|plos|0|introduction|MISC
In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.|7|plos|0|introduction|MISC
In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.|8|plos|0|introduction|MISC
However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.|9|plos|0|introduction|MISC
In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.|10|plos|0|introduction|MISC
This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.|11|plos|0|introduction|MISC
These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .|12|plos|0|introduction|MISC
The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.|13|plos|0|introduction|MISC
Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.|14|plos|0|introduction|MISC
Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.|15|plos|0|introduction|MISC
One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.|16|plos|0|introduction|CONT
Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.|17|plos|0|introduction|CONT
However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.|18|plos|0|introduction|MISC
We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.|19|plos|0|introduction|OWN
To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.|20|plos|0|introduction|AIM
We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.|21|plos|0|introduction|OWN
Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.|22|plos|0|introduction|OWN
For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.|23|plos|0|introduction|OWN
 The tradeoff between the need to suppress drug-resistant viruses and the problem of treatment toxicity has led to the development of various drug-sparing HIV-1 treatment strategies.|0|plos|1|abstract|MISC
 Here we use a stochastic simulation model for viral dynamics to investigate how the timing and duration of the induction phase of induction maintenance therapies might be optimized.|1|plos|1|abstract|AIM
 Our model suggests that under a variety of biologically plausible conditions, 6 10 mo of induction therapy are needed to achieve durable suppression and maximize the probability of eradicating viruses resistant to the maintenance regimen.|2|plos|1|abstract|OWN
 For induction regimens of more limited duration, a delayed-induction or -intensification period initiated sometime after the start of maintenance therapy appears to be optimal.|3|plos|1|abstract|OWN
 The optimal delay length depends on the fitness of resistant viruses and the rate at which target-cell populations recover after therapy is initiated.|4|plos|1|abstract|OWN
 These observations have implications for both the timing and the kinds of drugs selected for induction maintenance and therapy-intensification strategies.|5|plos|1|abstract|OWN
 The failure of antiretroviral therapies to completely suppress viral replication in some patients represents a major difficulty in the management of HIV infection.|6|plos|1|introduction|MISC
 In therapy-naive patients without clinically apparent resistance mutations, triple-drug therapy with two nucleoside analog reverse transcriptase inhibitors and a protease inhibitor or a non-nucleoside reverse transcriptase inhibitor is standard CITATION.|7|plos|1|introduction|MISC
 In these patients, treatment success rates, defined as viral load 50 copies/ml at 48 wk, range from 70 percent to 80 percent 85 percent.|8|plos|1|introduction|MISC
 However, in patients with previous regimen failure requiring salvage therapy, response rates are usually considerably lower CITATION CITATION, and it is frequently not possible to assemble a three-drug regimen with uncompromised activity against all viral strains present.|9|plos|1|introduction|MISC
 In these individuals, treatment failure often occurs after an initial period of response to a new regimen, and is usually associated with the appearance of multiply drug-resistant viral strains.|10|plos|1|introduction|MISC
 This has led to attempts to treat highly experienced patients with various deep salvage regimens consisting of four, five, or six individual drugs CITATION CITATION.|11|plos|1|introduction|MISC
 These patients are particularly vulnerable to the many drug interactions CITATION and adverse metabolic, hematologic, neurologic, cardiovascular, and gastrointestinal side effects that complicate HIV therapy and seriously undermine the success of clinical management CITATION CITATION .|12|plos|1|introduction|MISC
 The need to minimize drug resistance while reducing treatment-related toxicities has engendered an interest in induction maintenance strategies, in which a period of intensified antiretroviral therapy is followed by a simplified long-term regimen CITATION CITATION.|13|plos|1|introduction|MISC
 Most such trials have yielded higher failure rates in the treatment group than in controls receiving conventional therapy.|14|plos|1|introduction|MISC
 Failure typically occurs during maintenance therapy, and has been attributed to poor regimen adherence CITATION and recrudescence of resistance mutations present before institution of induction therapy CITATION.|15|plos|1|introduction|MISC
 One weakness of existing studies has been that induction therapy consisted of standard three-drug antiretroviral therapy regimens in common clinical use at the time of the study, under conditions now recognized to permit subclinical viral replication CITATION, CITATION.|16|plos|1|introduction|CONT
 Moreover, in these early studies, the induction phase only lasted between 3 to 6 mo, which may be insufficient.|17|plos|1|introduction|CONT
 However, two recent studies have shown the apparent effectiveness of induction therapy for 48 wk followed by maintenance therapy with atazanavir CITATION or lopinvir/ritonavir CITATION, CITATION, and this has led to new optimism concerning IM approaches.|18|plos|1|introduction|MISC
 We have hypothesized that a longer period of a highly suppressive induction therapy that is appropriately timed relative to the start of maintenance therapy may allow minority resistant variants to decay below a stochastic extinction threshold, allowing for successful long-term treatment with simpler and better-tolerated regimens.|19|plos|1|introduction|OWN
 To explore this hypothesis quantitatively, we constructed a detailed computer simulation model of the dynamics of sensitive and resistant viruses during a hypothetical IM regimen.|20|plos|1|introduction|AIM
 We show that the timing and duration of induction therapy relative to maintenance therapy can affect the probability that viruses resistant to the maintenance regimen will be eradicated in ways that are somewhat counterintuitive.|21|plos|1|introduction|OWN
 Under biologically plausible conditions, we find that 6 10 mo of induction therapy are required to maximize the probability of eradicating these resistant viruses.|22|plos|1|introduction|OWN
 For shorter induction periods, we find that it is optimal to use a delayed-induction regimen administered several days to weeks after the start of the intended long-term maintenance therapy.|23|plos|1|introduction|OWN
The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.|0|plos|0|abstract|MISC
hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.|1|plos|0|abstract|MISC
The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.|2|plos|0|abstract|MISC
The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.|3|plos|0|abstract|MISC
Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.|4|plos|0|abstract|AIM
A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.|5|plos|0|introduction|MISC
In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.|6|plos|0|introduction|MISC
There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.|7|plos|0|introduction|MISC
For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.|8|plos|0|introduction|MISC
Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.|9|plos|0|introduction|MISC
Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.|10|plos|0|introduction|MISC
There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.|11|plos|0|introduction|CONT
It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.|12|plos|0|introduction|AIM
Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.|13|plos|0|introduction|MISC
Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.|14|plos|0|introduction|MISC
Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.|15|plos|0|introduction|MISC
The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.|16|plos|0|introduction|MISC
The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.|17|plos|0|introduction|MISC
Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.|18|plos|0|introduction|CONT
Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.|19|plos|0|introduction|MISC
Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.|20|plos|0|introduction|MISC
Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.|21|plos|0|introduction|MISC
Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.|22|plos|0|introduction|MISC
Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.|23|plos|0|introduction|MISC
A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.|24|plos|0|introduction|MISC
Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.|25|plos|0|introduction|MISC
They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.|26|plos|0|introduction|MISC
They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.|27|plos|0|introduction|MISC
In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.|28|plos|0|introduction|MISC
Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.|29|plos|0|introduction|MISC
Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.|30|plos|0|introduction|MISC
The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.|31|plos|0|introduction|MISC
In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.|32|plos|0|introduction|OWN
The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.|33|plos|0|introduction|OWN
The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.|34|plos|0|introduction|OWN
The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.|35|plos|0|introduction|OWN
Simulation of four hIAPP peptides without lipid molecule was also performed.|36|plos|0|introduction|OWN
Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.|37|plos|0|introduction|OWN
 The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.|0|plos|0|abstract|MISC
 hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.|1|plos|0|abstract|MISC
 The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.|2|plos|0|abstract|MISC
 The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.|3|plos|0|abstract|MISC
 Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.|4|plos|0|abstract|AIM
 A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.|5|plos|0|introduction|MISC
 In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.|6|plos|0|introduction|MISC
 There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.|7|plos|0|introduction|MISC
 For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.|8|plos|0|introduction|MISC
 Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.|9|plos|0|introduction|MISC
 Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.|10|plos|0|introduction|MISC
 There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.|11|plos|0|introduction|MISC
 It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.|12|plos|0|introduction|OWN
 Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.|13|plos|0|introduction|MISC
 Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.|14|plos|0|introduction|MISC
 Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.|15|plos|0|introduction|MISC
 The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.|16|plos|0|introduction|MISC
 The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.|17|plos|0|introduction|MISC
 Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.|18|plos|0|introduction|MISC
 Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.|19|plos|0|introduction|MISC
 Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.|20|plos|0|introduction|MISC
 Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.|21|plos|0|introduction|MISC
 Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.|22|plos|0|introduction|MISC
 Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.|23|plos|0|introduction|MISC
 A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.|24|plos|0|introduction|MISC
 Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.|25|plos|0|introduction|MISC
 They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.|26|plos|0|introduction|MISC
 They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.|27|plos|0|introduction|MISC
 In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.|28|plos|0|introduction|MISC
 Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.|29|plos|0|introduction|MISC
 Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.|30|plos|0|introduction|MISC
 The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.|31|plos|0|introduction|MISC
 In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.|32|plos|0|introduction|OWN
 The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.|33|plos|0|introduction|OWN
 The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.|34|plos|0|introduction|OWN
 The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.|35|plos|0|introduction|OWN
 Simulation of four hIAPP peptides without lipid molecule was also performed.|36|plos|0|introduction|OWN
 Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.|37|plos|0|introduction|OWN
 The influence of lipid molecules on the aggregation of a highly amyloidogenic segment of human islet amyloid polypeptide, hIAPP20 29, and the corresponding sequence from rat has been studied by all-atom replica exchange molecular dynamics simulations with explicit solvent model.|0|plos|1|abstract|MISC
 hIAPP20 29 fragments aggregate into partially ordered -sheet oligomers and then undergo large conformational reorganization and convert into parallel/antiparallel -sheet oligomers in mixed in-register and out-of-register patterns.|1|plos|1|abstract|MISC
 The hydrophobic interaction between lipid tails and residues at positions 23 25 is found to stabilize the ordered -sheet structure, indicating a catalysis role of lipid molecules in hIAPP20 29 self-assembly.|2|plos|1|abstract|MISC
 The rat IAPP variants with three proline residues maintain unstructured micelle-like oligomers, which is consistent with non-amyloidogenic behavior observed in experimental studies.|3|plos|1|abstract|MISC
 Our study provides the atomic resolution descriptions of the catalytic function of lipid molecules on the aggregation of IAPP peptides.|4|plos|1|abstract|AIM
 A range of human diseases including Alzheimer's disease, Parkinson's disease, the spongiform encephalopathy and type 2 diabetes mellitus is associated with amyloid deposits of normally soluble proteins or peptides CITATION CITATION.|5|plos|1|introduction|MISC
 In T2DM, the main protein component of fibrillar protein deposits in the pancreatic islets of langerhans has been identified as a 37-residue hormone referred to as islet amyloid polypeptide or amylin CITATION, which is synthesized in -cells of the pancreas and cosecreted with insulin CITATION, CITATION.|6|plos|1|introduction|MISC
 There are convincing evidences that the toxicity of amyloid related diseases may be caused by the soluble intermediate oligomers instead of mature fibrils CITATION CITATION, and the interaction between lipid bilayer and these soluble oligomer CITATION CITATION.|7|plos|1|introduction|MISC
 For example, channel-like annular structures of oligomers of several amyloidogenic peptides have been observed on the lipid membrane CITATION, CITATION, and have been studied by molecular dynamics simulations as well CITATION, CITATION.|8|plos|1|introduction|MISC
 Moreover, up to 10 percent components in amyloid deposits from patient tissues were lipid molecules, indicating that the lipids can be uptaken from membranes and then wrapped into fibrillar amyloid CITATION CITATION.|9|plos|1|introduction|MISC
 Most studies so far treated the lipid bilayer as a template to exert its influences on the conformation and aggregation properties of peptides CITATION CITATION.|10|plos|1|introduction|MISC
 There is, however, missing information about how individual lipid molecule involving in the peptide aggregation process.|11|plos|1|introduction|CONT
 It will then be beneficial to understand the molecular details of how single lipid molecule influences the assembly process of amyloidogenic peptides which is the main focus of the current study.|12|plos|1|introduction|AIM
 Besides the external factors, such as lipid bilayer, pH value, the sequences of peptide themselves have great effects on the aggregation behaviors.|13|plos|1|introduction|MISC
 Several other species such as non-human primates CITATION, cats CITATION, raccoons CITATION, and rodent species can produce IAPP, but the primary sequence of IAPP varies slightly among species.|14|plos|1|introduction|MISC
 Importantly, IAPP from rodent species, such as rat/mouse IAPP lose capacities of aggregating into amyloid fibrils CITATION, but transgenic mouse models that express human IAPP develop islet deposits CITATION.|15|plos|1|introduction|MISC
 The rIAPP differs from hIAPP in six amino acids and five of them are clustered in a short decapeptide, which is considered to be strongly amyloidogenic and forms similar unbranched fibrils itself to the full-length hIAPP CITATION, CITATION.|16|plos|1|introduction|MISC
 The three proline substitutions in rIAPP20 29 are believed to be highly responsible for the lacking of the amyloidogenic property of the segment or full-length peptide CITATION.|17|plos|1|introduction|MISC
 Although rIAPP has been intensively applied in experimental research acting as a potential peptide inhibitor for peptide aggregation CITATION, CITATION, the molecular mechanism of its resistance to amyloid is still not crystal clear.|18|plos|1|introduction|CONT
 Here, the aggregation of rIAPP20 29 segments is subjected to the same simulation condition as hIAPP20 29 to explore the non-amyloidogenic properties of the peptide and meanwhile to evaluate the simulation results as a negative control.|19|plos|1|introduction|OWN
 Due to the metastable and short-lived nature of soluble pre-fibril oligomers at the early steps of fibril formation, experimental data are usually difficult to obtain CITATION, CITATION.|20|plos|1|introduction|MISC
 Thus, the computational approaches have been employed to complement experimental investigations to gain the insight into the aggregation mechanisms CITATION CITATION.|21|plos|1|introduction|OWN
 Considering multiple copies of peptides needed due to the self-assembly nature of amyloid formation, various simplified representations of molecular systems using implicit solvent models were preferred rather than all-atom models.|22|plos|1|introduction|OWN
 Santini et al. performed ART-OPEP simulations on trimer of A 16 22 by treating side chains as a bead and solvent implicitly CITATION.|23|plos|1|introduction|MISC
 A novel mechanism for single -strand to surmount unnatural registry without dissociation, referred to as reptation was proposed before experimental characterization CITATION.|24|plos|1|introduction|MISC
 Cheon et al. used ProFASi package to reduce the bonded potential energy to include torsional angles only and treated hydrogen bonds explicitly CITATION.|25|plos|1|introduction|MISC
 They were able to carry out two series of 100 Monte Carlo simulations on 20 copies of two fragments A 16 22 and A 25 35.|26|plos|1|introduction|MISC
 They observed early-stage events and obtained an atomic-detailed description of nucleated conformational conversion CITATION model for amyloid aggregation.|27|plos|1|introduction|MISC
 In these studies, simulations were usually started with randomly oriented, extended or random-coiled peptides which underwent ab initio folding to form -sheet oligomers.|28|plos|1|introduction|MISC
 Albeit simplified models allow studying large-scale systems CITATION or observing more events in limited simulation time CITATION, all-atom explicit solvent models can reproduce amyloid aggregation in aqueous environment more accurately and supply more information on sidechain contacts CITATION.|29|plos|1|introduction|MISC
 Nguyen et al. prolonged a series of conventional MD simulations to 300 ns on A 16 22 of 3 6 oligomer size with explicit solvent CITATION.|30|plos|1|introduction|MISC
 The extensive simulations were able to probe the interpeptide sidechain contacts and large conformational fluctuations upon monomer addition to preformed -sheet oligomers in a dock-lock mechanism.|31|plos|1|introduction|MISC
 In our studies, an enhanced-sampling method, replica exchange molecular dynamics CITATION was implemented CITATION, and all water and peptide atoms are treated explicitly by applying OPLS-AA force field CITATION.|32|plos|1|introduction|OWN
 The four copies of amyloidogenic segment hIAPP20 29 and an extra dioleoylphosphatidylcholine lipid molecule were initially set in extended conformation and dispersed in simulation boxes.|33|plos|1|introduction|OWN
 The formation of -sheet containing tetramers, was observed within 100 ns ab initio REMD folding simulations.|34|plos|1|introduction|OWN
 The acquirement of abundant intermediate states suggested two possible -sheet transition pathways.|35|plos|1|introduction|OWN
 Simulation of four hIAPP peptides without lipid molecule was also performed.|36|plos|1|introduction|OWN
 Nonamyloidogenic rat IAPP segments were studied as a negative control with the aim of understanding the inhibitory effect of three proline substitutions.|37|plos|1|introduction|OWN
Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.|0|plos|0|abstract|MISC
A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.|1|plos|0|abstract|OWN
The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.|2|plos|0|abstract|OWN
To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.|3|plos|0|abstract|OWN
Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.|4|plos|0|abstract|OWN
The results demonstrate that the frequency preference is caused by internal face features.|5|plos|0|abstract|OWN
Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.|6|plos|0|abstract|OWN
In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.|7|plos|0|introduction|MISC
Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.|8|plos|0|introduction|MISC
Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .|9|plos|0|introduction|MISC
As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.|10|plos|0|introduction|MISC
This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.|11|plos|0|introduction|MISC
Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.|12|plos|0|introduction|MISC
According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.|13|plos|0|introduction|MISC
An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .|14|plos|0|introduction|MISC
The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.|15|plos|0|introduction|AIM
The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .|16|plos|0|introduction|BASE
Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.|17|plos|0|introduction|MISC
Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.|18|plos|0|introduction|MISC
Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.|19|plos|0|introduction|MISC
Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.|20|plos|0|introduction|MISC
The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.|21|plos|0|introduction|MISC
The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.|22|plos|0|introduction|OWN
The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.|23|plos|0|introduction|OWN
Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.|24|plos|0|introduction|OWN
Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.|25|plos|0|introduction|OWN
The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.|26|plos|0|introduction|OWN
The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.|27|plos|0|introduction|OWN
This observation holds true for all of the internal face features.|28|plos|0|introduction|OWN
The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.|29|plos|0|introduction|OWN
 Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.|0|plos|0|abstract|MISC
 A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.|1|plos|0|abstract|MISC
 The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.|2|plos|0|abstract|AIM
 To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.|3|plos|0|abstract|OWN
 Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.|4|plos|0|abstract|OWN
 The results demonstrate that the frequency preference is caused by internal face features.|5|plos|0|abstract|OWN
 Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.|6|plos|0|abstract|OWN
 In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.|7|plos|0|introduction|MISC
 Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.|8|plos|0|introduction|MISC
 Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .|9|plos|0|introduction|MISC
 As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.|10|plos|0|introduction|MISC
 This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.|11|plos|0|introduction|MISC
 Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.|12|plos|0|introduction|MISC
 According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.|13|plos|0|introduction|MISC
 An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .|14|plos|0|introduction|MISC
 The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.|15|plos|0|introduction|BASE
 The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .|16|plos|0|introduction|MISC
 Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.|17|plos|0|introduction|MISC
 Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.|18|plos|0|introduction|MISC
 Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.|19|plos|0|introduction|MISC
 Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.|20|plos|0|introduction|MISC
 The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.|21|plos|0|introduction|MISC
 The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.|22|plos|0|introduction|AIM
 The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.|23|plos|0|introduction|OWN
 Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.|24|plos|0|introduction|OWN
 Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.|25|plos|0|introduction|OWN
 The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.|26|plos|0|introduction|OWN
 The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.|27|plos|0|introduction|OWN
 This observation holds true for all of the internal face features.|28|plos|0|introduction|OWN
 The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.|29|plos|0|introduction|OWN
 Numerous psychophysical experiments found that humans preferably rely on a narrow band of spatial frequencies for recognition of face identity.|0|plos|1|abstract|MISC
 A recently conducted theoretical study by the author suggests that this frequency preference reflects an adaptation of the brain's face processing machinery to this specific stimulus class.|1|plos|1|abstract|MISC
 The purpose of the present study is to examine this property in greater detail and to specifically elucidate the implication of internal face features.|2|plos|1|abstract|AIM
 To this end, I parameterized Gabor filters to match the spatial receptive field of contrast sensitive neurons in the primary visual cortex.|3|plos|1|abstract|OWN
 Filter responses to a large number of face images were computed, aligned for internal face features, and response-equalized.|4|plos|1|abstract|OWN
 The results demonstrate that the frequency preference is caused by internal face features.|5|plos|1|abstract|OWN
 Thus, the psychophysically observed human frequency bias for face processing seems to be specifically caused by the intrinsic spatial frequency content of internal face features.|6|plos|1|abstract|OWN
 In the brain, the structure of neuronal circuits for processing sensory information matches the statistical properties of the sensory signals CITATION.|7|plos|1|introduction|MISC
 Taking advantage of these statistical regularities contributes to an optimal encoding of sensory signals in neuronal responses, in the sense that the code conveys the highest information with respect to specific constraints CITATION CITATION.|8|plos|1|introduction|MISC
 Among the various constraints which were formulated we find, for example, keeping metabolic energy consumption as low as possible CITATION CITATION, or keeping total wiring length between processing units at a minimum CITATION, or maximizing the suppression of spatio-temporal redundancy in the input signal CITATION, CITATION CITATION .|9|plos|1|introduction|OWN
 As for visual stimuli, natural images reveal a conspicuous statistical regularity that comes as an approximately linear decrease of their amplitude spectra as a function of spatial frequency CITATION CITATION.|10|plos|1|introduction|MISC
 This means that pairs of luminance values are strongly correlated CITATION, and this property could be exploited for gain controlling of visual neurons.|11|plos|1|introduction|MISC
 Then, visual neurons would have equal sensitivities or response amplitudes independent of their spatial frequency preference CITATION.|12|plos|1|introduction|MISC
 According to this response equalization hypothesis, gain should thus be incremented with increasing spatial frequency, such that the distribution of response amplitudes of frequency-tuned neurons to a typical natural image is flat.|13|plos|1|introduction|MISC
 An argument in favor of employing response equalization is that it would lead to an improvement of information transmission from one neuronal stage to another, because the output of one stage would match the limited dynamic range of a second one CITATION .|14|plos|1|introduction|MISC
 The present article builds upon previously reported results for whitened amplitude spectra of face images CITATION : the whitened spectra reveal a spatial frequency maximum at 10 15 cycles per face, but only if external face features are suppressed.|15|plos|1|introduction|BASE
 The predicted frequency maximum nevertheless agrees well with numerous psychophysical experiments, which found that face identity is preferably processed in a narrow band of spatial frequencies from 8 to 16 cycles per face CITATION CITATION .|16|plos|1|introduction|MISC
 Despite of it all, the results presented in CITATION indicate that the maxima in the amplitude spectra are caused by the compound effect of horizontally oriented internal face features.|17|plos|1|introduction|MISC
 Quantitatively, the maxima thus occur in units of cycles per face height, whereas most psychophysical studies instead measure their results in terms of cycles per face width.|18|plos|1|introduction|MISC
 Furthermore, although a clear enhancement of horizontal amplitudes could be observed in the spectra, horizontal amplitudes showed a somewhat noisy dependence on spatial frequency.|19|plos|1|introduction|MISC
 Both effects are a consequence of that face features were not considered individually, what causes a mixing of the spatial frequency content of individual face features in the spectra.|20|plos|1|introduction|MISC
 The mixing leads to averaging-out effects such that any possible enhancement of spectral amplitudes at other than the horizontal orientation goes unnoticed, but also may cause interference effects which lead to the mentioned noisy dependence of amplitudes on spatial frequency.|21|plos|1|introduction|MISC
 The present study addresses the two issues by means of an extensive analysis of face images by means of Gabor filters.|22|plos|1|introduction|AIM
 The filters were thereby parameterized to match the spatial receptive field of band-limited, oriented and contrast sensitive neurons in the primary visual cortex CITATION CITATION.|23|plos|1|introduction|OWN
 Great care has been taken to guarantee the correct alignment of filter responses with respect to the position of internal face features prior to their averaging.|24|plos|1|introduction|OWN
 Doing so permits to precisely elucidate how the frequency dependence of Gabor responses is related to each of the four internal face features.|25|plos|1|introduction|MISC
 The resulting graphs of whitened Gabor amplitudes versus spatial frequency are smooth and reveal distinct maxima at nearly all orientations.|26|plos|1|introduction|OWN
 The most stable maxima, however, are observed at horizontal feature orientations in the first place, but also at vertical orientations.|27|plos|1|introduction|OWN
 This observation holds true for all of the internal face features.|28|plos|1|introduction|OWN
 The present study therefore shows how the individual internal face features contribute to the psychophysically observed frequency preference, and proposes concrete mechanisms of how higher amplitudes of whitened cell responses at an early level could possibly lead to the psychophysically measured effects.|29|plos|1|introduction|OWN
 The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.|0|plos|0|abstract|MISC
 It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.|1|plos|0|abstract|MISC
 These findings have led to the hypothesis that the hippocampus operates using a dual coding system.|2|plos|0|abstract|MISC
 To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.|3|plos|0|abstract|OWN
 We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.|4|plos|0|abstract|AIM
 This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.|5|plos|0|abstract|BASE
 Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.|6|plos|0|abstract|OWN
 The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.|7|plos|0|introduction|MISC
 Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.|8|plos|0|introduction|MISC
 The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.|9|plos|0|introduction|MISC
 Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.|10|plos|0|introduction|MISC
 It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.|11|plos|0|introduction|MISC
 This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.|12|plos|0|introduction|MISC
 These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.|13|plos|0|introduction|MISC
 Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.|14|plos|0|introduction|OWN
 The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.|15|plos|0|introduction|MISC
 The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.|16|plos|0|introduction|MISC
 Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.|17|plos|0|introduction|MISC
 Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.|18|plos|0|introduction|MISC
 Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.|19|plos|0|introduction|MISC
 The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.|20|plos|0|introduction|MISC
 Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.|21|plos|0|introduction|MISC
 Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.|22|plos|0|introduction|MISC
 No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.|23|plos|0|introduction|CONT
 Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.|24|plos|0|introduction|MISC
 It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.|25|plos|0|introduction|MISC
        Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.|26|plos|0|introduction|AIM
 We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.|27|plos|0|introduction|OWN
 Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.|28|plos|0|introduction|OWN
 Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.|29|plos|0|introduction|OWN
 These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.|30|plos|0|introduction|OWN
 Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .|31|plos|0|introduction|OWN
The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.|0|plos|0|abstract|MISC
It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.|1|plos|0|abstract|MISC
These findings have led to the hypothesis that the hippocampus operates using a dual coding system.|2|plos|0|abstract|MISC
To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.|3|plos|0|abstract|OWN
We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.|4|plos|0|abstract|AIM
This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.|5|plos|0|abstract|BASE
Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.|6|plos|0|abstract|OWN
The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.|7|plos|0|introduction|MISC
Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.|8|plos|0|introduction|MISC
The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.|9|plos|0|introduction|MISC
Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.|10|plos|0|introduction|MISC
It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.|11|plos|0|introduction|MISC
This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.|12|plos|0|introduction|MISC
These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.|13|plos|0|introduction|MISC
Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.|14|plos|0|introduction|OWN
The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.|15|plos|0|introduction|MISC
The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.|16|plos|0|introduction|MISC
Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.|17|plos|0|introduction|MISC
Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.|18|plos|0|introduction|MISC
Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.|19|plos|0|introduction|MISC
The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.|20|plos|0|introduction|MISC
Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.|21|plos|0|introduction|MISC
Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.|22|plos|0|introduction|MISC
No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.|23|plos|0|introduction|CONT
Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.|24|plos|0|introduction|MISC
It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.|25|plos|0|introduction|MISC
Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.|26|plos|0|introduction|AIM
We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.|27|plos|0|introduction|OWN
Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.|28|plos|0|introduction|OWN
Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.|29|plos|0|introduction|OWN
These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.|30|plos|0|introduction|OWN
Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .|31|plos|0|introduction|OWN
 The firing rate of single neurons in the mammalian hippocampus has been demonstrated to encode for a range of spatial and non-spatial stimuli.|0|plos|1|abstract|MISC
 It has also been demonstrated that phase of firing, with respect to the theta oscillation that dominates the hippocampal EEG during stereotype learning behaviour, correlates with an animal's spatial location.|1|plos|1|abstract|MISC
 These findings have led to the hypothesis that the hippocampus operates using a dual coding system.|2|plos|1|abstract|MISC
 To investigate the phenomenon of dual coding in the hippocampus, we examine a spiking recurrent network model with theta coded neural dynamics and an STDP rule that mediates rate-coded Hebbian learning when pre- and post-synaptic firing is stochastic.|3|plos|1|abstract|AIM
 We demonstrate that this plasticity rule can generate both symmetric and asymmetric connections between neurons that fire at concurrent or successive theta phase, respectively, and subsequently produce both pattern completion and sequence prediction from partial cues.|4|plos|1|abstract|OWN
 This unifies previously disparate auto- and hetero-associative network models of hippocampal function and provides them with a firmer basis in modern neurobiology.|5|plos|1|abstract|OWN
 Furthermore, the encoding and reactivation of activity in mutually exciting Hebbian cell assemblies demonstrated here is believed to represent a fundamental mechanism of cognitive processing in the brain.|6|plos|1|abstract|MISC
 The hippocampus and surrounding medial temporal lobe are implicated in declarative memory function in humans and other mammals CITATION.|7|plos|1|introduction|MISC
 Electrophysiology studies in a range of species have demonstrated that the activity of single pyramidal cells within this region can encode for the presence of both spatial and non-spatial stimuli CITATION.|8|plos|1|introduction|MISC
 The majority of empirical investigation has focussed on place cells neurons whose firing rate is directly correlated with an animal's spatial location within the corresponding place field CITATION.|9|plos|1|introduction|MISC
 Subsequent research has identified similar single cell responses to a variety of non-spatial cues including odour CITATION, complex visual images CITATION, CITATION, CITATION, running speed CITATION and the concept of a bed or nest CITATION.|10|plos|1|introduction|MISC
 It has also been demonstrated that the exact timing of place cell discharge, relative to the theta oscillation which dominates the hippocampal EEG during learning, correlates with distance travelled through a place field CITATION, CITATION, CITATION CITATION.|11|plos|1|introduction|MISC
 This phase precession mechanism creates a compressed theta coded firing pattern in place cells which corresponds to the sequence of place fields being traversed CITATION.|12|plos|1|introduction|MISC
 These findings have led to the hypothesis that the hippocampus operates using a dual rate and temporal coding system CITATION, CITATION.|13|plos|1|introduction|MISC
 Here we present a spiking neural network model which utilises a dual coding system in order to encode and recall both symmetric and asymmetric connections between neurons that exhibit repeated synchronous and asynchronous firing patterns respectively.|14|plos|1|introduction|AIM
 The postulated mnemonic function of the hippocampus has been extensively modelled using recurrent neural networks, and this approach is supported by empirical data CITATION CITATION.|15|plos|1|introduction|MISC
 The biological correlate of these models is widely believed to be the CA3 region, which exhibits dense recurrent connectivity and wherein synaptic plasticity can be easily and reliably induced.|16|plos|1|introduction|MISC
 Pharmacological and genetic knockout studies have demonstrated that NMDAr-dependent synaptic plasticity in CA3 is critical for the rapid encoding of novel information, and synaptic output from CA3 critical for its retrieval CITATION, CITATION.|17|plos|1|introduction|MISC
 Recurrent neural network models of hippocampal mnemonic function have generally utilised rate-coded Hebbian learning rules to generate reciprocal associations between neurons with concurrently elevated firing rates CITATION, CITATION.|18|plos|1|introduction|MISC
 Hypothetically, this corresponds to the presence of either multiple stimuli or multiple overlapping place fields encountered at a single location CITATION CITATION.|19|plos|1|introduction|MISC
 The hippocampus is also implicated in sequence learning, and temporally asymmetric plasticity rules have subsequently been employed in recurrent network models to generate hetero-associative connections between neurons that fire with repeated temporal correlation CITATION CITATION.|20|plos|1|introduction|MISC
 Hypothetically, this corresponds to a sequence of place fields being traversed or stimuli being encountered on a behavioural timescale CITATION.|21|plos|1|introduction|MISC
 Importantly, previous computational models of hetero-associative learning have typically encoded each successive stage of a learned sequence with the activity of a single neuron, while empirical studies estimate that place fields are typically encoded by an ensemble of several hundred place cells CITATION, CITATION CITATION.|22|plos|1|introduction|MISC
 No computational model has thus far integrated auto- and hetero- associative learning in order to simultaneously generate both bi-directional and asymmetric connections between neurons that are active at the same and successive theta phases respectively using a single temporally asymmetric synaptic plasticity rule.|23|plos|1|introduction|CONT
 Empirical data indicates that changes in the strength of synapses within the hippocampus can depend upon temporal correlations in pre- and post- synaptic firing according to a spike-timing dependent plasticity rule CITATION CITATION.|24|plos|1|introduction|MISC
 It is not yet clear if rate-coded auto-associative network models of hippocampal mnemonic function are compatible with STDP or theta coded neural dynamics.|25|plos|1|introduction|MISC
 Here, we examine the synaptic dynamics generated by several different STDP rules in a spiking recurrent neural network model of CA3 during the encoding of temporal, rate and dual coded activity patterns created by a phenomenological model of phase precession.|26|plos|1|introduction|AIM
 We demonstrate that under certain conditions - the STDP rule can generate both bi-directional connections between neurons which burst at concurrent theta phase and asymmetric connections between neurons which fire at consecutive theta phase.|27|plos|1|introduction|OWN
 Subsequent superthreshold stimulation of a small number of simulated neurons generates putative recall activity, driven by recurrent excitation, that corresponds to pattern completion and/or sequence prediction in auto- and/or hetero- associative connections respectively.|28|plos|1|introduction|OWN
 Interestingly, these neural dynamics are reminiscent of sharp wave ripple activity observed in vivo CITATION CITATION.|29|plos|1|introduction|MISC
 These findings demonstrate that STDP and theta coded neural dynamics are compatible with rate-coded auto-associative network models of hippocampal function.|30|plos|1|introduction|OWN
 Furthermore, the encoding and reactivation of dual coded Hebbian phase sequences of activity in mutually exciting neuronal ensembles demonstrated here has been proposed as a general neural coding mechanism for cognitive processing CITATION, CITATION CITATION .|31|plos|1|introduction|OWN
 Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.|0|plos|0|abstract|MISC
 However, they do not explicitly account for the synthesis of macromolecules.|1|plos|0|abstract|MISC
 Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.|2|plos|0|abstract|OWN
 Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.|3|plos|0|abstract|BASE
 This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.|4|plos|0|abstract|OWN
 Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.|5|plos|0|abstract|OWN
 For example, the model predicted accurately the ribosome production, without any parameterization.|6|plos|0|abstract|OWN
 Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.|7|plos|0|abstract|OWN
 Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.|8|plos|0|abstract|OWN
 This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.|9|plos|0|abstract|OWN
 High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.|10|plos|0|introduction|MISC
 A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.|11|plos|0|introduction|MISC
 One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.|12|plos|0|introduction|MISC
 This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .|13|plos|0|introduction|MISC
 These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.|14|plos|0|introduction|MISC
 Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.|15|plos|0|introduction|MISC
 The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.|16|plos|0|introduction|MISC
 By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.|17|plos|0|introduction|MISC
 Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.|18|plos|0|introduction|MISC
 This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.|19|plos|0|introduction|MISC
 Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.|20|plos|0|introduction|MISC
 The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .|21|plos|0|introduction|MISC
 While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.|22|plos|0|introduction|MISC
 In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.|23|plos|0|introduction|BASE
 We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.|24|plos|0|introduction|OWN
 This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.|25|plos|0|introduction|OWN
 Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.|26|plos|0|introduction|OWN
 Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.|27|plos|0|introduction|MISC
 These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.|28|plos|0|introduction|CONT
 Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.|29|plos|0|introduction|MISC
 In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.|30|plos|0|introduction|MISC
 Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .|31|plos|0|introduction|MISC
 In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.|32|plos|0|introduction|AIM
 We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.|33|plos|0|introduction|OWN
 All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.|34|plos|0|introduction|OWN
 Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.|35|plos|0|introduction|OWN
 This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.|36|plos|0|introduction|OWN
 After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.|37|plos|0|introduction|OWN
 Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.|38|plos|0|introduction|OWN
 Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.|39|plos|0|introduction|OWN
 Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.|40|plos|0|introduction|OWN
Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.|0|plos|0|abstract|MISC
However, they do not explicitly account for the synthesis of macromolecules.|1|plos|0|abstract|MISC
Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.|2|plos|0|abstract|OWN
Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.|3|plos|0|abstract|BASE
This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.|4|plos|0|abstract|OWN
Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.|5|plos|0|abstract|OWN
For example, the model predicted accurately the ribosome production, without any parameterization.|6|plos|0|abstract|OWN
Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.|7|plos|0|abstract|OWN
Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.|8|plos|0|abstract|OWN
This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.|9|plos|0|abstract|OWN
High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.|10|plos|0|introduction|MISC
A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.|11|plos|0|introduction|MISC
One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.|12|plos|0|introduction|MISC
This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .|13|plos|0|introduction|MISC
These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.|14|plos|0|introduction|MISC
Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.|15|plos|0|introduction|MISC
The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.|16|plos|0|introduction|MISC
By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.|17|plos|0|introduction|MISC
Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.|18|plos|0|introduction|MISC
This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.|19|plos|0|introduction|MISC
Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.|20|plos|0|introduction|MISC
The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .|21|plos|0|introduction|MISC
While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.|22|plos|0|introduction|MISC
In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.|23|plos|0|introduction|BASE
We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.|24|plos|0|introduction|OWN
This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.|25|plos|0|introduction|OWN
Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.|26|plos|0|introduction|OWN
Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.|27|plos|0|introduction|MISC
These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.|28|plos|0|introduction|CONT
Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.|29|plos|0|introduction|MISC
In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.|30|plos|0|introduction|MISC
Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .|31|plos|0|introduction|MISC
In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.|32|plos|0|introduction|AIM
We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.|33|plos|0|introduction|OWN
All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.|34|plos|0|introduction|OWN
Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.|35|plos|0|introduction|OWN
This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.|36|plos|0|introduction|OWN
After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.|37|plos|0|introduction|OWN
Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.|38|plos|0|introduction|OWN
Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.|39|plos|0|introduction|OWN
Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.|40|plos|0|introduction|OWN
 Metabolic network reconstructions represent valuable scaffolds for -omics data integration and are used to computationally interrogate network properties.|0|plos|1|abstract|MISC
 However, they do not explicitly account for the synthesis of macromolecules.|1|plos|1|abstract|CONT
 Here, we present the first genome-scale, fine-grained reconstruction of Escherichia coli's transcriptional and translational machinery, which produces 423 functional gene products in a sequence-specific manner and accounts for all necessary chemical transformations.|2|plos|1|abstract|AIM
 Legacy data from over 500 publications and three databases were reviewed, and many pathways were considered, including stable RNA maturation and modification, protein complex formation, and iron sulfur cluster biogenesis.|3|plos|1|abstract|OWN
 This reconstruction represents the most comprehensive knowledge base for these important cellular functions in E. coli and is unique in its scope.|4|plos|1|abstract|OWN
 Furthermore, it was converted into a mathematical model and used to: quantitatively integrate gene expression data as reaction constraints and compute functional network states, which were compared to reported experimental data.|5|plos|1|abstract|AIM
 For example, the model predicted accurately the ribosome production, without any parameterization.|6|plos|1|abstract|OWN
 Also, in silico rRNA operon deletion suggested that a high RNA polymerase density on the remaining rRNA operons is needed to reproduce the reported experimental ribosome numbers.|7|plos|1|abstract|MISC
 Moreover, functional protein modules were determined, and many were found to contain gene products from multiple subsystems, highlighting the functional interaction of these proteins.|8|plos|1|abstract|OWN
 This genome-scale reconstruction of E. coli's transcriptional and translational machinery presents a milestone in systems biology because it will enable quantitative integration of -omics datasets and thus the study of the mechanistic principles underlying the genotype phenotype relationship.|9|plos|1|abstract|OWN
 High-throughput experimental technologies enable the production of heterogeneous data, such as expression profiles and proteomic data, for almost any organism of interest.|10|plos|1|introduction|MISC
 A detailed mathematical representation of the in vivo cellular network is required to obtain a holistic understanding of cellular processes from these data sets and to quantitatively integrate them into a biological context.|11|plos|1|introduction|MISC
 One such approach is the bottom-up network reconstruction, which builds manually networks in a brick-by-brick manner using genome annotation and component-specific information CITATION, CITATION.|12|plos|1|introduction|MISC
 This reconstruction procedure is well established for metabolic reaction networks and has been applied to many organisms, including Human CITATION, Saccharomyces cerevisiae CITATION, CITATION, Leishmani major CITATION, Escherichia coli CITATION, Helicobacter pylori CITATION, Pseudomonas aeruginosa CITATION, and Pseudomonas putida CITATION, CITATION .|13|plos|1|introduction|MISC
 These bottom-up metabolic networks differ from other network reconstructions as they are tailored to the genomic content of the target organism and built manually using biochemical, physiological, and other experimental information in addition to the genome annotation.|14|plos|1|introduction|MISC
 Hence, these reconstructions can be thought of as biochemically, genetically, and genomically structured knowledge bases CITATION.|15|plos|1|introduction|MISC
 The reconstruction and modeling procedure is a 4-step process: obtaining a draft reaction list based on genome annotation and biochemical databases, refinement of reaction list using experimental information, conversion of the reaction list into a computable format and application of systems boundaries to define condition-specific models, and the evaluation and validation of the model content using various mathematical methods.|16|plos|1|introduction|MISC
 By iterating step 2 to 4, reconstructions that are self-consistent within their defined scope can be generated.|17|plos|1|introduction|MISC
 Metabolic network reconstruction have demonstrated to be useful in at least 5 areas of applications CITATION : biological discovery CITATION, phenotypic behavior CITATION, bacterial evolution CITATION, network analysis CITATION, and metabolic engineering CITATION.|18|plos|1|introduction|MISC
 This wide range of applications of the metabolic reconstructions is possible because they can be readily converted into predictive, condition-specific models.|19|plos|1|introduction|MISC
 Unlike more traditional approaches to modeling metabolism, the constraint-based modeling approach requires few, if any, parameters CITATION, CITATION.|20|plos|1|introduction|CONT
 The stoichiometric information encoded in the reconstruction can be represented mathematically as a stoichiometric matrix, S, where the rows correspond to the components and the columns correspond to the reactions .|21|plos|1|introduction|MISC
 While the COBRA approach has been successfully applied to metabolic networks, the same principles and assumptions can be also employed to reconstruct and model other cellular functions, such as signaling CITATION CITATION, regulation CITATION, and protein synthesis CITATION.|22|plos|1|introduction|MISC
 In this study, we extended and refined earlier work by Allen et al., which proposed a stoichiometric formalism to model protein synthesis and illustrated it on some E. coli genes and operons CITATION.|23|plos|1|introduction|BASE
 We created a more detailed, gene-specific representation of the transcriptional and translational processes, which explicitly accounts for the sequence-specific synthesis of DNA, mRNA, and proteins.|24|plos|1|introduction|OWN
 This reconstruction enables quantitative integration of high-throughput data such as gene expression, proteomic, and mRNA degradation data.|25|plos|1|introduction|OWN
 Moreover, proteins are produced in high copy numbers in growing cells; thus, any quantitative mechanistic modeling and analysis of high-throughput data needs to account for the synthesis cost associated with these molecules.|26|plos|1|introduction|MISC
 Numerous studies have been published that investigate protein synthesis using kinetic models CITATION CITATION.|27|plos|1|introduction|MISC
 These models are generally tailored to the questions they address making it difficult to readily apply them for modified problems.|28|plos|1|introduction|CONT
 Since stoichiometric relationships are a common requisite for any type of mechanistic modeling, organism-specific BiGG knowledge bases can be used as templates to derive problem-specific, mechanistic models.|29|plos|1|introduction|MISC
 In fact, network stoichiometry is a dominant feature of kinetic models as well CITATION.|30|plos|1|introduction|MISC
 Thus, network reconstruction serves as a platform for steady-state and kinetic modeling .|31|plos|1|introduction|MISC
 In this study, we present a new generation of network reconstructions, which directly account for the synthesis of individual mRNA and proteins.|32|plos|1|introduction|AIM
 We named the mathematical representation of this reconstruction the Expression matrix, or E-matrix, since it encodes the expression of mRNA and proteins.|33|plos|1|introduction|OWN
 All network reactions were formulated to account for gene-specific and E. coli-specific details, such as nucleotide composition, operon association, and sigma factor usage.|34|plos|1|introduction|OWN
 Furthermore, we used information from three databases and more than 500 scientific publications to formulate mechanistically detailed and accurate reactions.|35|plos|1|introduction|OWN
 This reconstruction is the first comprehensive database detailing the available information for these cellular functions and can thus be deemed a knowledge base.|36|plos|1|introduction|OWN
 After conversion of the E-matrix reconstruction into condition-specific models corresponding to different doubling times, we were able to accurately predict the ribosome production reported in literature, without any parameterization.|37|plos|1|introduction|OWN
 Furthermore, we show that the E-matrix can be used to study the effect of rRNA operon deletion.|38|plos|1|introduction|OWN
 Our results predict that a high density of RNA polymerases is required on the remaining rRNA operons, to achieve the reported ribosome numbers.|39|plos|1|introduction|OWN
 Finally, we show that proteins used in the E-matrix could be grouped into functional modules which lead to a more simplified view of the network.|40|plos|1|introduction|OWN
 Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.|0|plos|0|abstract|MISC
 While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.|1|plos|0|abstract|MISC
 We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.|2|plos|0|abstract|AIM
 Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.|3|plos|0|abstract|OWN
 For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.|4|plos|0|abstract|OWN
 Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.|5|plos|0|abstract|OWN
 At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.|6|plos|0|abstract|OWN
 These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.|7|plos|0|abstract|OWN
 Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.|8|plos|0|abstract|OWN
 The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.|9|plos|0|introduction|MISC
 For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.|10|plos|0|introduction|CONT
 Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.|11|plos|0|introduction|MISC
 Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.|12|plos|0|introduction|MISC
 Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.|13|plos|0|introduction|BASE
 However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.|14|plos|0|introduction|MISC
 Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.|15|plos|0|introduction|MISC
 While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.|16|plos|0|introduction|CONT
 Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.|17|plos|0|introduction|MISC
 Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.|18|plos|0|introduction|CONT
 A simple approach is to add noise sources to deterministic models.|19|plos|0|introduction|MISC
 However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.|20|plos|0|introduction|CONT
 A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.|21|plos|0|introduction|MISC
 However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.|22|plos|0|introduction|CONT
 First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.|23|plos|0|introduction|CONT
 This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.|24|plos|0|introduction|CONT
 Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.|25|plos|0|introduction|CONT
 Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .|26|plos|0|introduction|CONT
 To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.|27|plos|0|introduction|AIM
 We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.|28|plos|0|introduction|OWN
 Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.|29|plos|0|introduction|OWN
 We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.|30|plos|0|introduction|OWN
 We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.|31|plos|0|introduction|OWN
 Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.|32|plos|0|introduction|BASE
 We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.|33|plos|0|introduction|OWN
 By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.|34|plos|0|introduction|OWN
 Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .|35|plos|0|introduction|OWN
Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.|0|plos|0|abstract|MISC
While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.|1|plos|0|abstract|MISC
We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.|2|plos|0|abstract|AIM
Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.|3|plos|0|abstract|OWN
For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.|4|plos|0|abstract|OWN
Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.|5|plos|0|abstract|OWN
At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.|6|plos|0|abstract|OWN
These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.|7|plos|0|abstract|OWN
Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.|8|plos|0|abstract|OWN
The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.|9|plos|0|introduction|MISC
For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.|10|plos|0|introduction|CONT
Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.|11|plos|0|introduction|MISC
Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.|12|plos|0|introduction|MISC
Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.|13|plos|0|introduction|BASE
However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.|14|plos|0|introduction|MISC
Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.|15|plos|0|introduction|MISC
While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.|16|plos|0|introduction|CONT
Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.|17|plos|0|introduction|MISC
Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.|18|plos|0|introduction|CONT
A simple approach is to add noise sources to deterministic models.|19|plos|0|introduction|MISC
However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.|20|plos|0|introduction|CONT
A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.|21|plos|0|introduction|MISC
However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.|22|plos|0|introduction|CONT
First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.|23|plos|0|introduction|CONT
This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.|24|plos|0|introduction|CONT
Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.|25|plos|0|introduction|CONT
Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .|26|plos|0|introduction|CONT
To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.|27|plos|0|introduction|AIM
We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.|28|plos|0|introduction|OWN
Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.|29|plos|0|introduction|OWN
We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.|30|plos|0|introduction|OWN
We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.|31|plos|0|introduction|OWN
Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.|32|plos|0|introduction|BASE
We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.|33|plos|0|introduction|OWN
By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.|34|plos|0|introduction|OWN
Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .|35|plos|0|introduction|OWN
 Neuronal activity is mediated through changes in the probability of stochastic transitions between open and closed states of ion channels.|0|plos|1|abstract|MISC
 While differences in morphology define neuronal cell types and may underlie neurological disorders, very little is known about influences of stochastic ion channel gating in neurons with complex morphology.|1|plos|1|abstract|CONT
 We introduce and validate new computational tools that enable efficient generation and simulation of models containing stochastic ion channels distributed across dendritic and axonal membranes.|2|plos|1|abstract|AIM
 Comparison of five morphologically distinct neuronal cell types reveals that when all simulated neurons contain identical densities of stochastic ion channels, the amplitude of stochastic membrane potential fluctuations differs between cell types and depends on sub-cellular location.|3|plos|1|abstract|OWN
 For typical neurons, the amplitude of membrane potential fluctuations depends on channel kinetics as well as open probability.|4|plos|1|abstract|MISC
 Using a detailed model of a hippocampal CA1 pyramidal neuron, we show that when intrinsic ion channels gate stochastically, the probability of initiation of dendritic or somatic spikes by dendritic synaptic input varies continuously between zero and one, whereas when ion channels gate deterministically, the probability is either zero or one.|5|plos|1|abstract|OWN
 At physiological firing rates, stochastic gating of dendritic ion channels almost completely accounts for probabilistic somatic and dendritic spikes generated by the fully stochastic model.|6|plos|1|abstract|OWN
 These results suggest that the consequences of stochastic ion channel gating differ globally between neuronal cell-types and locally between neuronal compartments.|7|plos|1|abstract|OWN
 Whereas dendritic neurons are often assumed to behave deterministically, our simulations suggest that a direct consequence of stochastic gating of intrinsic ion channels is that spike output may instead be a probabilistic function of patterns of synaptic input to dendrites.|8|plos|1|abstract|OWN
 The appropriate level of physical detail required to understand how complex processes such as cognition and behavior emerge from more simple biological structures is unclear CITATION, CITATION.|9|plos|1|introduction|MISC
 For example, while it is possible to account for certain aspects of nervous system function using models that represent each neuron as a simple integrate and fire device, it is increasingly clear that this approach does not capture the full range of computations that many real neurons carry out CITATION, CITATION.|10|plos|1|introduction|MISC
 Dendritic and axonal morphology are defining features of neuronal cell types and have important influences on the computations that a neuron performs CITATION.|11|plos|1|introduction|MISC
 Differences in morphology determine how neurons respond to synaptic input and are sufficient to produce distinct patterns of spontaneous activity CITATION and degrees of action potential back-propagation from the soma into the dendrites CITATION.|12|plos|1|introduction|MISC
 Cable theory and compartmental modeling provide a foundation for predicting the propagation of electrical signals in the dendrites and axons of neurons CITATION, CITATION.|13|plos|1|introduction|MISC
 However, while the assumption that transitions between open and closed states of ion channels can be treated as a deterministic process may be sufficient for some purposes, recent evidence suggests that stochastic transitions between the states of individual ion channels could influence computations carried out by neurons CITATION CITATION.|14|plos|1|introduction|CONT
 Stochastic opening and closing of ion channels causes noisy fluctuations in the current or voltage recorded from a neuron CITATION CITATION.|15|plos|1|introduction|MISC
 While cable theory suggests that fluctuations of this kind might be particularly important in fine structures such as axons and dendrites CITATION, we nevertheless know very little about how neuronal morphology and stochastic gating of ion channels interact to determine how neurons respond to synaptic input.|16|plos|1|introduction|CONT
 Given the difficulty of reducing detailed morphological models to simple analytical forms that could also incorporate stochastic gating of individual ion channels CITATION, experimentally constrained numerical simulations will be important to enable these issues to be explored systematically.|17|plos|1|introduction|MISC
 Investigation of stochastic ion channel gating using numerical simulations has been limited by trades-offs between simulation accuracy and computation time CITATION.|18|plos|1|introduction|CONT
 A simple approach is to add noise sources to deterministic models.|19|plos|1|introduction|MISC
 However, as ion channels have multiple functional states with transitions that often depend on the membrane voltage CITATION, CITATION, CITATION, CITATION, this may not accurately account for the noise introduced by ion channel currents.|20|plos|1|introduction|CONT
 A more accurate alternative is to explicitly model transitions between different functional states for each ion channel on a neuron's membrane.|21|plos|1|introduction|MISC
 However, for neurons with complex axonal or dendritic architectures there are two substantial obstacles to this approach.|22|plos|1|introduction|CONT
 First, typical central neurons express large numbers of ion channels and simulations must be repeated many times to obtain statistically valid descriptions CITATION.|23|plos|1|introduction|CONT
 This is a formidable computational task and even relatively straightforward simulations of the consequences of stochastic channel gating can require substantial computing time.|24|plos|1|introduction|CONT
 Second, each neuronal ion channel occupies a specific location on the extra-cellular membrane, whereas most neuronal models represent the distribution of ion channels as the density of a deterministic conductance across an area of membrane.|25|plos|1|introduction|CONT
 Although this formalism has been successful for simulating many aspects of neuronal activity, it is of less use for models that explore the consequences of the localization of individual ion channels, for example to evaluate the macroscopic effects of short range interactions between ion channels and other signaling molecules CITATION, or the consequences of spatially heterogeneous distributions of ion channels within relatively small sub-cellular structures such as dendritic spines and axon terminals CITATION, CITATION .|26|plos|1|introduction|CONT
 To address the functional consequences of stochastic ion channel gating in neurons with extensive dendritic or axonal arborizations we developed a parallel stochastic ion channel simulator, which enables efficient simulation of the electrical activity of neurons with complex morphologies and arbitrary localization of stochastic ion channels on the extracellular membrane, while also addressing limitations of previous approaches.|27|plos|1|introduction|AIM
 We have also developed an interactive tool for visualization and development of models of neurons containing uniquely located ion channels.|28|plos|1|introduction|OWN
 Here, we illustrate the use of PSICS and ICING, outline the computational strategies used and provide benchmark data for evaluation.|29|plos|1|introduction|OWN
 We then identify previously unappreciated differences between the effects of stochastic ion channel gating on somatic and dendritic membrane potential activity in several different morphological classes of neuron.|30|plos|1|introduction|OWN
 We show that the consequences of stochastic gating depend on dendritic morphology and suggest novel functional roles for the kinetics of ion channel gating.|31|plos|1|introduction|OWN
 Using a previously well-validated realistic model of a CA1 pyramidal neuron we demonstrate that stochastic ion channel gating influences spike output in response to dendritic synaptic input.|32|plos|1|introduction|BASE
 We show that stochastic gating of axonal or dendritic ion channels substantially modifies synaptically driven dendritic and axonal spike output, with stochastic gating of voltage-dependent sodium and potassium channels having the greatest impact and hyperpolarization-activated channels the least.|33|plos|1|introduction|OWN
 By demonstrating that neuronal responses to dendritic synaptic input can be intrinsically probabilistic, these results offer a new and general perspective on synaptic integration by central neurons.|34|plos|1|introduction|OWN
 Full documentation for PSICS/ICING as well as the software, source code and examples are available from the project website .|35|plos|1|introduction|OWN
Alternative splicing contributes to both gene regulation and protein diversity.|0|plos|0|abstract|MISC
To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.|1|plos|0|abstract|AIM
In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.|2|plos|0|abstract|OWN
Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.|3|plos|0|abstract|OWN
A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.|4|plos|0|abstract|OWN
By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.|5|plos|0|abstract|OWN
Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.|6|plos|0|abstract|OWN
Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.|7|plos|0|abstract|OWN
While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.|8|plos|0|abstract|OWN
Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.|9|plos|0|introduction|MISC
Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.|10|plos|0|introduction|MISC
In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.|11|plos|0|introduction|MISC
Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .|12|plos|0|introduction|MISC
Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.|13|plos|0|introduction|MISC
These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.|14|plos|0|introduction|MISC
In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.|15|plos|0|introduction|MISC
Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.|16|plos|0|introduction|MISC
Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .|17|plos|0|introduction|MISC
A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.|18|plos|0|introduction|MISC
Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.|19|plos|0|introduction|MISC
In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.|20|plos|0|introduction|MISC
Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.|21|plos|0|introduction|MISC
Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .|22|plos|0|introduction|CONT
Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.|23|plos|0|introduction|MISC
The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.|24|plos|0|introduction|MISC
Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.|25|plos|0|introduction|CONT
The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.|26|plos|0|introduction|CONT
Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.|27|plos|0|introduction|MISC
The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.|28|plos|0|introduction|MISC
Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.|29|plos|0|introduction|MISC
To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.|30|plos|0|introduction|OWN
We examine splicing in these tissues by asking three questions.|31|plos|0|introduction|AIM
First we ask, Which RNA isoforms are present in a particular tissue sample?|32|plos|0|introduction|OWN
To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.|33|plos|0|introduction|OWN
This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.|34|plos|0|introduction|BASE
Using RT-PCR, we show that this method has a true-positive rate of 85 percent.|35|plos|0|introduction|OWN
Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?|36|plos|0|introduction|OWN
For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.|37|plos|0|introduction|BASE
After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.|38|plos|0|introduction|OWN
Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?|39|plos|0|introduction|OWN
To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.|40|plos|0|introduction|OWN
We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.|41|plos|0|introduction|OWN
Alternative splicing contributes to both gene regulation and protein diversity.|0|plos|0|abstract|MISC
To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.|1|plos|0|abstract|AIM
In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.|2|plos|0|abstract|OWN
Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.|3|plos|0|abstract|OWN
A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.|4|plos|0|abstract|OWN
By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.|5|plos|0|abstract|OWN
Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.|6|plos|0|abstract|OWN
Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.|7|plos|0|abstract|OWN
While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.|8|plos|0|abstract|OWN
Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.|9|plos|0|introduction|MISC
Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.|10|plos|0|introduction|MISC
In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.|11|plos|0|introduction|MISC
Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .|12|plos|0|introduction|MISC
Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.|13|plos|0|introduction|MISC
These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.|14|plos|0|introduction|MISC
In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.|15|plos|0|introduction|MISC
Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.|16|plos|0|introduction|MISC
Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .|17|plos|0|introduction|MISC
A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.|18|plos|0|introduction|MISC
Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.|19|plos|0|introduction|MISC
In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.|20|plos|0|introduction|MISC
Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.|21|plos|0|introduction|MISC
Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .|22|plos|0|introduction|CONT
Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.|23|plos|0|introduction|MISC
The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.|24|plos|0|introduction|MISC
Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.|25|plos|0|introduction|CONT
The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.|26|plos|0|introduction|CONT
Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.|27|plos|0|introduction|MISC
The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.|28|plos|0|introduction|MISC
Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.|29|plos|0|introduction|MISC
To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.|30|plos|0|introduction|OWN
We examine splicing in these tissues by asking three questions.|31|plos|0|introduction|AIM
First we ask, Which RNA isoforms are present in a particular tissue sample?|32|plos|0|introduction|OWN
To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.|33|plos|0|introduction|OWN
This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.|34|plos|0|introduction|BASE
Using RT-PCR, we show that this method has a true-positive rate of 85 percent.|35|plos|0|introduction|OWN
Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?|36|plos|0|introduction|OWN
For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.|37|plos|0|introduction|BASE
After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.|38|plos|0|introduction|OWN
Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?|39|plos|0|introduction|OWN
To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.|40|plos|0|introduction|OWN
We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.|41|plos|0|introduction|OWN
 Alternative splicing contributes to both gene regulation and protein diversity.|0|plos|1|abstract|MISC
 To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome.|1|plos|1|abstract|AIM
 In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 percent of the 6,216 alternative events we could detect.|2|plos|1|abstract|OWN
 Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle.|3|plos|1|abstract|OWN
 A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons.|4|plos|1|abstract|OWN
 By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation.|5|plos|1|abstract|OWN
 Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 5 splice site of exons included in muscle.|6|plos|1|abstract|OWN
 Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern.|7|plos|1|abstract|OWN
 While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually complex evolution of the regulation of alternative splicing in multigene families.|8|plos|1|abstract|OWN
 Splicing is an essential process that constructs protein coding messenger RNA sequences using tiny segments of information buried in the much larger primary transcripts of the eukaryotic gene.|9|plos|1|introduction|MISC
 Regulated alternative splicing can create different protein coding sequences under different biological circumstances, allowing the production of functionally related but distinct proteins.|10|plos|1|introduction|MISC
 In addition, alternative splicing can mediate the repression of gene expression by stimulating the formation of transcripts subject to nonsense-mediated decay CITATION CITATION.|11|plos|1|introduction|MISC
 Splicing patterns seem distinct in the vertebrate nervous system compared to other tissues CITATION, CITATION, and it is tempting to hypothesize that neural alternative splicing contributed to the rapid evolution of the vertebrate brain without large increases in gene number CITATION .|12|plos|1|introduction|MISC
 Biochemical analysis of alternative splicing has shown that numerous RNA binding proteins influence the use of specific splice sites to stimulate splicing events that lead to particular mRNA isoforms CITATION, CITATION.|13|plos|1|introduction|MISC
 These RNA binding proteins may activate or repress the use of splice sites by binding to nearby sequences in the exon or in the intron.|14|plos|1|introduction|MISC
 In many cases, multiple RNA binding proteins combine to create repressing and activating influences that produce patterns of splicing control CITATION, CITATION.|15|plos|1|introduction|MISC
 Some proteins, such as SR proteins and the CELF proteins, have mostly activating roles, whereas others, such as hnRNP A1, PTB, and nPTB, have mostly repressing roles.|16|plos|1|introduction|MISC
 Certain proteins can either activate or repress splicing in different contexts, depending on the position of their binding sites or the expression of other RNA binding proteins CITATION, CITATION .|17|plos|1|introduction|MISC
 A complete catalog of the RNA sequences corresponding to the enhancers and silencers bound by splicing regulatory proteins would greatly aid the understanding of splicing regulatory networks.|18|plos|1|introduction|MISC
 Thus far, there are only a handful of splicing regulators whose corresponding RNA binding motifs have been identified, whereas there may be many splicing regulators among the hundreds of RNA binding proteins encoded by the mouse genome.|19|plos|1|introduction|MISC
 In addition, several related but distinct genes produce proteins that bind the same or overlapping sets of sequences; for example, Fox-1 and RBM9 each bind UGCAUG CITATION, CITATION, and the branchpoint binding protein SF1 and the protein quaking each bind UACUAAC-like motifs CITATION CITATION.|20|plos|1|introduction|MISC
 Adding to this complexity is the tendency for the mRNAs of RNA binding proteins to be alternatively spliced, leading to multiple RNA binding protein isoforms with potentially different functions.|21|plos|1|introduction|MISC
 Currently, the methods available for expanding the list of known regulators and their target sequences are limited, and the development of this catalog is in the early stage CITATION .|22|plos|1|introduction|CONT
 Much of the available genomic information on alternative splicing is derived by the alignment of large numbers of expressed sequence tags and messenger RNAs to genome sequences.|23|plos|1|introduction|MISC
 The analysis of exons that appear to be constitutive or alternative has led to the successful identification of many distinguishing features of alternatively spliced regions CITATION CITATION, even allowing their accurate prediction without cDNA evidence CITATION, CITATION, CITATION.|24|plos|1|introduction|MISC
 Although cDNA libraries have been invaluable for discovering general features of alternatively spliced exons, it is difficult to connect specific regulatory sequences to specific biological conditions with confidence due to variable and sometimes missing information about the source materials and methods of cDNA library construction.|25|plos|1|introduction|CONT
 The relatively low number of transcripts present from any one gene also makes it difficult to estimate differences in expression levels using library representation as a measure.|26|plos|1|introduction|CONT
 Thus, more direct methods are needed to associate alternative splicing events with underlying biological conditions.|27|plos|1|introduction|CONT
 The recent application of microarray technology to questions of splicing and splicing regulation promises to reveal parallel connections between many splicing events and specific biological or experimental conditions CITATION CITATION.|28|plos|1|introduction|MISC
 Analysis of experimental changes in splicing for many genes simultaneously should reveal biological conditions necessary for proper splicing regulation in a way that analysis of cDNA libraries cannot, and with breadth that cannot be achieved by analysis of a reporter construct or a few endogenous target genes.|29|plos|1|introduction|CONT
 To demonstrate this, we constructed a DNA microarray designed to capture splicing information for about 6,200 alternative events in the mouse transcriptome, using a combination of splice junction and exon probes, and have hybridized RNA from 22 adult mouse tissues.|30|plos|1|introduction|AIM
 We examine splicing in these tissues by asking three questions.|31|plos|1|introduction|AIM
 First we ask, Which RNA isoforms are present in a particular tissue sample?|32|plos|1|introduction|OWN
 To answer this simple question, we used a new method based on comparing the intensity of the probes in a probe set to the distribution of intensities from all probes with similar G C level.|33|plos|1|introduction|OWN
 This is similar in spirit although different in approach to the present-absent calls from Affymetrix MAS 5.0 algorithms CITATION, as this microarray did not contain mismatch probes.|34|plos|1|introduction|BASE
 Using RT-PCR, we show that this method has a true-positive rate of 85 percent.|35|plos|1|introduction|OWN
 Second we ask, Which RNA isoforms are differentially expressed across the tissues examined?|36|plos|1|introduction|OWN
 For each RNA isoform, the intensities of the isoform-specific junction probes were examined across tissues using the Kruskal-Wallis statistical test.|37|plos|1|introduction|OWN
 After correcting for multiple testing, about 40 percent of the 6,216 total alternative splicing events examined were found to have more than one RNA form that was differentially expressed, indicating widespread tissue differences in splicing over the tissues.|38|plos|1|introduction|OWN
 Third we ask, Which cassette exons are included differentially between brain and nonbrain tissues?|39|plos|1|introduction|OWN
 To answer this, we used a regression-based bootstrapping method, which also allows an estimate of the relative change in skipping and inclusion in the two sample groups.|40|plos|1|introduction|OWN
 We analyzed the intron sequences associated with exon skipping events that are differentially regulated in brain or muscle relative to other tissues and found unusual patterns of sequence conservation that provide new information about tissue regulation of alternative splicing and its evolution.|41|plos|1|introduction|OWN
Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.|0|plos|0|abstract|MISC
Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.|1|plos|0|abstract|MISC
Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.|2|plos|0|abstract|MISC
We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.|3|plos|0|abstract|AIM
We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.|4|plos|0|abstract|OWN
The second level combines the two predictions of the first level.|5|plos|0|abstract|OWN
The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.|6|plos|0|abstract|OWN
Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.|7|plos|0|abstract|OWN
The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.|8|plos|0|abstract|OWN
For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.|9|plos|0|abstract|OWN
Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.|10|plos|0|introduction|MISC
In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.|11|plos|0|introduction|MISC
An example of particularly high relevance is Human Immunodeficiency Virus 1.|12|plos|0|introduction|MISC
HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.|13|plos|0|introduction|MISC
The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.|14|plos|0|introduction|MISC
In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.|15|plos|0|introduction|MISC
Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.|16|plos|0|introduction|MISC
This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.|17|plos|0|introduction|MISC
The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.|18|plos|0|introduction|MISC
The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.|19|plos|0|introduction|CONT
These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.|20|plos|0|introduction|MISC
This is a relatively fast and cheap standard procedure established in many clinics.|21|plos|0|introduction|MISC
At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.|22|plos|0|introduction|MISC
Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.|23|plos|0|introduction|CONT
Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.|24|plos|0|introduction|MISC
The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.|25|plos|0|introduction|MISC
This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.|26|plos|0|introduction|MISC
To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.|27|plos|0|introduction|MISC
Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.|28|plos|0|introduction|CONT
It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.|29|plos|0|introduction|MISC
A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.|30|plos|0|introduction|MISC
This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.|31|plos|0|introduction|MISC
To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.|32|plos|0|introduction|MISC
Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.|33|plos|0|introduction|BASE
By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.|34|plos|0|introduction|OWN
The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.|35|plos|0|introduction|OWN
Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.|0|plos|0|abstract|MISC
Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.|1|plos|0|abstract|MISC
Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.|2|plos|0|abstract|MISC
We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.|3|plos|0|abstract|AIM
We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.|4|plos|0|abstract|OWN
The second level combines the two predictions of the first level.|5|plos|0|abstract|OWN
The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.|6|plos|0|abstract|OWN
Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.|7|plos|0|abstract|OWN
The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.|8|plos|0|abstract|OWN
For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.|9|plos|0|abstract|OWN
Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.|10|plos|0|introduction|MISC
In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.|11|plos|0|introduction|MISC
An example of particularly high relevance is Human Immunodeficiency Virus 1.|12|plos|0|introduction|MISC
HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.|13|plos|0|introduction|MISC
The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.|14|plos|0|introduction|MISC
In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.|15|plos|0|introduction|MISC
Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.|16|plos|0|introduction|MISC
This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.|17|plos|0|introduction|MISC
The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.|18|plos|0|introduction|MISC
The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.|19|plos|0|introduction|CONT
These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.|20|plos|0|introduction|MISC
This is a relatively fast and cheap standard procedure established in many clinics.|21|plos|0|introduction|MISC
At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.|22|plos|0|introduction|MISC
Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.|23|plos|0|introduction|CONT
Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.|24|plos|0|introduction|MISC
The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.|25|plos|0|introduction|MISC
This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.|26|plos|0|introduction|MISC
To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.|27|plos|0|introduction|MISC
Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.|28|plos|0|introduction|CONT
It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.|29|plos|0|introduction|MISC
A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.|30|plos|0|introduction|MISC
This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.|31|plos|0|introduction|MISC
To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.|32|plos|0|introduction|MISC
Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.|33|plos|0|introduction|BASE
By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.|34|plos|0|introduction|OWN
The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.|35|plos|0|introduction|OWN
 Human Immunodeficiency Virus 1 uses for entry into host cells a receptor and one of two co-receptors.|0|plos|1|abstract|MISC
 Recently, a new class of antiretroviral drugs has entered clinical practice that specifically bind to the co-receptor CCR5, and thus inhibit virus entry.|1|plos|1|abstract|MISC
 Accurate prediction of the co-receptor used by the virus in the patient is important as it allows for personalized selection of effective drugs and prognosis of disease progression.|2|plos|1|abstract|MISC
 We have investigated whether it is possible to predict co-receptor usage accurately by analyzing the amino acid sequence of the main determinant of co-receptor usage, i.e., the third variable loop V3 of the gp120 protein.|3|plos|1|abstract|AIM
 We developed a two-level machine learning approach that in the first level considers two different properties important for protein-protein binding derived from structural models of V3 and V3 sequences.|4|plos|1|abstract|OWN
 The second level combines the two predictions of the first level.|5|plos|1|abstract|OWN
 The two-level method predicts usage of CXCR4 co-receptor for new V3 sequences within seconds, with an area under the ROC curve of 0.937 0.004.|6|plos|1|abstract|OWN
 Moreover, it is relatively robust against insertions and deletions, which frequently occur in V3.|7|plos|1|abstract|OWN
 The approach could help clinicians to find optimal personalized treatments, and it offers new insights into the molecular basis of co-receptor usage.|8|plos|1|abstract|OWN
 For instance, it quantifies the importance for co-receptor usage of a pocket that probably is responsible for binding sulfated tyrosine.|9|plos|1|abstract|OWN
 Specific protein interactions are central to biological processes, and the infection of cells with viruses is no exception there.|10|plos|1|introduction|MISC
 In the case of pathogenic viruses, such protein interactions are potential targets for medical intervention.|11|plos|1|introduction|MISC
 An example of particularly high relevance is Human Immunodeficiency Virus 1.|12|plos|1|introduction|MISC
 HIV-1 enters human cells in a process that comprises several steps, including the binding of the viral gp120 protein to the cellular receptor protein CD4 and a co-receptor protein, usually one of the two chemokine receptors CCR5 and CXCR4 CITATION.|13|plos|1|introduction|MISC
 The type of co-receptor used by the virus, the so-called co-receptor tropism, has a prognostic value, since patients with a CXCR4-tropic virus progress faster to Acquired Immunodeficiency Syndrome compared to patients with a CCR5-tropic virus CITATION.|14|plos|1|introduction|MISC
 In addition to the purely X4- and R5-tropic viruses, there are also dual-tropic strains, able to use both co-receptors.|15|plos|1|introduction|MISC
 Recently, the first drug that binds to CCR5, and thus inhibits productive binding of gp120, has been approved by regulatory authorities in several countries.|16|plos|1|introduction|MISC
 This has made the determination of co-receptor tropism directly relevant to anti-retroviral treatment, as CCR5-inhibitors are of course inactive against X4 virus.|17|plos|1|introduction|MISC
 The standard way of determining co-receptor tropism is by cell-based assays CITATION, CITATION.|18|plos|1|introduction|MISC
 The main drawbacks of these assays are that they are currently only carried out by a handful of specialized laboratories worldwide, and that the overall procedure typically takes several weeks.|19|plos|1|introduction|CONT
 These impediments to the wide application of entry inhibitors could be overcome by an approach similar to genotypic drug resistance testing CITATION, where drug resistance of a viral strain is inferred from comparison of mutational patterns obtained from sequencing parts of the genome of that strain with patterns of validated resistance mutations.|20|plos|1|introduction|MISC
 This is a relatively fast and cheap standard procedure established in many clinics.|21|plos|1|introduction|MISC
 At first glance, genotypic testing for co-receptor tropism seems to be possible since the main molecular determinant of tropism is known to be the third variable loop of the viral glycoprotein gp120 CITATION, a peptide stretch of about 35 amino-acids with a disulfide bridge connecting the terminal cysteins.|22|plos|1|introduction|MISC
 Unfortunately, as suggested by its name, V3 is notorious for its high sequence variability CITATION including also some variability in length, and this has made it difficult to use it as a basis for genotypic co-receptor tropism testing.|23|plos|1|introduction|MISC
 Nevertheless, the relevance of the quest has prompted many groups to develop models that link properties of V3 to co-receptor tropism.|24|plos|1|introduction|MISC
 The importance of electrostatics for co-receptor tropism has been recognized early on, and the best-known model, the so-called 11/25-rule, refers to charges of V3-residues 11 and 25: if one of these is positive, then the virus is CXCR4-tropic CITATION, CITATION.|25|plos|1|introduction|MISC
 This rule has a specificity of more than 0.9, but only a low to moderate sensitivity of about 0.4 0.6, depending on the test data, which is not satisfactory for routine clinical application.|26|plos|1|introduction|CONT
 To improve predictions from sequence, several groups have applied machine learning methods, such as artificial neural networks CITATION, position specific scoring matrices CITATION, decision trees, or support vector machines CITATION.|27|plos|1|introduction|MISC
 Still, prediction accuracies fall short of what seems reasonable for regular clinical use CITATION.|28|plos|1|introduction|CONT
 It is unclear whether the limited accuracies are the footprint of tropism-determinants outside V3, or the consequence of model imperfections.|29|plos|1|introduction|MISC
 A milestone for the understanding of co-receptor tropism was the X-ray structure of gp120 with the V3 loop in a biological context CITATION.|30|plos|1|introduction|MISC
 This paved the way for the development of prediction methods that use, in addition to V3 sequence, structural information.|31|plos|1|introduction|MISC
 To our knowledge, the first of these methods has been that of Sander et al. CITATION, which was mainly based on geometric distances of amino-acid pairs within the structure of V3.|32|plos|1|introduction|MISC
 Although our method, detailed in the following, relies on the same experimental structure by Huang et al. CITATION, it differs from that of Sander et al. in several respects, e.g. it deals with indels, and, perhaps most crucially, it uses as descriptors properties that directly determine interaction of V3 with the co-receptors.|33|plos|1|introduction|BASE
 By the latter we consider a seemingly trivial but fundamental fact that so far has not been thoroughly exploited: although V3 is highly variable, all X4-tropic V3 loops share one property, namely, they preferentially have a physical binding interaction with CXCR4, while R5-tropic V3 loops preferably interacts with CCR5.|34|plos|1|introduction|OWN
 The accuracy of the method makes it attractive as clinical tool for patient tailored decisions on treatment with entry inhibitors, and it suggests that co-receptor tropism can be explained almost exclusively based on V3.|35|plos|1|introduction|OWN
